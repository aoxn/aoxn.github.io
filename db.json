{"meta":{"version":1,"warehouse":"4.0.1"},"models":{"Asset":[{"_id":"source/uploads/headpic.jpg","path":"uploads/headpic.jpg","modified":0,"renderable":0},{"_id":"source/upload/15134909004013.jpg","path":"upload/15134909004013.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"source/upload/img.png","path":"upload/img.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/_posts/KuberntesAPI设计哲学.md","hash":"722da9521dcdefb406d6b7c7da7ebc1409467b86","modified":1654654154806},{"_id":"source/_posts/highavailableregistry.md","hash":"3aad02ce157390d949cc921ca59686075df9841c","modified":1654654154806},{"_id":"source/_posts/pagetable.md","hash":"b8afeedb280a950f35a51fbcc31c0854d773aaa3","modified":1654654154807},{"_id":"source/_posts/swithprocess.md","hash":"52ef649a6781f4f2c1e1d6af37298e882ac4d412","modified":1654654154806},{"_id":"source/_posts/keyjump.md","hash":"1933ff9f697ce735258c2ebdbec3732673a15bff","modified":1654654154807},{"_id":"source/categories/index.md","hash":"4424e825d898608c4fed29e072c8048b9cc51160","modified":1654654154810},{"_id":"source/about/index.md","hash":"a45ad22491f1f1b3a47abbbe8f4556acb4735bba","modified":1654690754528},{"_id":"source/tags/index.md","hash":"16b9f02d787b281fd37651e4f4957c250c70e872","modified":1654654154809},{"_id":"source/uploads/headpic.jpg","hash":"28982da158b0e06bbf729630c21e65caa207191f","modified":1654654154807},{"_id":"source/upload/15134909004013.jpg","hash":"1ad8ca1a1e4bdfdbd8122edb91ff3d49543f0a52","modified":1654654154811},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1654654154939},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1654654154818},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1654654154818},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1654654154890},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1654654154891},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1654654154889},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1654654154879},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1654654154878},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1654654154812},{"_id":"themes/next/package.json","hash":"93a74dbc0fe3a1208a02e9cec3c15c2375339cc1","modified":1654654154946},{"_id":"themes/next/README.cn.md","hash":"58ffe752bc4b7f0069fcd6304bbc2d5ff7b80f89","modified":1654654154812},{"_id":"themes/next/README.md","hash":"aa2fb15ef016074b10755323c99025df506d5ca3","modified":1654654154946},{"_id":"themes/next/_config.yml","hash":"5904bb42cf6d11a1f2648b8bc00a367a5fda1034","modified":1654680793582},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1654654154846},{"_id":"themes/next/bower.json","hash":"6d6ae7531cf3fedc97c58cdad664f5793eb3cc88","modified":1654654154940},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1654654154940},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1654654154942},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1654654154943},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1654654154945},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1654654154942},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1654654154943},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1654654154944},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1654654154941},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1654654154941},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1654654154943},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1654654154944},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1654654154942},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1654654154941},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1654654154946},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1654654154944},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1654654154945},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1654654154816},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1654654154828},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1654654154816},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1654654154821},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1654654154821},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1654654154947},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1654654154820},{"_id":"themes/next/scripts/merge-configs.js","hash":"cb617ddf692f56e6b6129564d52e302f50b28243","modified":1654654154948},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1654654154814},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1654654154815},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1654654154813},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1654654154814},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1654654154816},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1654654154815},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1654654154813},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1654654154845},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1654654154845},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1654654154844},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1654654154845},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"9efc455894921a66bbc074055d3b39c8a34a48a4","modified":1654654154844},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1654654154825},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1654654154844},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1654654154826},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1654654154825},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1654654154826},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1654654154822},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1654654154824},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1654654154826},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1654654154819},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1654654154819},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1654654154819},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1654654154837},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1654654154833},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1654654154838},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1654654154837},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1654654154949},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1654654154949},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1654654154833},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1654654154951},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1654654154950},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1654654154834},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1654654154950},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1654654154833},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1654654154950},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1654654154948},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1654654154949},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1654654154949},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1654654154880},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1654654154892},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1654654154894},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1654654154892},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1654654154896},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1654654154893},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1654654154892},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1654654154891},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1654654154895},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1654654154893},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1654654154896},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1654654154895},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1654654154894},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1654654154894},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1654654154896},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1654654154891},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1654654154893},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1654654154895},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1654654154892},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1654654154824},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1654654154824},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1654654154823},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1654654154823},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1654654154822},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1654654154828},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1654654154827},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1654654154827},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1654654154820},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1654654154828},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1654654154817},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1654654154818},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1654654154841},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1654654154840},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1654654154842},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1654654154839},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1654654154840},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1654654154843},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1654654154842},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1654654154839},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1654654154840},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1654654154843},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1654654154841},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1654654154842},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1654654154839},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1654654154832},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1654654154830},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1654654154831},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1654654154832},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1654654154831},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1654654154829},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1654654154833},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1654654154830},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1654654154831},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1654654154838},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1654654154834},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1654654154835},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1654654154835},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1654654154890},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1654654154890},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1654654154890},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1654654154878},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1654654154847},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1654654154879},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1654654154879},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1654654154879},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1654654154900},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1654654154899},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1654654154898},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1654654154900},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1654654154898},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1654654154897},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1654654154899},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1654654154900},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1654654154898},{"_id":"themes/next/source/js/src/utils.js","hash":"dbdc3d1300eec7da9632608ebc0e5b697779dad7","modified":1654654154899},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1654654154898},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1654654154906},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1654654154920},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1654654154907},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1654654154914},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1654654154902},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1654654154914},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1654654154901},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1654654154935},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1654654154902},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1654654154902},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1654654154935},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1654654154933},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1654654154935},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1654654154934},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1654654154915},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1654654154928},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1654654154927},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1654654154927},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1654654154927},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1654654154928},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1654654154927},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1654654154904},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1654654154918},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1654654154904},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1654654154904},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1654654154919},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1654654154917},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1654654154916},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1654654154919},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1654654154918},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1654654154919},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1654654154916},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1654654154917},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1654654154917},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1654654154916},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1654654154919},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1654654154918},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1654654154916},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1654654154925},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1654654154923},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1654654154921},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1654654154923},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1654654154921},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1654654154921},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1654654154922},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1654654154921},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1654654154836},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1654654154863},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1654654154836},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1654654154868},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1654654154864},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1654654154864},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1654654154851},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1654654154864},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1654654154868},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1654654154877},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1654654154849},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1654654154848},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1654654154850},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1654654154849},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1654654154850},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1654654154849},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1654654154882},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1654654154887},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1654654154888},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1654654154889},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1654654154887},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1654654154889},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1654654154888},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1654654154887},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1654654154881},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"02fb8fa6b6c252b6bed469539cd057716606a787","modified":1654654154882},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1654654154882},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1654654154881},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1654654154881},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1654654154884},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1654654154883},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1654654154884},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"bcf52192942c0afc410c74a0fb458e7936ddc3d5","modified":1654654154885},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1654654154885},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1654654154884},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1654654154897},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1654654154929},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1654654154932},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1654654154929},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1654654154911},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1654654154911},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1654654154910},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1654654154909},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1654654154908},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1654654154910},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1654654154909},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1654654154909},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1654654154908},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1654654154903},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1654654154903},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1654654154934},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1654654154933},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1654654154934},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1654654154926},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1654654154926},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1654654154856},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1654654154876},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1654654154876},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1654654154877},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1654654154876},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1654654154875},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1654654154863},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1654654154862},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1654654154862},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1654654154875},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1654654154874},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1654654154874},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1654654154874},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1654654154873},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1654654154852},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1654654154854},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1654654154852},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1654654154852},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1654654154855},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1654654154853},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1654654154855},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"77c92a449ce84d558d26d052681f2e0dd77c70c9","modified":1654654154854},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1654654154854},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1654654154853},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1654654154861},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1654654154860},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1654654154860},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1654654154861},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1654654154860},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1654654154857},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1654654154858},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1654654154859},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1654654154856},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1654654154857},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1654654154859},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1654654154858},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1654654154861},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1654654154857},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1654654154859},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1654654154868},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1654654154866},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1654654154867},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1654654154865},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1654654154867},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1654654154866},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1654654154871},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1654654154867},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1654654154865},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1654654154866},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1654654154870},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1654654154870},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1654654154871},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1654654154869},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1654654154873},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1654654154870},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1654654154871},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1654654154872},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1654654154872},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1654654154886},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1654654154880},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1654654154888},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1654654154930},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1654654154931},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1654654154931},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1654654154931},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1654654154932},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1654654154912},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1654654154913},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1654654154913},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1654654154913},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1654654154912},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1654654154912},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1654654154915},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1654654154930},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1654654154939},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1654654154938},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1654654154937},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1654654154939},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1654654154938},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1654654154922},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1654654154905},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1654654154936},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1654654154924},{"_id":"source/_posts/遇见复原力.md","hash":"d87a63498843e5578debe54c17ffce54f504cb02","modified":1654690256053},{"_id":"public/atom.xml","hash":"ccd31f0809236d5053d1fa78500a29e8ef46d2ce","modified":1654691354355},{"_id":"public/sitemap.xml","hash":"a029484a5031a765dc72b0d48f9da281302bd2b5","modified":1654691354355},{"_id":"public/sitemap.txt","hash":"082a44a03d122ea53f26c7ddaaa137a641243e5e","modified":1654691354355},{"_id":"public/categories/index.html","hash":"1f682da181dd912f141d445d0c8f5273c01fa6b4","modified":1654688728069},{"_id":"public/about/index.html","hash":"5b8f9cb781d86027a94ba062453d6c1fa857dcf2","modified":1654691354355},{"_id":"public/tags/index.html","hash":"df09326b97ccfd7c2bd16126aac51669650b766e","modified":1654688728069},{"_id":"public/2022/06/08/遇见复原力/index.html","hash":"b36b3ea7e48fad4226979da1bbad4558868fa3a9","modified":1654690260005},{"_id":"public/categories/Kubernetes/index.html","hash":"08244c514a0d5eba4daf0c86133ad40622eb163c","modified":1654688728069},{"_id":"public/categories/容器/index.html","hash":"b89858be46dd38820db9020d48c8c2b5b73756a7","modified":1654688728069},{"_id":"public/categories/操作系统/index.html","hash":"f1db40b10aeb65ff14d481d869ef589c0cccbe94","modified":1654688728069},{"_id":"public/categories/Kubernetes/Container/index.html","hash":"ab27b832909d5299357f7dde69423fb062203f95","modified":1654688728069},{"_id":"public/categories/容器/Docker/index.html","hash":"ed6abcec78496f194922eb7e940c3b6ba0aba42c","modified":1654688728069},{"_id":"public/categories/操作系统/Linux内核/index.html","hash":"f43e86c0cae7c90d9ec76328c47003bf883470e9","modified":1654688728069},{"_id":"public/categories/Kubernetes/Container/容器/index.html","hash":"663a4a8783445de782d346278cd7dd3362e137a0","modified":1654688728069},{"_id":"public/archives/index.html","hash":"e0dbb112dbafe9460de08f0cac76d17cc4402b7a","modified":1654688728069},{"_id":"public/archives/2015/index.html","hash":"087c3060ee20e3138da2a385ba777b4a059529bc","modified":1654688728069},{"_id":"public/archives/2015/11/index.html","hash":"bdfa67d69268b793f9839ec2650ef02b75609d08","modified":1654688728069},{"_id":"public/archives/2016/index.html","hash":"9f7805c372c8435ad31f06a5c2c5cecf82d91d9b","modified":1654688728069},{"_id":"public/archives/2016/01/index.html","hash":"3d8356ef20b0b483b9cf142d8724e7eb1b5703b6","modified":1654688728069},{"_id":"public/archives/2017/index.html","hash":"dbf0b0ec3e6eb4782bee0b4218593a4bf48d87b1","modified":1654688728069},{"_id":"public/archives/2017/12/index.html","hash":"ad905c3a8e9e7a595cb3b78eac517779e0df8fc1","modified":1654688728069},{"_id":"public/archives/2022/index.html","hash":"19a6075aa294dcef842376b25d90e4dcc945bd53","modified":1654688728069},{"_id":"public/archives/2022/06/index.html","hash":"2121dcb5f40eae75a7fd02c2e142dc49c8d1a854","modified":1654688728069},{"_id":"public/tags/Kubernetes/index.html","hash":"14b7c61fb118a0a2d48805e09b9ab75d19d26e8e","modified":1654688728069},{"_id":"public/tags/Docker/index.html","hash":"53dd214896998eea23b66cee9c7c6743491426c8","modified":1654688728069},{"_id":"public/tags/Registry/index.html","hash":"3e73d67998211154387d2b3e3130009073d85b56","modified":1654688728069},{"_id":"public/tags/页表/index.html","hash":"572fdc50d91677a991db9b833efd3518c6b339c3","modified":1654688728069},{"_id":"public/tags/进程切换/index.html","hash":"46b2de8b8ccf1a26f0b712ecac89f7e794ac579e","modified":1654688728069},{"_id":"public/tags/实模式/index.html","hash":"254103560f71f34868050a988bbab190d04c843c","modified":1654688728069},{"_id":"public/tags/保护模式/index.html","hash":"c34ef28918db44df24830502361300b89c044713","modified":1654688728069},{"_id":"public/2017/12/16/KuberntesAPI设计哲学/index.html","hash":"f470d96f522e614ef1a1112a663edd16db80ff45","modified":1654688728069},{"_id":"public/2016/01/27/pagetable/index.html","hash":"ede461819eb471850c2aa373dbf330da8e00df97","modified":1654688728069},{"_id":"public/2015/11/05/swithprocess/index.html","hash":"0e13d0c966ec6edadd4db39e187bc6484e010309","modified":1654688728069},{"_id":"public/2015/11/05/keyjump/index.html","hash":"90e01f4ab1fd85de31b0a04632a57dfb37e0af53","modified":1654688728069},{"_id":"public/2015/11/04/highavailableregistry/index.html","hash":"f2e86e68e5a7855e479224c2d29f87fc3ae534e8","modified":1654688728069},{"_id":"public/index.html","hash":"966b2d42705b01d5d513d936d06a51817c74b55e","modified":1654691354355},{"_id":"source/_posts/管理集群.md","hash":"449cd6a03a56060928aea0db0faf367508cc075c","modified":1654689949764},{"_id":"public/2022/06/08/管理集群/index.html","hash":"d7d9f925f1f9dc9ed33f50ff8358a90f9b30e172","modified":1654690260005},{"_id":"source/_posts/云原生应用示例.md","hash":"dbb2a8677304f04db8e214335b7085a467e26a64","modified":1654690721061},{"_id":"source/upload/img.png","hash":"29f6a14ef0dde0f87c148ad9f107d43a6f72bbfb","modified":1654688513046},{"_id":"public/2022/06/08/云原生应用示例/index.html","hash":"0067c8e141b196c97b463da5804311340238113d","modified":1654691354355},{"_id":"public/upload/img.png","hash":"29f6a14ef0dde0f87c148ad9f107d43a6f72bbfb","modified":1654688728069}],"Category":[{"name":"Kubernetes","_id":"cl4507g6m0004jjrh5sed1t6d"},{"name":"容器","_id":"cl4507g6s000ajjrhe2nj4uby"},{"name":"操作系统","_id":"cl4507g6t000djjrh6oq49xe5"},{"name":"Container","parent":"cl4507g6m0004jjrh5sed1t6d","_id":"cl4507g6v000jjjrh1u4lfnoy"},{"name":"Docker","parent":"cl4507g6s000ajjrhe2nj4uby","_id":"cl4507g6x000pjjrhb7wmeltk"},{"name":"Linux内核","parent":"cl4507g6t000djjrh6oq49xe5","_id":"cl4507g6y000rjjrhdug55iit"},{"name":"容器","parent":"cl4507g6v000jjjrh1u4lfnoy","_id":"cl4507g6z000xjjrh2qbbdjs7"}],"Data":[],"Page":[{"title":"分类","date":"2015-11-05T06:53:46.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"title: 分类 \ndate: 2015-11-05 14:53:46\ntype: \"categories\"\n---\n","updated":"2022-06-08T02:09:14.810Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cl4507g6c0000jjrh3ktwh6kh","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"about","date":"2015-11-05T06:59:34.000Z","comments":0,"_content":"\n**知识在于分享**\n\n## 勾搭一下：\n\nWechat **@AoxnKKB**\n\nMail To: **spacex_nice@163.com**\n","source":"about/index.md","raw":"title: about\ndate: 2015-11-05 14:59:34\ncomments: false\n---\n\n**知识在于分享**\n\n## 勾搭一下：\n\nWechat **@AoxnKKB**\n\nMail To: **spacex_nice@163.com**\n","updated":"2022-06-08T12:19:14.528Z","path":"about/index.html","_id":"cl4507g6k0002jjrhe8im1gwp","layout":"page","content":"<p><strong>知识在于分享</strong></p>\n<h2 id=\"勾搭一下：\"><a href=\"#勾搭一下：\" class=\"headerlink\" title=\"勾搭一下：\"></a>勾搭一下：</h2><p>Wechat <strong>@AoxnKKB</strong></p>\n<p>Mail To: <strong><a href=\"mailto:&#x73;&#112;&#x61;&#99;&#x65;&#x78;&#x5f;&#x6e;&#x69;&#x63;&#101;&#64;&#49;&#x36;&#x33;&#46;&#x63;&#111;&#109;\">&#x73;&#112;&#x61;&#99;&#x65;&#x78;&#x5f;&#x6e;&#x69;&#x63;&#101;&#64;&#49;&#x36;&#x33;&#46;&#x63;&#111;&#109;</a></strong></p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>知识在于分享</strong></p>\n<h2 id=\"勾搭一下：\"><a href=\"#勾搭一下：\" class=\"headerlink\" title=\"勾搭一下：\"></a>勾搭一下：</h2><p>Wechat <strong>@AoxnKKB</strong></p>\n<p>Mail To: <strong><a href=\"mailto:&#x73;&#112;&#x61;&#99;&#x65;&#x78;&#x5f;&#x6e;&#x69;&#x63;&#101;&#64;&#49;&#x36;&#x33;&#46;&#x63;&#111;&#109;\">&#x73;&#112;&#x61;&#99;&#x65;&#x78;&#x5f;&#x6e;&#x69;&#x63;&#101;&#64;&#49;&#x36;&#x33;&#46;&#x63;&#111;&#109;</a></strong></p>\n"},{"title":"tags","date":"2015-11-05T07:00:21.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"title: tags\ndate: 2015-11-05 15:00:21\ntype: \"tags\"\ncomments: false\n---\n","updated":"2022-06-08T02:09:14.809Z","path":"tags/index.html","layout":"page","_id":"cl4507g6o0006jjrh4vkrfaov","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Kubernetes API设计哲学","date":"2017-12-16T07:21:02.000Z","_content":"\n# Kubernetes API设计哲学\nkubernetes API设计时遵循了如下几个特征，声明式的API设计、水平触发的reconcile、异步的pull模式\n## 声明式（Declarative）\n用户指定的是期望达到的状态，而不是操作命令。\nKubernetes API设计的理念如下： 用户将一个资源对象的期望状态通过描述文件的方式发送给API接口，然后API接口通过不间断的操作使资源对象达到并稳定在期望的状态之上。例如，用户将一个具有多个副本、及指定版本的deployment发送到API接口时，API首先对该资源对象做校验然后进行持久化存储，接下来一个reconcile操作会取回该资源并执行该资源的定义操作，直到该资源的实际状态与定义的期望状态一致。期间如果实际状态被意外更改，仍然会被reconcile到期望状态。\n\n**reconcile**\nreconcile是一组预定义的循环操作(通常的实现是controller-manager)，watch storage上资源对象的期望状态的变化，并对比资源的本地实际状态，执行相关操作使资源最终达到该期望的一致状态，具有幂等性。\n![](/upload/15134909004013.jpg)\n\n## 水平触发（Level trigger）\nKubernetes API是基于水平触发的实现，系统会驱动资源达到当前所期望的状态，而不管在此之前被设置了哪些不同的期望状态，以当前为基准。例如对当前正在进行rollout的deployment更改其镜像，会促使系统放弃执行当前未完成的rollout,转而进入新的desired state reconcile 流程，即切换到最新的修改上。\n\n## 异步性（Asynchronous）\nKubernetes API接口异步的执行资源的desired state reconcile操作。这意味着创建资源的API请求会在完成校验并存储后立刻返回给调用方而不会立刻执行任何的资源创建动作。这也会导致许多创建中的错误不会在这个阶段返回给用户。解决的方式是定义良好的事件通知机制，资源reconcile过程中产生的错误信息写入到资源关联的事件中，用户通过watch事件了解创建过程。\n\n\n## Reference\n[Building and using Kubernetes API](https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/api_building_overview.md)\n\n","source":"_posts/KuberntesAPI设计哲学.md","raw":"title: Kubernetes API设计哲学\ndate: 2017-12-16 15:21:02\ntags: [\"Kubernetes\"]\ncategories: [\"Kubernetes\",\"Container\",\"容器\"]\n---\n\n# Kubernetes API设计哲学\nkubernetes API设计时遵循了如下几个特征，声明式的API设计、水平触发的reconcile、异步的pull模式\n## 声明式（Declarative）\n用户指定的是期望达到的状态，而不是操作命令。\nKubernetes API设计的理念如下： 用户将一个资源对象的期望状态通过描述文件的方式发送给API接口，然后API接口通过不间断的操作使资源对象达到并稳定在期望的状态之上。例如，用户将一个具有多个副本、及指定版本的deployment发送到API接口时，API首先对该资源对象做校验然后进行持久化存储，接下来一个reconcile操作会取回该资源并执行该资源的定义操作，直到该资源的实际状态与定义的期望状态一致。期间如果实际状态被意外更改，仍然会被reconcile到期望状态。\n\n**reconcile**\nreconcile是一组预定义的循环操作(通常的实现是controller-manager)，watch storage上资源对象的期望状态的变化，并对比资源的本地实际状态，执行相关操作使资源最终达到该期望的一致状态，具有幂等性。\n![](/upload/15134909004013.jpg)\n\n## 水平触发（Level trigger）\nKubernetes API是基于水平触发的实现，系统会驱动资源达到当前所期望的状态，而不管在此之前被设置了哪些不同的期望状态，以当前为基准。例如对当前正在进行rollout的deployment更改其镜像，会促使系统放弃执行当前未完成的rollout,转而进入新的desired state reconcile 流程，即切换到最新的修改上。\n\n## 异步性（Asynchronous）\nKubernetes API接口异步的执行资源的desired state reconcile操作。这意味着创建资源的API请求会在完成校验并存储后立刻返回给调用方而不会立刻执行任何的资源创建动作。这也会导致许多创建中的错误不会在这个阶段返回给用户。解决的方式是定义良好的事件通知机制，资源reconcile过程中产生的错误信息写入到资源关联的事件中，用户通过watch事件了解创建过程。\n\n\n## Reference\n[Building and using Kubernetes API](https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/api_building_overview.md)\n\n","slug":"KuberntesAPI设计哲学","published":1,"updated":"2022-06-08T02:09:14.806Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4507g6g0001jjrh2lra3s19","content":"<h1 id=\"Kubernetes-API设计哲学\"><a href=\"#Kubernetes-API设计哲学\" class=\"headerlink\" title=\"Kubernetes API设计哲学\"></a>Kubernetes API设计哲学</h1><p>kubernetes API设计时遵循了如下几个特征，声明式的API设计、水平触发的reconcile、异步的pull模式</p>\n<h2 id=\"声明式（Declarative）\"><a href=\"#声明式（Declarative）\" class=\"headerlink\" title=\"声明式（Declarative）\"></a>声明式（Declarative）</h2><p>用户指定的是期望达到的状态，而不是操作命令。<br>Kubernetes API设计的理念如下： 用户将一个资源对象的期望状态通过描述文件的方式发送给API接口，然后API接口通过不间断的操作使资源对象达到并稳定在期望的状态之上。例如，用户将一个具有多个副本、及指定版本的deployment发送到API接口时，API首先对该资源对象做校验然后进行持久化存储，接下来一个reconcile操作会取回该资源并执行该资源的定义操作，直到该资源的实际状态与定义的期望状态一致。期间如果实际状态被意外更改，仍然会被reconcile到期望状态。</p>\n<p><strong>reconcile</strong><br>reconcile是一组预定义的循环操作(通常的实现是controller-manager)，watch storage上资源对象的期望状态的变化，并对比资源的本地实际状态，执行相关操作使资源最终达到该期望的一致状态，具有幂等性。<br><img src=\"/upload/15134909004013.jpg\"></p>\n<h2 id=\"水平触发（Level-trigger）\"><a href=\"#水平触发（Level-trigger）\" class=\"headerlink\" title=\"水平触发（Level trigger）\"></a>水平触发（Level trigger）</h2><p>Kubernetes API是基于水平触发的实现，系统会驱动资源达到当前所期望的状态，而不管在此之前被设置了哪些不同的期望状态，以当前为基准。例如对当前正在进行rollout的deployment更改其镜像，会促使系统放弃执行当前未完成的rollout,转而进入新的desired state reconcile 流程，即切换到最新的修改上。</p>\n<h2 id=\"异步性（Asynchronous）\"><a href=\"#异步性（Asynchronous）\" class=\"headerlink\" title=\"异步性（Asynchronous）\"></a>异步性（Asynchronous）</h2><p>Kubernetes API接口异步的执行资源的desired state reconcile操作。这意味着创建资源的API请求会在完成校验并存储后立刻返回给调用方而不会立刻执行任何的资源创建动作。这也会导致许多创建中的错误不会在这个阶段返回给用户。解决的方式是定义良好的事件通知机制，资源reconcile过程中产生的错误信息写入到资源关联的事件中，用户通过watch事件了解创建过程。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><a href=\"https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/api_building_overview.md\">Building and using Kubernetes API</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Kubernetes-API设计哲学\"><a href=\"#Kubernetes-API设计哲学\" class=\"headerlink\" title=\"Kubernetes API设计哲学\"></a>Kubernetes API设计哲学</h1><p>kubernetes API设计时遵循了如下几个特征，声明式的API设计、水平触发的reconcile、异步的pull模式</p>\n<h2 id=\"声明式（Declarative）\"><a href=\"#声明式（Declarative）\" class=\"headerlink\" title=\"声明式（Declarative）\"></a>声明式（Declarative）</h2><p>用户指定的是期望达到的状态，而不是操作命令。<br>Kubernetes API设计的理念如下： 用户将一个资源对象的期望状态通过描述文件的方式发送给API接口，然后API接口通过不间断的操作使资源对象达到并稳定在期望的状态之上。例如，用户将一个具有多个副本、及指定版本的deployment发送到API接口时，API首先对该资源对象做校验然后进行持久化存储，接下来一个reconcile操作会取回该资源并执行该资源的定义操作，直到该资源的实际状态与定义的期望状态一致。期间如果实际状态被意外更改，仍然会被reconcile到期望状态。</p>\n<p><strong>reconcile</strong><br>reconcile是一组预定义的循环操作(通常的实现是controller-manager)，watch storage上资源对象的期望状态的变化，并对比资源的本地实际状态，执行相关操作使资源最终达到该期望的一致状态，具有幂等性。<br><img src=\"/upload/15134909004013.jpg\"></p>\n<h2 id=\"水平触发（Level-trigger）\"><a href=\"#水平触发（Level-trigger）\" class=\"headerlink\" title=\"水平触发（Level trigger）\"></a>水平触发（Level trigger）</h2><p>Kubernetes API是基于水平触发的实现，系统会驱动资源达到当前所期望的状态，而不管在此之前被设置了哪些不同的期望状态，以当前为基准。例如对当前正在进行rollout的deployment更改其镜像，会促使系统放弃执行当前未完成的rollout,转而进入新的desired state reconcile 流程，即切换到最新的修改上。</p>\n<h2 id=\"异步性（Asynchronous）\"><a href=\"#异步性（Asynchronous）\" class=\"headerlink\" title=\"异步性（Asynchronous）\"></a>异步性（Asynchronous）</h2><p>Kubernetes API接口异步的执行资源的desired state reconcile操作。这意味着创建资源的API请求会在完成校验并存储后立刻返回给调用方而不会立刻执行任何的资源创建动作。这也会导致许多创建中的错误不会在这个阶段返回给用户。解决的方式是定义良好的事件通知机制，资源reconcile过程中产生的错误信息写入到资源关联的事件中，用户通过watch事件了解创建过程。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><a href=\"https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/api_building_overview.md\">Building and using Kubernetes API</a></p>\n"},{"title":"基于haproxy和keepalived的高可用的私有docker registry","date":"2015-11-04T14:02:11.000Z","_content":"\n本文用于搭建一个基于haproxy和keepalived的高可用的私有docker registry\n\n## 部署结构\n\n* haproxy keepalived 主：221.228.86.4\n* haproxy keepalived 备：221.228.86.5\n* docker registry 1：    221.228.86.6\n* docker registry 2：    221.228.86.67\n* VIP : 221.228.86.70 \n\n```bash\n                    221.228.86.100\n             +-----------VIP----------+   \n             |                        |\n             |                        |\n           Master                   Backup\n        221.228.86.4             221.228.86.6\n        +----------+             +----------+\n        | HAProxy  |             | HAProxy  |\n        |keepalived|             |keepalived|\n        +----------+             +----------+\n             |  \n             v  \n    +--------+---------+ \n    |        |         |\n    |        |         |\n    v        v         v\n+------+  +------+  +------+\n| WEB1 |  | WEB2 |  | WEB3 |\n+------+  +------+  +------+\n```\n\n<!-- more -->\n\n#### 修改docker daemon 启动参数 \n```bash\n修改/etc/default/docker文件，在DOCKER_OPTS=\"--insecure-registry vip:5000\"\n```\n\n## Docker Registry 配置\n* 参考 https://github.com/docker/distribution/blob/master/docs/configuration.md\n* 221.228.86.5 221.228.86.67\n* 在/var/lib/pri_docker_registry/目录下建立config.yml配置文件,内容如下：\n\n```bash\nversion: 0.1\nlog:\n  level: debug\n  formatter: json\n  fields:\n    service: registry\n  hooks:\n    - type: mail\n      disabled: true\n      levels:\n        - panic\n      options:\n        smtp:\n          addr: mail.yy.com:25\n          username: xxx\n          password: pass\n          insecure: true\n        from: sender@yy.com\n        to:\n          - errors@yy.com\nstorage:\n    filesystem:\n        rootdirectory: /var/lib/registry\n    delete:\n      enabled: true\n    redirect:\n      disable: false\nhttp:\n    addr: :5000\n    headers:\n        X-Content-Type-Options: [nosniff]\nhealth:\n  storagedriver:\n    enabled: true\n    interval: 10s\n    threshold: 3\n```\n\n* 在docker registry 1 、2、3上分别执行如下命令\n\n```bash\n$ mkdir -p /var/lib/registry\n$ docker run -d -p 5000:5000 --net=host --restart=always -v /var/lib/pri_docker_registry/config.yml:/etc/docker/registry/config.yml -v /var/lib/registry:/var/lib/registry -v /etc/ceph/:/etc/ceph/ --name docker-registry registry:2.1\n```\n一定要将ceph的配置文件挂载进入容器，rados访问ceph需要\n* 配置iptables规则，只允许proxy机器访问\n\n```bash\n$ iptables -A INPUT -d 221.228.86.67/32 -m comment --comment \"not allow to be connected except proxy\" -j DROP\n$ iptables -I INPUT 1 -s 221.228.86.4 -d 221.228.86.67 -j ACCEPT\n$ iptables -I INPUT 1 -s 221.228.86.6 -d 221.228.86.67 -j ACCEPT\n# 添加办公网的访问\n$ iptables -I INPUT -s 183.60.177.224/27 -d 221.228.86.70/32 -j ACCEPT\n  # 配置221.228.86.6\n$ iptables -A INPUT -d 221.228.86.5/32 -m comment --comment \"not allow to be connected except proxy\" -j DROP\n$ iptables -I INPUT 1 -s 221.228.86.4 -d 221.228.86.5 -j ACCEPT\n$ iptables -I INPUT 1 -s 221.228.86.6 -d 221.228.86.5 -j ACCEPT\n$ iptables -I INPUT -s 183.60.177.224/27 -d 221.228.86.70/32 -j ACCEPT\n```\n\n## Haproxy 配置\n* 参考 https://hub.docker.com/_/haproxy/\n* 在haproxy 主备上分别执行下面命令,在/var/lib/hadporxy/目录下建立haproxy.cfg配置文件,内容如下:\n\n```bash\nglobal\n    log 127.0.0.1 local3 info\n    maxconn 4096\n    nbproc 1\n    pidfile /var/lib/haproxy/haproxy.pid\ndefaults\n    maxconn 2000\n    timeout connect 5000\n    timeout client 30000\n    timeout server 30000\n    mode http\n    stats uri /admin?stats\n    option forwardfor\nfrontend http_server\n    bind :5000\n    log global\n    default_backend docker-registry\n    #acl test hdr_dom(host) -i test.domain.com\n    #use_backend cache_test if test\nbackend docker-registry\n    #balance roundrobin\n    balance source\n    option httpchk GET /v2/ HTTP/1.1\\r\\nHost:221.228.86.6\n    server inst1 221.228.86.5:5000 check inter 5000 fall 3\n    server inst2 221.228.86.67:5000 check inter 5000 fall 3\n#HAProxy管理页面 \nlisten admin_stat\n    bind *:1158                    #管理页面端口\n    mode http                        \n    stats refresh 10s                #自动刷新时间\n    stats uri /haproxy                 #页面名称\n    stats realm Haproxy\\ Statistics     #登录提示\n    stats auth admin:admin    #帐号密码\n    stats hide-version \nstats admin if TRUE\n```\n\n#### 创建haproxy-registry容器\n\n```bash\n$ mkdir -p /var/lib/haproxy\n$ docker run -d --name haproxy-registry --net=host -v /var/lib/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro haproxy:1.5\n```\n#### 清除Iptables 规则，目前让所有机器都能访问该proxy\n\n```bash\n$ iptables -D INPUT -d 221.228.86.5/32 -m comment --comment sigma -j DROP\n```\n\n## Keepalived主备配置\n使用Keeplalived管理浮动IP\n\n* 在221.228.86.4 221.228.86.6机器上同时安装keepalived，keepalived配置文件区别设置\n#### Ubuntu Keepalived安装\n\n```bash\n$ apt-get install keepalived\n```\n#### Master（221.228.86.4）服务器配置 /etc/keepalived/keepalived.conf\n\n```bash\nglobal_defs {\n   notification_email {\n       xieyaoyao@yy.com\n   }\n   notification_email_from mail@example.org\n   smtp_server mail.yy.com\n   smtp_connect_timeout 30\n   router_id LVS_DEVEL\n}\n#监测haproxy进程状态，每2秒执行一次\nvrrp_script chk_haproxy {\n    script \"/usr/local/keepalived/chk_haproxy.sh\"\n    interval 2\n    weight 2\n}\nvrrp_instance VI_1 {\n    state MASTER #标示状态为MASTER\n    interface eth0\n    virtual_router_id 51\n    priority 101   #MASTER权重要高于BACKUP\n    advert_int 1\n    mcast_src_ip 221.228.86.4 #Master服务器IP\n    authentication {\n        auth_type PASS #主从服务器验证方式\n        auth_pass 1111\n    }\n    track_script {\n        chk_haproxy #监测haproxy进程状态\n    }\n    #VIP\n    virtual_ipaddress {\n        221.228.86.70 #虚拟IP\n    }\n}\n```\n#### Backup（221.228.86.6）服务器上的配置 /etc/keepalived/keepalived.conf\n\n```bash\nglobal_defs {\n   notification_email {\n   user@example.com\n   }\n   notification_email_from mail@example.org\n   smtp_server 192.168.x.x\n   smtp_connect_timeout 30\n   router_id LVS_DEVEL\n}\n#监测haproxy进程状态，每2秒执行一次\nvrrp_script chk_haproxy {\n    script \"/usr/local/keepalived/chk_haproxy.sh\"\n    interval 2\n    weight 2\n}\nvrrp_instance VI_1 {\n    state BACKUP #状态为BACKUP\n    interface eth0\n    virtual_router_id 51\n    priority 100  #权重要低于MASTER\n    advert_int 1\n    mcast_src_ip 221.228.86.6 #Backup服务器的IP\n    authentication {\n        auth_type PASS\n        auth_pass 1111\n    }\n    track_script {\n        chk_haproxy #监测haproxy进程状态\n    }\n    #VIP\n    virtual_ipaddress {\n        221.228.86.70 #虚拟IP\n    }\n}\n```\n#### chk_haproxy.sh内容 /usr/local/keepalived/chk_haproxy.sh\n\n```bash\n#!/bin/bash\n#\n# author: weizhifeng\n# description: \n# 定时查看haproxy是否存在，如果不存在则启动haproxy，\n# 如果启动失败，则停止keepalived\n# \nstatus=$(docker inspect  -f \"{{.State.Running}}\" haproxy-registry)\nif [ \"${status}\" = \"false\" ]; then\n    docker start haproxy-registry\n    status2=$(docker inspect  -f \"{{.State.Running}}\" haproxy-registry)\n    if [ \"${status2}\" = \"false\"  ]; then\n            service keepalived stop\n    fi\nfi\n```\n#### 启动keepalived服务\n\n```bash\n$ service keepalived start\n```\n\n\n\n\n\n\n\n\n\t\t\n","source":"_posts/highavailableregistry.md","raw":"title: 基于haproxy和keepalived的高可用的私有docker registry \ndate: 2015-11-04 22:02:11\ntags: [\"Docker\",\"Registry\"]\ncategories: [\"容器\",\"Docker\"]\n---\n\n本文用于搭建一个基于haproxy和keepalived的高可用的私有docker registry\n\n## 部署结构\n\n* haproxy keepalived 主：221.228.86.4\n* haproxy keepalived 备：221.228.86.5\n* docker registry 1：    221.228.86.6\n* docker registry 2：    221.228.86.67\n* VIP : 221.228.86.70 \n\n```bash\n                    221.228.86.100\n             +-----------VIP----------+   \n             |                        |\n             |                        |\n           Master                   Backup\n        221.228.86.4             221.228.86.6\n        +----------+             +----------+\n        | HAProxy  |             | HAProxy  |\n        |keepalived|             |keepalived|\n        +----------+             +----------+\n             |  \n             v  \n    +--------+---------+ \n    |        |         |\n    |        |         |\n    v        v         v\n+------+  +------+  +------+\n| WEB1 |  | WEB2 |  | WEB3 |\n+------+  +------+  +------+\n```\n\n<!-- more -->\n\n#### 修改docker daemon 启动参数 \n```bash\n修改/etc/default/docker文件，在DOCKER_OPTS=\"--insecure-registry vip:5000\"\n```\n\n## Docker Registry 配置\n* 参考 https://github.com/docker/distribution/blob/master/docs/configuration.md\n* 221.228.86.5 221.228.86.67\n* 在/var/lib/pri_docker_registry/目录下建立config.yml配置文件,内容如下：\n\n```bash\nversion: 0.1\nlog:\n  level: debug\n  formatter: json\n  fields:\n    service: registry\n  hooks:\n    - type: mail\n      disabled: true\n      levels:\n        - panic\n      options:\n        smtp:\n          addr: mail.yy.com:25\n          username: xxx\n          password: pass\n          insecure: true\n        from: sender@yy.com\n        to:\n          - errors@yy.com\nstorage:\n    filesystem:\n        rootdirectory: /var/lib/registry\n    delete:\n      enabled: true\n    redirect:\n      disable: false\nhttp:\n    addr: :5000\n    headers:\n        X-Content-Type-Options: [nosniff]\nhealth:\n  storagedriver:\n    enabled: true\n    interval: 10s\n    threshold: 3\n```\n\n* 在docker registry 1 、2、3上分别执行如下命令\n\n```bash\n$ mkdir -p /var/lib/registry\n$ docker run -d -p 5000:5000 --net=host --restart=always -v /var/lib/pri_docker_registry/config.yml:/etc/docker/registry/config.yml -v /var/lib/registry:/var/lib/registry -v /etc/ceph/:/etc/ceph/ --name docker-registry registry:2.1\n```\n一定要将ceph的配置文件挂载进入容器，rados访问ceph需要\n* 配置iptables规则，只允许proxy机器访问\n\n```bash\n$ iptables -A INPUT -d 221.228.86.67/32 -m comment --comment \"not allow to be connected except proxy\" -j DROP\n$ iptables -I INPUT 1 -s 221.228.86.4 -d 221.228.86.67 -j ACCEPT\n$ iptables -I INPUT 1 -s 221.228.86.6 -d 221.228.86.67 -j ACCEPT\n# 添加办公网的访问\n$ iptables -I INPUT -s 183.60.177.224/27 -d 221.228.86.70/32 -j ACCEPT\n  # 配置221.228.86.6\n$ iptables -A INPUT -d 221.228.86.5/32 -m comment --comment \"not allow to be connected except proxy\" -j DROP\n$ iptables -I INPUT 1 -s 221.228.86.4 -d 221.228.86.5 -j ACCEPT\n$ iptables -I INPUT 1 -s 221.228.86.6 -d 221.228.86.5 -j ACCEPT\n$ iptables -I INPUT -s 183.60.177.224/27 -d 221.228.86.70/32 -j ACCEPT\n```\n\n## Haproxy 配置\n* 参考 https://hub.docker.com/_/haproxy/\n* 在haproxy 主备上分别执行下面命令,在/var/lib/hadporxy/目录下建立haproxy.cfg配置文件,内容如下:\n\n```bash\nglobal\n    log 127.0.0.1 local3 info\n    maxconn 4096\n    nbproc 1\n    pidfile /var/lib/haproxy/haproxy.pid\ndefaults\n    maxconn 2000\n    timeout connect 5000\n    timeout client 30000\n    timeout server 30000\n    mode http\n    stats uri /admin?stats\n    option forwardfor\nfrontend http_server\n    bind :5000\n    log global\n    default_backend docker-registry\n    #acl test hdr_dom(host) -i test.domain.com\n    #use_backend cache_test if test\nbackend docker-registry\n    #balance roundrobin\n    balance source\n    option httpchk GET /v2/ HTTP/1.1\\r\\nHost:221.228.86.6\n    server inst1 221.228.86.5:5000 check inter 5000 fall 3\n    server inst2 221.228.86.67:5000 check inter 5000 fall 3\n#HAProxy管理页面 \nlisten admin_stat\n    bind *:1158                    #管理页面端口\n    mode http                        \n    stats refresh 10s                #自动刷新时间\n    stats uri /haproxy                 #页面名称\n    stats realm Haproxy\\ Statistics     #登录提示\n    stats auth admin:admin    #帐号密码\n    stats hide-version \nstats admin if TRUE\n```\n\n#### 创建haproxy-registry容器\n\n```bash\n$ mkdir -p /var/lib/haproxy\n$ docker run -d --name haproxy-registry --net=host -v /var/lib/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro haproxy:1.5\n```\n#### 清除Iptables 规则，目前让所有机器都能访问该proxy\n\n```bash\n$ iptables -D INPUT -d 221.228.86.5/32 -m comment --comment sigma -j DROP\n```\n\n## Keepalived主备配置\n使用Keeplalived管理浮动IP\n\n* 在221.228.86.4 221.228.86.6机器上同时安装keepalived，keepalived配置文件区别设置\n#### Ubuntu Keepalived安装\n\n```bash\n$ apt-get install keepalived\n```\n#### Master（221.228.86.4）服务器配置 /etc/keepalived/keepalived.conf\n\n```bash\nglobal_defs {\n   notification_email {\n       xieyaoyao@yy.com\n   }\n   notification_email_from mail@example.org\n   smtp_server mail.yy.com\n   smtp_connect_timeout 30\n   router_id LVS_DEVEL\n}\n#监测haproxy进程状态，每2秒执行一次\nvrrp_script chk_haproxy {\n    script \"/usr/local/keepalived/chk_haproxy.sh\"\n    interval 2\n    weight 2\n}\nvrrp_instance VI_1 {\n    state MASTER #标示状态为MASTER\n    interface eth0\n    virtual_router_id 51\n    priority 101   #MASTER权重要高于BACKUP\n    advert_int 1\n    mcast_src_ip 221.228.86.4 #Master服务器IP\n    authentication {\n        auth_type PASS #主从服务器验证方式\n        auth_pass 1111\n    }\n    track_script {\n        chk_haproxy #监测haproxy进程状态\n    }\n    #VIP\n    virtual_ipaddress {\n        221.228.86.70 #虚拟IP\n    }\n}\n```\n#### Backup（221.228.86.6）服务器上的配置 /etc/keepalived/keepalived.conf\n\n```bash\nglobal_defs {\n   notification_email {\n   user@example.com\n   }\n   notification_email_from mail@example.org\n   smtp_server 192.168.x.x\n   smtp_connect_timeout 30\n   router_id LVS_DEVEL\n}\n#监测haproxy进程状态，每2秒执行一次\nvrrp_script chk_haproxy {\n    script \"/usr/local/keepalived/chk_haproxy.sh\"\n    interval 2\n    weight 2\n}\nvrrp_instance VI_1 {\n    state BACKUP #状态为BACKUP\n    interface eth0\n    virtual_router_id 51\n    priority 100  #权重要低于MASTER\n    advert_int 1\n    mcast_src_ip 221.228.86.6 #Backup服务器的IP\n    authentication {\n        auth_type PASS\n        auth_pass 1111\n    }\n    track_script {\n        chk_haproxy #监测haproxy进程状态\n    }\n    #VIP\n    virtual_ipaddress {\n        221.228.86.70 #虚拟IP\n    }\n}\n```\n#### chk_haproxy.sh内容 /usr/local/keepalived/chk_haproxy.sh\n\n```bash\n#!/bin/bash\n#\n# author: weizhifeng\n# description: \n# 定时查看haproxy是否存在，如果不存在则启动haproxy，\n# 如果启动失败，则停止keepalived\n# \nstatus=$(docker inspect  -f \"{{.State.Running}}\" haproxy-registry)\nif [ \"${status}\" = \"false\" ]; then\n    docker start haproxy-registry\n    status2=$(docker inspect  -f \"{{.State.Running}}\" haproxy-registry)\n    if [ \"${status2}\" = \"false\"  ]; then\n            service keepalived stop\n    fi\nfi\n```\n#### 启动keepalived服务\n\n```bash\n$ service keepalived start\n```\n\n\n\n\n\n\n\n\n\t\t\n","slug":"highavailableregistry","published":1,"updated":"2022-06-08T02:09:14.806Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4507g6k0003jjrh0irda8vc","content":"<p>本文用于搭建一个基于haproxy和keepalived的高可用的私有docker registry</p>\n<h2 id=\"部署结构\"><a href=\"#部署结构\" class=\"headerlink\" title=\"部署结构\"></a>部署结构</h2><ul>\n<li>haproxy keepalived 主：221.228.86.4</li>\n<li>haproxy keepalived 备：221.228.86.5</li>\n<li>docker registry 1：    221.228.86.6</li>\n<li>docker registry 2：    221.228.86.67</li>\n<li>VIP : 221.228.86.70</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">                    221.228.86.100</span><br><span class=\"line\">             +-----------VIP----------+   </span><br><span class=\"line\">             |                        |</span><br><span class=\"line\">             |                        |</span><br><span class=\"line\">           Master                   Backup</span><br><span class=\"line\">        221.228.86.4             221.228.86.6</span><br><span class=\"line\">        +----------+             +----------+</span><br><span class=\"line\">        | HAProxy  |             | HAProxy  |</span><br><span class=\"line\">        |keepalived|             |keepalived|</span><br><span class=\"line\">        +----------+             +----------+</span><br><span class=\"line\">             |  </span><br><span class=\"line\">             v  </span><br><span class=\"line\">    +--------+---------+ </span><br><span class=\"line\">    |        |         |</span><br><span class=\"line\">    |        |         |</span><br><span class=\"line\">    v        v         v</span><br><span class=\"line\">+------+  +------+  +------+</span><br><span class=\"line\">| WEB1 |  | WEB2 |  | WEB3 |</span><br><span class=\"line\">+------+  +------+  +------+</span><br></pre></td></tr></table></figure>\n\n<span id=\"more\"></span>\n\n<h4 id=\"修改docker-daemon-启动参数\"><a href=\"#修改docker-daemon-启动参数\" class=\"headerlink\" title=\"修改docker daemon 启动参数\"></a>修改docker daemon 启动参数</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">修改/etc/default/docker文件，在DOCKER_OPTS=<span class=\"string\">&quot;--insecure-registry vip:5000&quot;</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Docker-Registry-配置\"><a href=\"#Docker-Registry-配置\" class=\"headerlink\" title=\"Docker Registry 配置\"></a>Docker Registry 配置</h2><ul>\n<li>参考 <a href=\"https://github.com/docker/distribution/blob/master/docs/configuration.md\">https://github.com/docker/distribution/blob/master/docs/configuration.md</a></li>\n<li>221.228.86.5 221.228.86.67</li>\n<li>在&#x2F;var&#x2F;lib&#x2F;pri_docker_registry&#x2F;目录下建立config.yml配置文件,内容如下：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: 0.1</span><br><span class=\"line\"><span class=\"built_in\">log</span>:</span><br><span class=\"line\">  level: debug</span><br><span class=\"line\">  formatter: json</span><br><span class=\"line\">  fields:</span><br><span class=\"line\">    service: registry</span><br><span class=\"line\">  hooks:</span><br><span class=\"line\">    - <span class=\"built_in\">type</span>: mail</span><br><span class=\"line\">      disabled: <span class=\"literal\">true</span></span><br><span class=\"line\">      levels:</span><br><span class=\"line\">        - panic</span><br><span class=\"line\">      options:</span><br><span class=\"line\">        smtp:</span><br><span class=\"line\">          addr: mail.yy.com:25</span><br><span class=\"line\">          username: xxx</span><br><span class=\"line\">          password: pass</span><br><span class=\"line\">          insecure: <span class=\"literal\">true</span></span><br><span class=\"line\">        from: sender@yy.com</span><br><span class=\"line\">        to:</span><br><span class=\"line\">          - errors@yy.com</span><br><span class=\"line\">storage:</span><br><span class=\"line\">    filesystem:</span><br><span class=\"line\">        rootdirectory: /var/lib/registry</span><br><span class=\"line\">    delete:</span><br><span class=\"line\">      enabled: <span class=\"literal\">true</span></span><br><span class=\"line\">    redirect:</span><br><span class=\"line\">      <span class=\"built_in\">disable</span>: <span class=\"literal\">false</span></span><br><span class=\"line\">http:</span><br><span class=\"line\">    addr: :5000</span><br><span class=\"line\">    headers:</span><br><span class=\"line\">        X-Content-Type-Options: [nosniff]</span><br><span class=\"line\">health:</span><br><span class=\"line\">  storagedriver:</span><br><span class=\"line\">    enabled: <span class=\"literal\">true</span></span><br><span class=\"line\">    interval: 10s</span><br><span class=\"line\">    threshold: 3</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>在docker registry 1 、2、3上分别执行如下命令</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">mkdir</span> -p /var/lib/registry</span><br><span class=\"line\">$ docker run -d -p 5000:5000 --net=host --restart=always -v /var/lib/pri_docker_registry/config.yml:/etc/docker/registry/config.yml -v /var/lib/registry:/var/lib/registry -v /etc/ceph/:/etc/ceph/ --name docker-registry registry:2.1</span><br></pre></td></tr></table></figure>\n<p>一定要将ceph的配置文件挂载进入容器，rados访问ceph需要</p>\n<ul>\n<li>配置iptables规则，只允许proxy机器访问</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ iptables -A INPUT -d 221.228.86.67/32 -m comment --comment <span class=\"string\">&quot;not allow to be connected except proxy&quot;</span> -j DROP</span><br><span class=\"line\">$ iptables -I INPUT 1 -s 221.228.86.4 -d 221.228.86.67 -j ACCEPT</span><br><span class=\"line\">$ iptables -I INPUT 1 -s 221.228.86.6 -d 221.228.86.67 -j ACCEPT</span><br><span class=\"line\"><span class=\"comment\"># 添加办公网的访问</span></span><br><span class=\"line\">$ iptables -I INPUT -s 183.60.177.224/27 -d 221.228.86.70/32 -j ACCEPT</span><br><span class=\"line\">  <span class=\"comment\"># 配置221.228.86.6</span></span><br><span class=\"line\">$ iptables -A INPUT -d 221.228.86.5/32 -m comment --comment <span class=\"string\">&quot;not allow to be connected except proxy&quot;</span> -j DROP</span><br><span class=\"line\">$ iptables -I INPUT 1 -s 221.228.86.4 -d 221.228.86.5 -j ACCEPT</span><br><span class=\"line\">$ iptables -I INPUT 1 -s 221.228.86.6 -d 221.228.86.5 -j ACCEPT</span><br><span class=\"line\">$ iptables -I INPUT -s 183.60.177.224/27 -d 221.228.86.70/32 -j ACCEPT</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Haproxy-配置\"><a href=\"#Haproxy-配置\" class=\"headerlink\" title=\"Haproxy 配置\"></a>Haproxy 配置</h2><ul>\n<li>参考 <a href=\"https://hub.docker.com/_/haproxy/\">https://hub.docker.com/_/haproxy/</a></li>\n<li>在haproxy 主备上分别执行下面命令,在&#x2F;var&#x2F;lib&#x2F;hadporxy&#x2F;目录下建立haproxy.cfg配置文件,内容如下:</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">global</span><br><span class=\"line\">    <span class=\"built_in\">log</span> 127.0.0.1 local3 info</span><br><span class=\"line\">    maxconn 4096</span><br><span class=\"line\">    nbproc 1</span><br><span class=\"line\">    pidfile /var/lib/haproxy/haproxy.pid</span><br><span class=\"line\">defaults</span><br><span class=\"line\">    maxconn 2000</span><br><span class=\"line\">    <span class=\"built_in\">timeout</span> connect 5000</span><br><span class=\"line\">    <span class=\"built_in\">timeout</span> client 30000</span><br><span class=\"line\">    <span class=\"built_in\">timeout</span> server 30000</span><br><span class=\"line\">    mode http</span><br><span class=\"line\">    stats uri /admin?stats</span><br><span class=\"line\">    option forwardfor</span><br><span class=\"line\">frontend http_server</span><br><span class=\"line\">    <span class=\"built_in\">bind</span> :5000</span><br><span class=\"line\">    <span class=\"built_in\">log</span> global</span><br><span class=\"line\">    default_backend docker-registry</span><br><span class=\"line\">    <span class=\"comment\">#acl test hdr_dom(host) -i test.domain.com</span></span><br><span class=\"line\">    <span class=\"comment\">#use_backend cache_test if test</span></span><br><span class=\"line\">backend docker-registry</span><br><span class=\"line\">    <span class=\"comment\">#balance roundrobin</span></span><br><span class=\"line\">    balance <span class=\"built_in\">source</span></span><br><span class=\"line\">    option httpchk GET /v2/ HTTP/1.1\\r\\nHost:221.228.86.6</span><br><span class=\"line\">    server inst1 221.228.86.5:5000 check inter 5000 fall 3</span><br><span class=\"line\">    server inst2 221.228.86.67:5000 check inter 5000 fall 3</span><br><span class=\"line\"><span class=\"comment\">#HAProxy管理页面 </span></span><br><span class=\"line\">listen admin_stat</span><br><span class=\"line\">    <span class=\"built_in\">bind</span> *:1158                    <span class=\"comment\">#管理页面端口</span></span><br><span class=\"line\">    mode http                        </span><br><span class=\"line\">    stats refresh 10s                <span class=\"comment\">#自动刷新时间</span></span><br><span class=\"line\">    stats uri /haproxy                 <span class=\"comment\">#页面名称</span></span><br><span class=\"line\">    stats realm Haproxy\\ Statistics     <span class=\"comment\">#登录提示</span></span><br><span class=\"line\">    stats auth admin:admin    <span class=\"comment\">#帐号密码</span></span><br><span class=\"line\">    stats hide-version </span><br><span class=\"line\">stats admin <span class=\"keyword\">if</span> TRUE</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"创建haproxy-registry容器\"><a href=\"#创建haproxy-registry容器\" class=\"headerlink\" title=\"创建haproxy-registry容器\"></a>创建haproxy-registry容器</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">mkdir</span> -p /var/lib/haproxy</span><br><span class=\"line\">$ docker run -d --name haproxy-registry --net=host -v /var/lib/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro haproxy:1.5</span><br></pre></td></tr></table></figure>\n<h4 id=\"清除Iptables-规则，目前让所有机器都能访问该proxy\"><a href=\"#清除Iptables-规则，目前让所有机器都能访问该proxy\" class=\"headerlink\" title=\"清除Iptables 规则，目前让所有机器都能访问该proxy\"></a>清除Iptables 规则，目前让所有机器都能访问该proxy</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ iptables -D INPUT -d 221.228.86.5/32 -m comment --comment sigma -j DROP</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Keepalived主备配置\"><a href=\"#Keepalived主备配置\" class=\"headerlink\" title=\"Keepalived主备配置\"></a>Keepalived主备配置</h2><p>使用Keeplalived管理浮动IP</p>\n<ul>\n<li>在221.228.86.4 221.228.86.6机器上同时安装keepalived，keepalived配置文件区别设置<h4 id=\"Ubuntu-Keepalived安装\"><a href=\"#Ubuntu-Keepalived安装\" class=\"headerlink\" title=\"Ubuntu Keepalived安装\"></a>Ubuntu Keepalived安装</h4></li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ apt-get install keepalived</span><br></pre></td></tr></table></figure>\n<h4 id=\"Master（221-228-86-4）服务器配置-x2F-etc-x2F-keepalived-x2F-keepalived-conf\"><a href=\"#Master（221-228-86-4）服务器配置-x2F-etc-x2F-keepalived-x2F-keepalived-conf\" class=\"headerlink\" title=\"Master（221.228.86.4）服务器配置 &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf\"></a>Master（221.228.86.4）服务器配置 &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">global_defs &#123;</span><br><span class=\"line\">   notification_email &#123;</span><br><span class=\"line\">       xieyaoyao@yy.com</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   notification_email_from mail@example.org</span><br><span class=\"line\">   smtp_server mail.yy.com</span><br><span class=\"line\">   smtp_connect_timeout 30</span><br><span class=\"line\">   router_id LVS_DEVEL</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">#监测haproxy进程状态，每2秒执行一次</span></span><br><span class=\"line\">vrrp_script chk_haproxy &#123;</span><br><span class=\"line\">    script <span class=\"string\">&quot;/usr/local/keepalived/chk_haproxy.sh&quot;</span></span><br><span class=\"line\">    interval 2</span><br><span class=\"line\">    weight 2</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">vrrp_instance VI_1 &#123;</span><br><span class=\"line\">    state MASTER <span class=\"comment\">#标示状态为MASTER</span></span><br><span class=\"line\">    interface eth0</span><br><span class=\"line\">    virtual_router_id 51</span><br><span class=\"line\">    priority 101   <span class=\"comment\">#MASTER权重要高于BACKUP</span></span><br><span class=\"line\">    advert_int 1</span><br><span class=\"line\">    mcast_src_ip 221.228.86.4 <span class=\"comment\">#Master服务器IP</span></span><br><span class=\"line\">    authentication &#123;</span><br><span class=\"line\">        auth_type PASS <span class=\"comment\">#主从服务器验证方式</span></span><br><span class=\"line\">        auth_pass 1111</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    track_script &#123;</span><br><span class=\"line\">        chk_haproxy <span class=\"comment\">#监测haproxy进程状态</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">#VIP</span></span><br><span class=\"line\">    virtual_ipaddress &#123;</span><br><span class=\"line\">        221.228.86.70 <span class=\"comment\">#虚拟IP</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Backup（221-228-86-6）服务器上的配置-x2F-etc-x2F-keepalived-x2F-keepalived-conf\"><a href=\"#Backup（221-228-86-6）服务器上的配置-x2F-etc-x2F-keepalived-x2F-keepalived-conf\" class=\"headerlink\" title=\"Backup（221.228.86.6）服务器上的配置 &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf\"></a>Backup（221.228.86.6）服务器上的配置 &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">global_defs &#123;</span><br><span class=\"line\">   notification_email &#123;</span><br><span class=\"line\">   user@example.com</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   notification_email_from mail@example.org</span><br><span class=\"line\">   smtp_server 192.168.x.x</span><br><span class=\"line\">   smtp_connect_timeout 30</span><br><span class=\"line\">   router_id LVS_DEVEL</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">#监测haproxy进程状态，每2秒执行一次</span></span><br><span class=\"line\">vrrp_script chk_haproxy &#123;</span><br><span class=\"line\">    script <span class=\"string\">&quot;/usr/local/keepalived/chk_haproxy.sh&quot;</span></span><br><span class=\"line\">    interval 2</span><br><span class=\"line\">    weight 2</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">vrrp_instance VI_1 &#123;</span><br><span class=\"line\">    state BACKUP <span class=\"comment\">#状态为BACKUP</span></span><br><span class=\"line\">    interface eth0</span><br><span class=\"line\">    virtual_router_id 51</span><br><span class=\"line\">    priority 100  <span class=\"comment\">#权重要低于MASTER</span></span><br><span class=\"line\">    advert_int 1</span><br><span class=\"line\">    mcast_src_ip 221.228.86.6 <span class=\"comment\">#Backup服务器的IP</span></span><br><span class=\"line\">    authentication &#123;</span><br><span class=\"line\">        auth_type PASS</span><br><span class=\"line\">        auth_pass 1111</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    track_script &#123;</span><br><span class=\"line\">        chk_haproxy <span class=\"comment\">#监测haproxy进程状态</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">#VIP</span></span><br><span class=\"line\">    virtual_ipaddress &#123;</span><br><span class=\"line\">        221.228.86.70 <span class=\"comment\">#虚拟IP</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"chk-haproxy-sh内容-x2F-usr-x2F-local-x2F-keepalived-x2F-chk-haproxy-sh\"><a href=\"#chk-haproxy-sh内容-x2F-usr-x2F-local-x2F-keepalived-x2F-chk-haproxy-sh\" class=\"headerlink\" title=\"chk_haproxy.sh内容 &#x2F;usr&#x2F;local&#x2F;keepalived&#x2F;chk_haproxy.sh\"></a>chk_haproxy.sh内容 &#x2F;usr&#x2F;local&#x2F;keepalived&#x2F;chk_haproxy.sh</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># author: weizhifeng</span></span><br><span class=\"line\"><span class=\"comment\"># description: </span></span><br><span class=\"line\"><span class=\"comment\"># 定时查看haproxy是否存在，如果不存在则启动haproxy，</span></span><br><span class=\"line\"><span class=\"comment\"># 如果启动失败，则停止keepalived</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">status=$(docker inspect  -f <span class=\"string\">&quot;&#123;&#123;.State.Running&#125;&#125;&quot;</span> haproxy-registry)</span><br><span class=\"line\"><span class=\"keyword\">if</span> [ <span class=\"string\">&quot;<span class=\"variable\">$&#123;status&#125;</span>&quot;</span> = <span class=\"string\">&quot;false&quot;</span> ]; <span class=\"keyword\">then</span></span><br><span class=\"line\">    docker start haproxy-registry</span><br><span class=\"line\">    status2=$(docker inspect  -f <span class=\"string\">&quot;&#123;&#123;.State.Running&#125;&#125;&quot;</span> haproxy-registry)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> [ <span class=\"string\">&quot;<span class=\"variable\">$&#123;status2&#125;</span>&quot;</span> = <span class=\"string\">&quot;false&quot;</span>  ]; <span class=\"keyword\">then</span></span><br><span class=\"line\">            service keepalived stop</span><br><span class=\"line\">    <span class=\"keyword\">fi</span></span><br><span class=\"line\"><span class=\"keyword\">fi</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"启动keepalived服务\"><a href=\"#启动keepalived服务\" class=\"headerlink\" title=\"启动keepalived服务\"></a>启动keepalived服务</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ service keepalived start</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n\n\n\n","site":{"data":{}},"excerpt":"<p>本文用于搭建一个基于haproxy和keepalived的高可用的私有docker registry</p>\n<h2 id=\"部署结构\"><a href=\"#部署结构\" class=\"headerlink\" title=\"部署结构\"></a>部署结构</h2><ul>\n<li>haproxy keepalived 主：221.228.86.4</li>\n<li>haproxy keepalived 备：221.228.86.5</li>\n<li>docker registry 1：    221.228.86.6</li>\n<li>docker registry 2：    221.228.86.67</li>\n<li>VIP : 221.228.86.70</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">                    221.228.86.100</span><br><span class=\"line\">             +-----------VIP----------+   </span><br><span class=\"line\">             |                        |</span><br><span class=\"line\">             |                        |</span><br><span class=\"line\">           Master                   Backup</span><br><span class=\"line\">        221.228.86.4             221.228.86.6</span><br><span class=\"line\">        +----------+             +----------+</span><br><span class=\"line\">        | HAProxy  |             | HAProxy  |</span><br><span class=\"line\">        |keepalived|             |keepalived|</span><br><span class=\"line\">        +----------+             +----------+</span><br><span class=\"line\">             |  </span><br><span class=\"line\">             v  </span><br><span class=\"line\">    +--------+---------+ </span><br><span class=\"line\">    |        |         |</span><br><span class=\"line\">    |        |         |</span><br><span class=\"line\">    v        v         v</span><br><span class=\"line\">+------+  +------+  +------+</span><br><span class=\"line\">| WEB1 |  | WEB2 |  | WEB3 |</span><br><span class=\"line\">+------+  +------+  +------+</span><br></pre></td></tr></table></figure>","more":"<h4 id=\"修改docker-daemon-启动参数\"><a href=\"#修改docker-daemon-启动参数\" class=\"headerlink\" title=\"修改docker daemon 启动参数\"></a>修改docker daemon 启动参数</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">修改/etc/default/docker文件，在DOCKER_OPTS=<span class=\"string\">&quot;--insecure-registry vip:5000&quot;</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Docker-Registry-配置\"><a href=\"#Docker-Registry-配置\" class=\"headerlink\" title=\"Docker Registry 配置\"></a>Docker Registry 配置</h2><ul>\n<li>参考 <a href=\"https://github.com/docker/distribution/blob/master/docs/configuration.md\">https://github.com/docker/distribution/blob/master/docs/configuration.md</a></li>\n<li>221.228.86.5 221.228.86.67</li>\n<li>在&#x2F;var&#x2F;lib&#x2F;pri_docker_registry&#x2F;目录下建立config.yml配置文件,内容如下：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: 0.1</span><br><span class=\"line\"><span class=\"built_in\">log</span>:</span><br><span class=\"line\">  level: debug</span><br><span class=\"line\">  formatter: json</span><br><span class=\"line\">  fields:</span><br><span class=\"line\">    service: registry</span><br><span class=\"line\">  hooks:</span><br><span class=\"line\">    - <span class=\"built_in\">type</span>: mail</span><br><span class=\"line\">      disabled: <span class=\"literal\">true</span></span><br><span class=\"line\">      levels:</span><br><span class=\"line\">        - panic</span><br><span class=\"line\">      options:</span><br><span class=\"line\">        smtp:</span><br><span class=\"line\">          addr: mail.yy.com:25</span><br><span class=\"line\">          username: xxx</span><br><span class=\"line\">          password: pass</span><br><span class=\"line\">          insecure: <span class=\"literal\">true</span></span><br><span class=\"line\">        from: sender@yy.com</span><br><span class=\"line\">        to:</span><br><span class=\"line\">          - errors@yy.com</span><br><span class=\"line\">storage:</span><br><span class=\"line\">    filesystem:</span><br><span class=\"line\">        rootdirectory: /var/lib/registry</span><br><span class=\"line\">    delete:</span><br><span class=\"line\">      enabled: <span class=\"literal\">true</span></span><br><span class=\"line\">    redirect:</span><br><span class=\"line\">      <span class=\"built_in\">disable</span>: <span class=\"literal\">false</span></span><br><span class=\"line\">http:</span><br><span class=\"line\">    addr: :5000</span><br><span class=\"line\">    headers:</span><br><span class=\"line\">        X-Content-Type-Options: [nosniff]</span><br><span class=\"line\">health:</span><br><span class=\"line\">  storagedriver:</span><br><span class=\"line\">    enabled: <span class=\"literal\">true</span></span><br><span class=\"line\">    interval: 10s</span><br><span class=\"line\">    threshold: 3</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>在docker registry 1 、2、3上分别执行如下命令</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">mkdir</span> -p /var/lib/registry</span><br><span class=\"line\">$ docker run -d -p 5000:5000 --net=host --restart=always -v /var/lib/pri_docker_registry/config.yml:/etc/docker/registry/config.yml -v /var/lib/registry:/var/lib/registry -v /etc/ceph/:/etc/ceph/ --name docker-registry registry:2.1</span><br></pre></td></tr></table></figure>\n<p>一定要将ceph的配置文件挂载进入容器，rados访问ceph需要</p>\n<ul>\n<li>配置iptables规则，只允许proxy机器访问</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ iptables -A INPUT -d 221.228.86.67/32 -m comment --comment <span class=\"string\">&quot;not allow to be connected except proxy&quot;</span> -j DROP</span><br><span class=\"line\">$ iptables -I INPUT 1 -s 221.228.86.4 -d 221.228.86.67 -j ACCEPT</span><br><span class=\"line\">$ iptables -I INPUT 1 -s 221.228.86.6 -d 221.228.86.67 -j ACCEPT</span><br><span class=\"line\"><span class=\"comment\"># 添加办公网的访问</span></span><br><span class=\"line\">$ iptables -I INPUT -s 183.60.177.224/27 -d 221.228.86.70/32 -j ACCEPT</span><br><span class=\"line\">  <span class=\"comment\"># 配置221.228.86.6</span></span><br><span class=\"line\">$ iptables -A INPUT -d 221.228.86.5/32 -m comment --comment <span class=\"string\">&quot;not allow to be connected except proxy&quot;</span> -j DROP</span><br><span class=\"line\">$ iptables -I INPUT 1 -s 221.228.86.4 -d 221.228.86.5 -j ACCEPT</span><br><span class=\"line\">$ iptables -I INPUT 1 -s 221.228.86.6 -d 221.228.86.5 -j ACCEPT</span><br><span class=\"line\">$ iptables -I INPUT -s 183.60.177.224/27 -d 221.228.86.70/32 -j ACCEPT</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Haproxy-配置\"><a href=\"#Haproxy-配置\" class=\"headerlink\" title=\"Haproxy 配置\"></a>Haproxy 配置</h2><ul>\n<li>参考 <a href=\"https://hub.docker.com/_/haproxy/\">https://hub.docker.com/_/haproxy/</a></li>\n<li>在haproxy 主备上分别执行下面命令,在&#x2F;var&#x2F;lib&#x2F;hadporxy&#x2F;目录下建立haproxy.cfg配置文件,内容如下:</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">global</span><br><span class=\"line\">    <span class=\"built_in\">log</span> 127.0.0.1 local3 info</span><br><span class=\"line\">    maxconn 4096</span><br><span class=\"line\">    nbproc 1</span><br><span class=\"line\">    pidfile /var/lib/haproxy/haproxy.pid</span><br><span class=\"line\">defaults</span><br><span class=\"line\">    maxconn 2000</span><br><span class=\"line\">    <span class=\"built_in\">timeout</span> connect 5000</span><br><span class=\"line\">    <span class=\"built_in\">timeout</span> client 30000</span><br><span class=\"line\">    <span class=\"built_in\">timeout</span> server 30000</span><br><span class=\"line\">    mode http</span><br><span class=\"line\">    stats uri /admin?stats</span><br><span class=\"line\">    option forwardfor</span><br><span class=\"line\">frontend http_server</span><br><span class=\"line\">    <span class=\"built_in\">bind</span> :5000</span><br><span class=\"line\">    <span class=\"built_in\">log</span> global</span><br><span class=\"line\">    default_backend docker-registry</span><br><span class=\"line\">    <span class=\"comment\">#acl test hdr_dom(host) -i test.domain.com</span></span><br><span class=\"line\">    <span class=\"comment\">#use_backend cache_test if test</span></span><br><span class=\"line\">backend docker-registry</span><br><span class=\"line\">    <span class=\"comment\">#balance roundrobin</span></span><br><span class=\"line\">    balance <span class=\"built_in\">source</span></span><br><span class=\"line\">    option httpchk GET /v2/ HTTP/1.1\\r\\nHost:221.228.86.6</span><br><span class=\"line\">    server inst1 221.228.86.5:5000 check inter 5000 fall 3</span><br><span class=\"line\">    server inst2 221.228.86.67:5000 check inter 5000 fall 3</span><br><span class=\"line\"><span class=\"comment\">#HAProxy管理页面 </span></span><br><span class=\"line\">listen admin_stat</span><br><span class=\"line\">    <span class=\"built_in\">bind</span> *:1158                    <span class=\"comment\">#管理页面端口</span></span><br><span class=\"line\">    mode http                        </span><br><span class=\"line\">    stats refresh 10s                <span class=\"comment\">#自动刷新时间</span></span><br><span class=\"line\">    stats uri /haproxy                 <span class=\"comment\">#页面名称</span></span><br><span class=\"line\">    stats realm Haproxy\\ Statistics     <span class=\"comment\">#登录提示</span></span><br><span class=\"line\">    stats auth admin:admin    <span class=\"comment\">#帐号密码</span></span><br><span class=\"line\">    stats hide-version </span><br><span class=\"line\">stats admin <span class=\"keyword\">if</span> TRUE</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"创建haproxy-registry容器\"><a href=\"#创建haproxy-registry容器\" class=\"headerlink\" title=\"创建haproxy-registry容器\"></a>创建haproxy-registry容器</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">mkdir</span> -p /var/lib/haproxy</span><br><span class=\"line\">$ docker run -d --name haproxy-registry --net=host -v /var/lib/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro haproxy:1.5</span><br></pre></td></tr></table></figure>\n<h4 id=\"清除Iptables-规则，目前让所有机器都能访问该proxy\"><a href=\"#清除Iptables-规则，目前让所有机器都能访问该proxy\" class=\"headerlink\" title=\"清除Iptables 规则，目前让所有机器都能访问该proxy\"></a>清除Iptables 规则，目前让所有机器都能访问该proxy</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ iptables -D INPUT -d 221.228.86.5/32 -m comment --comment sigma -j DROP</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Keepalived主备配置\"><a href=\"#Keepalived主备配置\" class=\"headerlink\" title=\"Keepalived主备配置\"></a>Keepalived主备配置</h2><p>使用Keeplalived管理浮动IP</p>\n<ul>\n<li>在221.228.86.4 221.228.86.6机器上同时安装keepalived，keepalived配置文件区别设置<h4 id=\"Ubuntu-Keepalived安装\"><a href=\"#Ubuntu-Keepalived安装\" class=\"headerlink\" title=\"Ubuntu Keepalived安装\"></a>Ubuntu Keepalived安装</h4></li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ apt-get install keepalived</span><br></pre></td></tr></table></figure>\n<h4 id=\"Master（221-228-86-4）服务器配置-x2F-etc-x2F-keepalived-x2F-keepalived-conf\"><a href=\"#Master（221-228-86-4）服务器配置-x2F-etc-x2F-keepalived-x2F-keepalived-conf\" class=\"headerlink\" title=\"Master（221.228.86.4）服务器配置 &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf\"></a>Master（221.228.86.4）服务器配置 &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">global_defs &#123;</span><br><span class=\"line\">   notification_email &#123;</span><br><span class=\"line\">       xieyaoyao@yy.com</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   notification_email_from mail@example.org</span><br><span class=\"line\">   smtp_server mail.yy.com</span><br><span class=\"line\">   smtp_connect_timeout 30</span><br><span class=\"line\">   router_id LVS_DEVEL</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">#监测haproxy进程状态，每2秒执行一次</span></span><br><span class=\"line\">vrrp_script chk_haproxy &#123;</span><br><span class=\"line\">    script <span class=\"string\">&quot;/usr/local/keepalived/chk_haproxy.sh&quot;</span></span><br><span class=\"line\">    interval 2</span><br><span class=\"line\">    weight 2</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">vrrp_instance VI_1 &#123;</span><br><span class=\"line\">    state MASTER <span class=\"comment\">#标示状态为MASTER</span></span><br><span class=\"line\">    interface eth0</span><br><span class=\"line\">    virtual_router_id 51</span><br><span class=\"line\">    priority 101   <span class=\"comment\">#MASTER权重要高于BACKUP</span></span><br><span class=\"line\">    advert_int 1</span><br><span class=\"line\">    mcast_src_ip 221.228.86.4 <span class=\"comment\">#Master服务器IP</span></span><br><span class=\"line\">    authentication &#123;</span><br><span class=\"line\">        auth_type PASS <span class=\"comment\">#主从服务器验证方式</span></span><br><span class=\"line\">        auth_pass 1111</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    track_script &#123;</span><br><span class=\"line\">        chk_haproxy <span class=\"comment\">#监测haproxy进程状态</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">#VIP</span></span><br><span class=\"line\">    virtual_ipaddress &#123;</span><br><span class=\"line\">        221.228.86.70 <span class=\"comment\">#虚拟IP</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Backup（221-228-86-6）服务器上的配置-x2F-etc-x2F-keepalived-x2F-keepalived-conf\"><a href=\"#Backup（221-228-86-6）服务器上的配置-x2F-etc-x2F-keepalived-x2F-keepalived-conf\" class=\"headerlink\" title=\"Backup（221.228.86.6）服务器上的配置 &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf\"></a>Backup（221.228.86.6）服务器上的配置 &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">global_defs &#123;</span><br><span class=\"line\">   notification_email &#123;</span><br><span class=\"line\">   user@example.com</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   notification_email_from mail@example.org</span><br><span class=\"line\">   smtp_server 192.168.x.x</span><br><span class=\"line\">   smtp_connect_timeout 30</span><br><span class=\"line\">   router_id LVS_DEVEL</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">#监测haproxy进程状态，每2秒执行一次</span></span><br><span class=\"line\">vrrp_script chk_haproxy &#123;</span><br><span class=\"line\">    script <span class=\"string\">&quot;/usr/local/keepalived/chk_haproxy.sh&quot;</span></span><br><span class=\"line\">    interval 2</span><br><span class=\"line\">    weight 2</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">vrrp_instance VI_1 &#123;</span><br><span class=\"line\">    state BACKUP <span class=\"comment\">#状态为BACKUP</span></span><br><span class=\"line\">    interface eth0</span><br><span class=\"line\">    virtual_router_id 51</span><br><span class=\"line\">    priority 100  <span class=\"comment\">#权重要低于MASTER</span></span><br><span class=\"line\">    advert_int 1</span><br><span class=\"line\">    mcast_src_ip 221.228.86.6 <span class=\"comment\">#Backup服务器的IP</span></span><br><span class=\"line\">    authentication &#123;</span><br><span class=\"line\">        auth_type PASS</span><br><span class=\"line\">        auth_pass 1111</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    track_script &#123;</span><br><span class=\"line\">        chk_haproxy <span class=\"comment\">#监测haproxy进程状态</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">#VIP</span></span><br><span class=\"line\">    virtual_ipaddress &#123;</span><br><span class=\"line\">        221.228.86.70 <span class=\"comment\">#虚拟IP</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"chk-haproxy-sh内容-x2F-usr-x2F-local-x2F-keepalived-x2F-chk-haproxy-sh\"><a href=\"#chk-haproxy-sh内容-x2F-usr-x2F-local-x2F-keepalived-x2F-chk-haproxy-sh\" class=\"headerlink\" title=\"chk_haproxy.sh内容 &#x2F;usr&#x2F;local&#x2F;keepalived&#x2F;chk_haproxy.sh\"></a>chk_haproxy.sh内容 &#x2F;usr&#x2F;local&#x2F;keepalived&#x2F;chk_haproxy.sh</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/bin/bash</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># author: weizhifeng</span></span><br><span class=\"line\"><span class=\"comment\"># description: </span></span><br><span class=\"line\"><span class=\"comment\"># 定时查看haproxy是否存在，如果不存在则启动haproxy，</span></span><br><span class=\"line\"><span class=\"comment\"># 如果启动失败，则停止keepalived</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">status=$(docker inspect  -f <span class=\"string\">&quot;&#123;&#123;.State.Running&#125;&#125;&quot;</span> haproxy-registry)</span><br><span class=\"line\"><span class=\"keyword\">if</span> [ <span class=\"string\">&quot;<span class=\"variable\">$&#123;status&#125;</span>&quot;</span> = <span class=\"string\">&quot;false&quot;</span> ]; <span class=\"keyword\">then</span></span><br><span class=\"line\">    docker start haproxy-registry</span><br><span class=\"line\">    status2=$(docker inspect  -f <span class=\"string\">&quot;&#123;&#123;.State.Running&#125;&#125;&quot;</span> haproxy-registry)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> [ <span class=\"string\">&quot;<span class=\"variable\">$&#123;status2&#125;</span>&quot;</span> = <span class=\"string\">&quot;false&quot;</span>  ]; <span class=\"keyword\">then</span></span><br><span class=\"line\">            service keepalived stop</span><br><span class=\"line\">    <span class=\"keyword\">fi</span></span><br><span class=\"line\"><span class=\"keyword\">fi</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"启动keepalived服务\"><a href=\"#启动keepalived服务\" class=\"headerlink\" title=\"启动keepalived服务\"></a>启动keepalived服务</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ service keepalived start</span><br></pre></td></tr></table></figure>"},{"title":"内核页表的传递","date":"2016-01-27T01:24:44.000Z","_content":"## 问题一：\n每个用户进程都有自己的页全局目录及页表，然后内核代表进程在内核态执行，此时如果内核代码修改了内核页表，那么这些修改是如何传递到其他用户进程的？毕竟所有用户进程维护自己的页表，同时关于内核线性空间的页表还必须相同，这个是如何做到的？\n\n<!-- more -->\n\n答案是:\n\nmap\\_vm\\_area()并不触及当前进程页表，而是直接修改init进程页表也就是主内核页表。一旦内核代表当前进程访问非连续内存区是，缺页发生，缺页处理程序会检查该地址是否是内核地址，并且该线性地址是否在主内核页表中。一旦处理程序发现一个主内核页表的有一个该线性地址的非空项，则将该项拷贝到当前进程的页表中。\n\n## 问题二：\n那么问题又来了，第一次内核访问非连续存储区的时候，由于进程页表的相应项为空会发生缺页异常，但是如果此后主内核页表项被修改，然后内核再次代表该进程访问非连续物理内存区的时候，由于页表已经被拷贝过旧的值，因此不会发生缺页异常，因此就会访问都错误的非连续的内存区，是这样么？\n\n答案是：\n\n因为在上一步的页表拷贝中，只拷贝了页全局目录的部分项，因此当前进程与主内核页表共享相同的页表项，并且这些页表项一旦被分配就不会被回收，但可以修改。这样所有对主内核页表的修改都会传递到当前进程。如果进程试图访问一个已经被释放发非连续内存区也会引发缺页异。\n\n参见《深入理解Linux内核》\n\n","source":"_posts/pagetable.md","raw":"title: 内核页表的传递\ndate: 2016-01-27 09:24:44\ntags: [\"页表\"]\ncategories: [\"操作系统\",\"Linux内核\"]\n---\n## 问题一：\n每个用户进程都有自己的页全局目录及页表，然后内核代表进程在内核态执行，此时如果内核代码修改了内核页表，那么这些修改是如何传递到其他用户进程的？毕竟所有用户进程维护自己的页表，同时关于内核线性空间的页表还必须相同，这个是如何做到的？\n\n<!-- more -->\n\n答案是:\n\nmap\\_vm\\_area()并不触及当前进程页表，而是直接修改init进程页表也就是主内核页表。一旦内核代表当前进程访问非连续内存区是，缺页发生，缺页处理程序会检查该地址是否是内核地址，并且该线性地址是否在主内核页表中。一旦处理程序发现一个主内核页表的有一个该线性地址的非空项，则将该项拷贝到当前进程的页表中。\n\n## 问题二：\n那么问题又来了，第一次内核访问非连续存储区的时候，由于进程页表的相应项为空会发生缺页异常，但是如果此后主内核页表项被修改，然后内核再次代表该进程访问非连续物理内存区的时候，由于页表已经被拷贝过旧的值，因此不会发生缺页异常，因此就会访问都错误的非连续的内存区，是这样么？\n\n答案是：\n\n因为在上一步的页表拷贝中，只拷贝了页全局目录的部分项，因此当前进程与主内核页表共享相同的页表项，并且这些页表项一旦被分配就不会被回收，但可以修改。这样所有对主内核页表的修改都会传递到当前进程。如果进程试图访问一个已经被释放发非连续内存区也会引发缺页异。\n\n参见《深入理解Linux内核》\n\n","slug":"pagetable","published":1,"updated":"2022-06-08T02:09:14.807Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4507g6p0007jjrhdr5m8yz5","content":"<h2 id=\"问题一：\"><a href=\"#问题一：\" class=\"headerlink\" title=\"问题一：\"></a>问题一：</h2><p>每个用户进程都有自己的页全局目录及页表，然后内核代表进程在内核态执行，此时如果内核代码修改了内核页表，那么这些修改是如何传递到其他用户进程的？毕竟所有用户进程维护自己的页表，同时关于内核线性空间的页表还必须相同，这个是如何做到的？</p>\n<span id=\"more\"></span>\n\n<p>答案是:</p>\n<p>map_vm_area()并不触及当前进程页表，而是直接修改init进程页表也就是主内核页表。一旦内核代表当前进程访问非连续内存区是，缺页发生，缺页处理程序会检查该地址是否是内核地址，并且该线性地址是否在主内核页表中。一旦处理程序发现一个主内核页表的有一个该线性地址的非空项，则将该项拷贝到当前进程的页表中。</p>\n<h2 id=\"问题二：\"><a href=\"#问题二：\" class=\"headerlink\" title=\"问题二：\"></a>问题二：</h2><p>那么问题又来了，第一次内核访问非连续存储区的时候，由于进程页表的相应项为空会发生缺页异常，但是如果此后主内核页表项被修改，然后内核再次代表该进程访问非连续物理内存区的时候，由于页表已经被拷贝过旧的值，因此不会发生缺页异常，因此就会访问都错误的非连续的内存区，是这样么？</p>\n<p>答案是：</p>\n<p>因为在上一步的页表拷贝中，只拷贝了页全局目录的部分项，因此当前进程与主内核页表共享相同的页表项，并且这些页表项一旦被分配就不会被回收，但可以修改。这样所有对主内核页表的修改都会传递到当前进程。如果进程试图访问一个已经被释放发非连续内存区也会引发缺页异。</p>\n<p>参见《深入理解Linux内核》</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"问题一：\"><a href=\"#问题一：\" class=\"headerlink\" title=\"问题一：\"></a>问题一：</h2><p>每个用户进程都有自己的页全局目录及页表，然后内核代表进程在内核态执行，此时如果内核代码修改了内核页表，那么这些修改是如何传递到其他用户进程的？毕竟所有用户进程维护自己的页表，同时关于内核线性空间的页表还必须相同，这个是如何做到的？</p>","more":"<p>答案是:</p>\n<p>map_vm_area()并不触及当前进程页表，而是直接修改init进程页表也就是主内核页表。一旦内核代表当前进程访问非连续内存区是，缺页发生，缺页处理程序会检查该地址是否是内核地址，并且该线性地址是否在主内核页表中。一旦处理程序发现一个主内核页表的有一个该线性地址的非空项，则将该项拷贝到当前进程的页表中。</p>\n<h2 id=\"问题二：\"><a href=\"#问题二：\" class=\"headerlink\" title=\"问题二：\"></a>问题二：</h2><p>那么问题又来了，第一次内核访问非连续存储区的时候，由于进程页表的相应项为空会发生缺页异常，但是如果此后主内核页表项被修改，然后内核再次代表该进程访问非连续物理内存区的时候，由于页表已经被拷贝过旧的值，因此不会发生缺页异常，因此就会访问都错误的非连续的内存区，是这样么？</p>\n<p>答案是：</p>\n<p>因为在上一步的页表拷贝中，只拷贝了页全局目录的部分项，因此当前进程与主内核页表共享相同的页表项，并且这些页表项一旦被分配就不会被回收，但可以修改。这样所有对主内核页表的修改都会传递到当前进程。如果进程试图访问一个已经被释放发非连续内存区也会引发缺页异。</p>\n<p>参见《深入理解Linux内核》</p>"},{"title":"进程切换时的现场保护问题","date":"2015-11-05T14:53:06.000Z","_content":"## 进程切换流程\n进程的切换只会发生在内核精心定义的点上：schedule()函数。从本质上来说进程的切换由两个步骤完成，或两个标志性动作：  \n\n* 1.切换页全局目录来安装一个新的地址空间，cr3寄存器。\n这里请考虑这样一个问题，一旦页全局目录被切换成其他进程的，那么eip在取下一条指令时通过页表映射的方式还能正常的取到当前执行流的下一条指令吗？毕竟两个进程的页全局目录、页表大部分情况肯定会不一样。\n答案是：进程切换发生在内核态，所有代码的引用的地址空间大于0xC0000000,这部分页表属于主内核页表，而所有进程具有相同的主内核页表（至于为何具有相同主内核也表，见下一节），所以指令可以正常取并执行。  \n* 2.切换内核态堆栈与硬件上下文。  \n那么切换时的现场保护问题，尤其是SS段寄存器和esp寄存器的保护 。由于进程的切换发生在内核态，通常在进入内核态中断发生时，硬件上下文会被自动保存。\n\n<!-- more -->\n\n## 切换问题\n一直有一个疑问，在中断发生时上下文切换的时候会发生栈的切换，并且把原来的栈段寄存器SS和esp寄存器的值一起保存到新的内核栈段寄存器SS和esp寄存器指向的内核栈，而且这个时候还不能使用任何的其他寄存器辅助（因为硬件上下文还没有保存，任何对寄存器的引用都会破坏进程的上下文），并且任何的push操作都会改变当前的esp，这个切换过程究竟是如何实现的？\n\n## 答案\nMDD，硬件上下文的切换是硬件自动完成的，不需要软件做。软件做的只是提供好中断号、idtr寄存器、gdtr寄存器、tr寄存器的值。  \n中断发生时，硬件做了一系列工作，包含获取中断号，从idtr寄存器根据中断号找到中断处理程序入口地址CS：EIP，根据CS及gdtr寄存器找到中断入口代码的段基址及特权位，判断如果特权位发生变化则需要切换到内核栈，切换的过程是从tr寄存器找到当前进程的TSS段，然后把TSS段里保存的当前进程的内核栈SS：ESP放入当前SS：ESP中，并将原来的SS：ESP值push到当前栈中，然后push CS,EIP,EFLAG等寄存器。注意这些都是硬件完成的。而通用寄存器的保护工作应该是由中断处理程序ISR做的。\n    \n    参见《深入理解Linux内核》\n","source":"_posts/swithprocess.md","raw":"title: 进程切换时的现场保护问题\ndate: 2015-11-05 22:53:06\ntags: [\"进程切换\"]\ncategories: [\"操作系统\",\"Linux内核\"]\n---\n## 进程切换流程\n进程的切换只会发生在内核精心定义的点上：schedule()函数。从本质上来说进程的切换由两个步骤完成，或两个标志性动作：  \n\n* 1.切换页全局目录来安装一个新的地址空间，cr3寄存器。\n这里请考虑这样一个问题，一旦页全局目录被切换成其他进程的，那么eip在取下一条指令时通过页表映射的方式还能正常的取到当前执行流的下一条指令吗？毕竟两个进程的页全局目录、页表大部分情况肯定会不一样。\n答案是：进程切换发生在内核态，所有代码的引用的地址空间大于0xC0000000,这部分页表属于主内核页表，而所有进程具有相同的主内核页表（至于为何具有相同主内核也表，见下一节），所以指令可以正常取并执行。  \n* 2.切换内核态堆栈与硬件上下文。  \n那么切换时的现场保护问题，尤其是SS段寄存器和esp寄存器的保护 。由于进程的切换发生在内核态，通常在进入内核态中断发生时，硬件上下文会被自动保存。\n\n<!-- more -->\n\n## 切换问题\n一直有一个疑问，在中断发生时上下文切换的时候会发生栈的切换，并且把原来的栈段寄存器SS和esp寄存器的值一起保存到新的内核栈段寄存器SS和esp寄存器指向的内核栈，而且这个时候还不能使用任何的其他寄存器辅助（因为硬件上下文还没有保存，任何对寄存器的引用都会破坏进程的上下文），并且任何的push操作都会改变当前的esp，这个切换过程究竟是如何实现的？\n\n## 答案\nMDD，硬件上下文的切换是硬件自动完成的，不需要软件做。软件做的只是提供好中断号、idtr寄存器、gdtr寄存器、tr寄存器的值。  \n中断发生时，硬件做了一系列工作，包含获取中断号，从idtr寄存器根据中断号找到中断处理程序入口地址CS：EIP，根据CS及gdtr寄存器找到中断入口代码的段基址及特权位，判断如果特权位发生变化则需要切换到内核栈，切换的过程是从tr寄存器找到当前进程的TSS段，然后把TSS段里保存的当前进程的内核栈SS：ESP放入当前SS：ESP中，并将原来的SS：ESP值push到当前栈中，然后push CS,EIP,EFLAG等寄存器。注意这些都是硬件完成的。而通用寄存器的保护工作应该是由中断处理程序ISR做的。\n    \n    参见《深入理解Linux内核》\n","slug":"swithprocess","published":1,"updated":"2022-06-08T02:09:14.806Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4507g6q0008jjrh5y1c7ymc","content":"<h2 id=\"进程切换流程\"><a href=\"#进程切换流程\" class=\"headerlink\" title=\"进程切换流程\"></a>进程切换流程</h2><p>进程的切换只会发生在内核精心定义的点上：schedule()函数。从本质上来说进程的切换由两个步骤完成，或两个标志性动作：  </p>\n<ul>\n<li>1.切换页全局目录来安装一个新的地址空间，cr3寄存器。<br>这里请考虑这样一个问题，一旦页全局目录被切换成其他进程的，那么eip在取下一条指令时通过页表映射的方式还能正常的取到当前执行流的下一条指令吗？毕竟两个进程的页全局目录、页表大部分情况肯定会不一样。<br>答案是：进程切换发生在内核态，所有代码的引用的地址空间大于0xC0000000,这部分页表属于主内核页表，而所有进程具有相同的主内核页表（至于为何具有相同主内核也表，见下一节），所以指令可以正常取并执行。  </li>\n<li>2.切换内核态堆栈与硬件上下文。<br>那么切换时的现场保护问题，尤其是SS段寄存器和esp寄存器的保护 。由于进程的切换发生在内核态，通常在进入内核态中断发生时，硬件上下文会被自动保存。</li>\n</ul>\n<span id=\"more\"></span>\n\n<h2 id=\"切换问题\"><a href=\"#切换问题\" class=\"headerlink\" title=\"切换问题\"></a>切换问题</h2><p>一直有一个疑问，在中断发生时上下文切换的时候会发生栈的切换，并且把原来的栈段寄存器SS和esp寄存器的值一起保存到新的内核栈段寄存器SS和esp寄存器指向的内核栈，而且这个时候还不能使用任何的其他寄存器辅助（因为硬件上下文还没有保存，任何对寄存器的引用都会破坏进程的上下文），并且任何的push操作都会改变当前的esp，这个切换过程究竟是如何实现的？</p>\n<h2 id=\"答案\"><a href=\"#答案\" class=\"headerlink\" title=\"答案\"></a>答案</h2><p>MDD，硬件上下文的切换是硬件自动完成的，不需要软件做。软件做的只是提供好中断号、idtr寄存器、gdtr寄存器、tr寄存器的值。<br>中断发生时，硬件做了一系列工作，包含获取中断号，从idtr寄存器根据中断号找到中断处理程序入口地址CS：EIP，根据CS及gdtr寄存器找到中断入口代码的段基址及特权位，判断如果特权位发生变化则需要切换到内核栈，切换的过程是从tr寄存器找到当前进程的TSS段，然后把TSS段里保存的当前进程的内核栈SS：ESP放入当前SS：ESP中，并将原来的SS：ESP值push到当前栈中，然后push CS,EIP,EFLAG等寄存器。注意这些都是硬件完成的。而通用寄存器的保护工作应该是由中断处理程序ISR做的。</p>\n<pre><code>参见《深入理解Linux内核》\n</code></pre>\n","site":{"data":{}},"excerpt":"<h2 id=\"进程切换流程\"><a href=\"#进程切换流程\" class=\"headerlink\" title=\"进程切换流程\"></a>进程切换流程</h2><p>进程的切换只会发生在内核精心定义的点上：schedule()函数。从本质上来说进程的切换由两个步骤完成，或两个标志性动作：  </p>\n<ul>\n<li>1.切换页全局目录来安装一个新的地址空间，cr3寄存器。<br>这里请考虑这样一个问题，一旦页全局目录被切换成其他进程的，那么eip在取下一条指令时通过页表映射的方式还能正常的取到当前执行流的下一条指令吗？毕竟两个进程的页全局目录、页表大部分情况肯定会不一样。<br>答案是：进程切换发生在内核态，所有代码的引用的地址空间大于0xC0000000,这部分页表属于主内核页表，而所有进程具有相同的主内核页表（至于为何具有相同主内核也表，见下一节），所以指令可以正常取并执行。  </li>\n<li>2.切换内核态堆栈与硬件上下文。<br>那么切换时的现场保护问题，尤其是SS段寄存器和esp寄存器的保护 。由于进程的切换发生在内核态，通常在进入内核态中断发生时，硬件上下文会被自动保存。</li>\n</ul>","more":"<h2 id=\"切换问题\"><a href=\"#切换问题\" class=\"headerlink\" title=\"切换问题\"></a>切换问题</h2><p>一直有一个疑问，在中断发生时上下文切换的时候会发生栈的切换，并且把原来的栈段寄存器SS和esp寄存器的值一起保存到新的内核栈段寄存器SS和esp寄存器指向的内核栈，而且这个时候还不能使用任何的其他寄存器辅助（因为硬件上下文还没有保存，任何对寄存器的引用都会破坏进程的上下文），并且任何的push操作都会改变当前的esp，这个切换过程究竟是如何实现的？</p>\n<h2 id=\"答案\"><a href=\"#答案\" class=\"headerlink\" title=\"答案\"></a>答案</h2><p>MDD，硬件上下文的切换是硬件自动完成的，不需要软件做。软件做的只是提供好中断号、idtr寄存器、gdtr寄存器、tr寄存器的值。<br>中断发生时，硬件做了一系列工作，包含获取中断号，从idtr寄存器根据中断号找到中断处理程序入口地址CS：EIP，根据CS及gdtr寄存器找到中断入口代码的段基址及特权位，判断如果特权位发生变化则需要切换到内核栈，切换的过程是从tr寄存器找到当前进程的TSS段，然后把TSS段里保存的当前进程的内核栈SS：ESP放入当前SS：ESP中，并将原来的SS：ESP值push到当前栈中，然后push CS,EIP,EFLAG等寄存器。注意这些都是硬件完成的。而通用寄存器的保护工作应该是由中断处理程序ISR做的。</p>\n<pre><code>参见《深入理解Linux内核》\n</code></pre>"},{"title":"关于从x86实模式到保护模式的关键一跳的指令连续执行问题","date":"2015-11-05T14:06:15.000Z","_content":"\n## 问题描述\n关于实模式与保护模式的基础可以参考《深入理解linux内核》，或相关博文。\n\n```bash\n1.实模式下的寻址方式\n      cs <<4 + ip\n2.保护模式下的寻址方式\n      base(index(cs)) +eip\n3.x86实模式进入保护模式的代码\n       lgdt  GdtPtr        //加载 GDTR\n       cli                       //关中断\n                                 //打开地址线A20等\n                                 //下面准备切换到保护模式\n       mov   %cr0，%eax\n       or       0x1,%eax\n       mov   %eax,%cr0 //将cr0的PE位置1，进入保护模式\n\n       jmp   dword SelectorSode32:0x0\n```\n\n**问题:**  \n当打开cr0的PE位的瞬间，处理器进入保护模式，寻址方式改变。此时cs的值并没有改变，并且打开cr0瞬间处理器对cs的解释方式完全不一样，那么问题来了，如何确保在进入保护模式后下一条指令被顺利执行？\n\n<!-- more -->\n\n## 解决过程  \n你可能会想  \n\n* 1.是不是cs段寄存器的内容足够特别，使得无论此时采用实模式的寻址方式还是保护模式的寻址方式所解释出来的地址相等？然后你会发现其实这个cs目前值没有任何规律，完全不对头。\n* 2.翻一翻x86指令集，发现x86规定，在cpu执行长跳转指令的时候才会自动改变cs寄存器的值。然后你会去找，是不是前后有哪些地方执行过长跳转指令？然后没找到。\n* 3.然后没招的时候你还可能想是不是并不一定就是接着下一条指令执行呢？然后就真的按照当前cs段的值以实模式的方式计算一下下条指令会在那个地方，最后发现根本行不通。\n\n** 图书馆、百度、Google齐上阵后发现**\n\n## 问题答案  \n段寄存器后面都有隐藏的高速缓冲寄存器，当cs寄存器值被更新时，这个高速缓冲寄存器的值会根据当时的寻址方式更新，比如实模式下就是cs<<4后放入高速缓冲寄存器，等需要取指令的时候就直接取高速缓冲寄存器的值与eip相加即可，并不会真的再去cs段寄存器找这个值然后做像保护模式下的寻址方式那样推导。这样就解释了上面的情况，虽然寻址方式改变了，但是cs段寄存器的值没变，高速缓冲寄存器的值就不会变，基址仍然是实模式时的值，从而实际上计算出来的还是实模式的地址，保证了指令流的持续执行。内核在等到切换准备就绪的时候就会执行一个长跳转指令来刷新cs段寄存器的值，从而真的跳转导保护模式下了。\n\n     可以参考《x86汇编语言：从实模式到保护模式》，里面也做了相关描述。\n","source":"_posts/keyjump.md","raw":"title: 关于从x86实模式到保护模式的关键一跳的指令连续执行问题 \ndate: 2015-11-05 22:06:15\ntags: [\"实模式\",\"保护模式\"]\ncategories: [\"操作系统\",\"Linux内核\"]\n---\n\n## 问题描述\n关于实模式与保护模式的基础可以参考《深入理解linux内核》，或相关博文。\n\n```bash\n1.实模式下的寻址方式\n      cs <<4 + ip\n2.保护模式下的寻址方式\n      base(index(cs)) +eip\n3.x86实模式进入保护模式的代码\n       lgdt  GdtPtr        //加载 GDTR\n       cli                       //关中断\n                                 //打开地址线A20等\n                                 //下面准备切换到保护模式\n       mov   %cr0，%eax\n       or       0x1,%eax\n       mov   %eax,%cr0 //将cr0的PE位置1，进入保护模式\n\n       jmp   dword SelectorSode32:0x0\n```\n\n**问题:**  \n当打开cr0的PE位的瞬间，处理器进入保护模式，寻址方式改变。此时cs的值并没有改变，并且打开cr0瞬间处理器对cs的解释方式完全不一样，那么问题来了，如何确保在进入保护模式后下一条指令被顺利执行？\n\n<!-- more -->\n\n## 解决过程  \n你可能会想  \n\n* 1.是不是cs段寄存器的内容足够特别，使得无论此时采用实模式的寻址方式还是保护模式的寻址方式所解释出来的地址相等？然后你会发现其实这个cs目前值没有任何规律，完全不对头。\n* 2.翻一翻x86指令集，发现x86规定，在cpu执行长跳转指令的时候才会自动改变cs寄存器的值。然后你会去找，是不是前后有哪些地方执行过长跳转指令？然后没找到。\n* 3.然后没招的时候你还可能想是不是并不一定就是接着下一条指令执行呢？然后就真的按照当前cs段的值以实模式的方式计算一下下条指令会在那个地方，最后发现根本行不通。\n\n** 图书馆、百度、Google齐上阵后发现**\n\n## 问题答案  \n段寄存器后面都有隐藏的高速缓冲寄存器，当cs寄存器值被更新时，这个高速缓冲寄存器的值会根据当时的寻址方式更新，比如实模式下就是cs<<4后放入高速缓冲寄存器，等需要取指令的时候就直接取高速缓冲寄存器的值与eip相加即可，并不会真的再去cs段寄存器找这个值然后做像保护模式下的寻址方式那样推导。这样就解释了上面的情况，虽然寻址方式改变了，但是cs段寄存器的值没变，高速缓冲寄存器的值就不会变，基址仍然是实模式时的值，从而实际上计算出来的还是实模式的地址，保证了指令流的持续执行。内核在等到切换准备就绪的时候就会执行一个长跳转指令来刷新cs段寄存器的值，从而真的跳转导保护模式下了。\n\n     可以参考《x86汇编语言：从实模式到保护模式》，里面也做了相关描述。\n","slug":"keyjump","published":1,"updated":"2022-06-08T02:09:14.807Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4507g6r0009jjrh90bk0sw6","content":"<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><p>关于实模式与保护模式的基础可以参考《深入理解linux内核》，或相关博文。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.实模式下的寻址方式</span><br><span class=\"line\">      cs &lt;&lt;<span class=\"string\">4 + ip</span></span><br><span class=\"line\"><span class=\"string\">2.保护模式下的寻址方式</span></span><br><span class=\"line\"><span class=\"string\">      base(index(cs)) +eip</span></span><br><span class=\"line\"><span class=\"string\">3.x86实模式进入保护模式的代码</span></span><br><span class=\"line\"><span class=\"string\">       lgdt  GdtPtr        //加载 GDTR</span></span><br><span class=\"line\"><span class=\"string\">       cli                       //关中断</span></span><br><span class=\"line\"><span class=\"string\">                                 //打开地址线A20等</span></span><br><span class=\"line\"><span class=\"string\">                                 //下面准备切换到保护模式</span></span><br><span class=\"line\"><span class=\"string\">       mov   %cr0，%eax</span></span><br><span class=\"line\"><span class=\"string\">       or       0x1,%eax</span></span><br><span class=\"line\"><span class=\"string\">       mov   %eax,%cr0 //将cr0的PE位置1，进入保护模式</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">       jmp   dword SelectorSode32:0x0</span></span><br></pre></td></tr></table></figure>\n\n<p><strong>问题:</strong><br>当打开cr0的PE位的瞬间，处理器进入保护模式，寻址方式改变。此时cs的值并没有改变，并且打开cr0瞬间处理器对cs的解释方式完全不一样，那么问题来了，如何确保在进入保护模式后下一条指令被顺利执行？</p>\n<span id=\"more\"></span>\n\n<h2 id=\"解决过程\"><a href=\"#解决过程\" class=\"headerlink\" title=\"解决过程\"></a>解决过程</h2><p>你可能会想  </p>\n<ul>\n<li>1.是不是cs段寄存器的内容足够特别，使得无论此时采用实模式的寻址方式还是保护模式的寻址方式所解释出来的地址相等？然后你会发现其实这个cs目前值没有任何规律，完全不对头。</li>\n<li>2.翻一翻x86指令集，发现x86规定，在cpu执行长跳转指令的时候才会自动改变cs寄存器的值。然后你会去找，是不是前后有哪些地方执行过长跳转指令？然后没找到。</li>\n<li>3.然后没招的时候你还可能想是不是并不一定就是接着下一条指令执行呢？然后就真的按照当前cs段的值以实模式的方式计算一下下条指令会在那个地方，最后发现根本行不通。</li>\n</ul>\n<p>** 图书馆、百度、Google齐上阵后发现**</p>\n<h2 id=\"问题答案\"><a href=\"#问题答案\" class=\"headerlink\" title=\"问题答案\"></a>问题答案</h2><p>段寄存器后面都有隐藏的高速缓冲寄存器，当cs寄存器值被更新时，这个高速缓冲寄存器的值会根据当时的寻址方式更新，比如实模式下就是cs&lt;&lt;4后放入高速缓冲寄存器，等需要取指令的时候就直接取高速缓冲寄存器的值与eip相加即可，并不会真的再去cs段寄存器找这个值然后做像保护模式下的寻址方式那样推导。这样就解释了上面的情况，虽然寻址方式改变了，但是cs段寄存器的值没变，高速缓冲寄存器的值就不会变，基址仍然是实模式时的值，从而实际上计算出来的还是实模式的地址，保证了指令流的持续执行。内核在等到切换准备就绪的时候就会执行一个长跳转指令来刷新cs段寄存器的值，从而真的跳转导保护模式下了。</p>\n<pre><code> 可以参考《x86汇编语言：从实模式到保护模式》，里面也做了相关描述。\n</code></pre>\n","site":{"data":{}},"excerpt":"<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><p>关于实模式与保护模式的基础可以参考《深入理解linux内核》，或相关博文。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.实模式下的寻址方式</span><br><span class=\"line\">      cs &lt;&lt;<span class=\"string\">4 + ip</span></span><br><span class=\"line\"><span class=\"string\">2.保护模式下的寻址方式</span></span><br><span class=\"line\"><span class=\"string\">      base(index(cs)) +eip</span></span><br><span class=\"line\"><span class=\"string\">3.x86实模式进入保护模式的代码</span></span><br><span class=\"line\"><span class=\"string\">       lgdt  GdtPtr        //加载 GDTR</span></span><br><span class=\"line\"><span class=\"string\">       cli                       //关中断</span></span><br><span class=\"line\"><span class=\"string\">                                 //打开地址线A20等</span></span><br><span class=\"line\"><span class=\"string\">                                 //下面准备切换到保护模式</span></span><br><span class=\"line\"><span class=\"string\">       mov   %cr0，%eax</span></span><br><span class=\"line\"><span class=\"string\">       or       0x1,%eax</span></span><br><span class=\"line\"><span class=\"string\">       mov   %eax,%cr0 //将cr0的PE位置1，进入保护模式</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">       jmp   dword SelectorSode32:0x0</span></span><br></pre></td></tr></table></figure>\n\n<p><strong>问题:</strong><br>当打开cr0的PE位的瞬间，处理器进入保护模式，寻址方式改变。此时cs的值并没有改变，并且打开cr0瞬间处理器对cs的解释方式完全不一样，那么问题来了，如何确保在进入保护模式后下一条指令被顺利执行？</p>","more":"<h2 id=\"解决过程\"><a href=\"#解决过程\" class=\"headerlink\" title=\"解决过程\"></a>解决过程</h2><p>你可能会想  </p>\n<ul>\n<li>1.是不是cs段寄存器的内容足够特别，使得无论此时采用实模式的寻址方式还是保护模式的寻址方式所解释出来的地址相等？然后你会发现其实这个cs目前值没有任何规律，完全不对头。</li>\n<li>2.翻一翻x86指令集，发现x86规定，在cpu执行长跳转指令的时候才会自动改变cs寄存器的值。然后你会去找，是不是前后有哪些地方执行过长跳转指令？然后没找到。</li>\n<li>3.然后没招的时候你还可能想是不是并不一定就是接着下一条指令执行呢？然后就真的按照当前cs段的值以实模式的方式计算一下下条指令会在那个地方，最后发现根本行不通。</li>\n</ul>\n<p>** 图书馆、百度、Google齐上阵后发现**</p>\n<h2 id=\"问题答案\"><a href=\"#问题答案\" class=\"headerlink\" title=\"问题答案\"></a>问题答案</h2><p>段寄存器后面都有隐藏的高速缓冲寄存器，当cs寄存器值被更新时，这个高速缓冲寄存器的值会根据当时的寻址方式更新，比如实模式下就是cs&lt;&lt;4后放入高速缓冲寄存器，等需要取指令的时候就直接取高速缓冲寄存器的值与eip相加即可，并不会真的再去cs段寄存器找这个值然后做像保护模式下的寻址方式那样推导。这样就解释了上面的情况，虽然寻址方式改变了，但是cs段寄存器的值没变，高速缓冲寄存器的值就不会变，基址仍然是实模式时的值，从而实际上计算出来的还是实模式的地址，保证了指令流的持续执行。内核在等到切换准备就绪的时候就会执行一个长跳转指令来刷新cs段寄存器的值，从而真的跳转导保护模式下了。</p>\n<pre><code> 可以参考《x86汇编语言：从实模式到保护模式》，里面也做了相关描述。\n</code></pre>"},{"title":"遇见复原力","date":"2022-06-08T04:32:49.000Z","_content":"\n## 复原力\n\n复原力一词来源于微软创新研究院，全称是社会复原力，它研究的是通过大规模部署可以适应未来各种变数以及不确定性的创新技术来应对社会的各种潜在危机，从而增强社会复原力。如COVID-19、全球气候变暖、环境污染，探索在危机来临前中后的创新技术储备，快速应对方法，快速复原的能力。是一种从无序和混乱中回归秩序的能力。这也是运维所要解决的问题，让系统回归秩序。\n\n本文探索的是云原生时代下容器基础设施在面临各种危机时的复原力问题，即基础设施复原力。\n\n<!-- more -->\n\n## 基础设施复原力\n随着容器技术的不断发展，k8s已经成为了云原生时代下的基础设施，它为应用向下屏蔽了云上基础设施的复杂性，向上提供了应用的全生命周期管理能力，将分布式应用管理的复杂性收敛转化到了分布式系统本身的管理问题上。然而，它虽然解决了应用在分布式环境下的编排问题，但自身的分布式系统的运维复杂性仍然存在。底层基础设施失效，可用区故障，节点故障，组件失效，网络分区，抖动，甚至地震、火灾等风险始终存在，如何保证系统的稳定性以及面对这些危机时候的系统复原力是一项非常具有挑战的工作，这也构成了云原生时代下的基础设施复原力问题。\n\n事实上，k8s面向终态的设计已经让应用具备了从外部扰动中回到稳态的能力，面对应用进程异常退出，容器失败，网络故障，配置紊乱等问题，k8s能够实现检测并自行原地恢复，或者替换掉失效应用副本来使应用处于预期状态，这是k8s的应用复原力。通过以上理念，我们希望云原生时代的基础设施也能具备从外部扰动中自动回归稳态的能力，同时以一种尽可能小的代价去解决k8s平台自身运维复杂性的问题，因此我们设计了`wdrip`，以一种逐渐收敛的方案来解决k8s分布式系统的自动化运维问题，实现基础设施的复原力。\n\n\n## 故障复原场景示例\nk8s 面向终态的设计为应用的复原力提供了支撑。从终态设计上你能看到k8s 让应用从外部扰动中回归稳态的思想, wdrip也同样采用面向终态的思想，在基础设施层解决k8s的自运维问题。下面会演示一些常见的k8s故障场景，并且观察系统是如何在无人为干预的情况下自动恢复的。\n\n### 前置条件\nwdrip的基本功能及其使用，请移步{% post_link '管理集群' %}\n\n- 请使用wdrip创建一个k8s集群。\n\n- 请部署示例应用，或者部署您自己觉得合适的[容器化应用]{% post_link '云原生应用示例' %}。\n\n\n### 场景一：节点故障\n节点故障是k8s集群故障里最常见的一种场景了，也是危害相对较小的故障，wdrip可以侦测节点的异常状态，然后尝试修复。修复的流程如下\n如果节点状态变为NotReady，那么等待2个heartbeat周期\n- 如果周期内持续NotReady，wdrip尝试重启kubelet，并等待节点Ready\n- 如果节点仍然NotReady，wdrip尝试驱逐节点Pod，然后继续尝试重启ECS，并等待节点Ready\n- 如果节点仍然NotReady，wdrip继续尝试重置ECS（替换节点的系统盘，并重新初始化节点），并等待节点Ready\n- 如果节点仍然NotReady，wdrip删除这个节点，等待系统重启创建一个全新的节点副本。\n- 如果节点仍然NotReady，修复失败。等待throttle周期后重复以上步骤。\n\n通常可以通过以下手段模拟节点故障：\n1. 方法一：通过`kubectl delete no ${your-node}` 删除对应的Node Object。等待节点自动恢复并注册回来，恢复全程无需人工干预，时长约2分钟。\n2. 方法二：登录ECS节点，systemctl stop kubelet/docker等。等待节点NotReady后，系统会自行恢复节点状态。恢复全程无需人工干预。\n3. 方法三：到ECS控制台上【关闭】节点对应的ECS。系统会自行恢复节点状态。恢复全程无需人工干预。\n4. 方法四：到ECS控制台上【删除】节点对应的ECS。系统会自行恢复节点状态。恢复全程无需人工干预。\n\n[去试一试]{% post_link '管理集群' %}\n\n### 场景二：管控故障\n管控故障通常情况下是比较严重的，k8s集群依赖etcd组成高可用集群，一旦etcd quorum丢失，集群管控将会直接不可用。因此正常情况下管控的恢复要更加的复杂。\nwdrip精心抽象了管控的恢复流程，操作前会尽量确保etcd quorum始终保持在可用状态。但仍然具备在quorum丢失的情况下从混乱中使用snapshot恢复集群管控。\n\n通常可以通过以下方法模拟管控故障：\n1. 方法一：通过kubectl命令删除master node。\n2. 方法二：登录Master节点ECS，stop ETCD服务，删除etcd数据，stop kubelet服务，stop docker服务等。\n3. 方法三：登录ECS控制台【关闭】一台Master节点ECS。\n4. 方法四：登录ECS控制台【删除】一台Master节点ECS。\n\n对于3个master或更多个master副本的集群，你可以操作让一台ECS节点故障，等待查看系统如何回复到稳定状态，这个过程中整个系统全程仍然可以提供服务。重要的是，你可以操作多台Master ECS节点，让他们同时处于故障状态，这时候系统大概率会因为quorum丢失从而造成集群不可用，但你仍然能够观察到wdrip会通过各种手段将集群恢复到正常的稳定状态，并且无需人工干预。\n对于worker节点，只要不涉及到管控面访问相关的业务，恢复的过程中用户的业务将不受任何影响，实现业务的连续性。\n\n### 场景三：可用区故障\n虽然可用区级别的故障的概率非常低，但我们仍然在设计的时候考虑到这种场景，wdrip支持创建跨可用区的集群，添加多个不同可用区的节点池可以让节点分布到不同可用区，提高集群面对可用区故障时候的韧性。\n\n当管控所在的可用区出现故障时候，集群不可用，集群上的应用大概率也会不可用，wdrip支持snapshot整个k8s集群，然后在其他可用区使用该snapshot一键重新拉起一个一模一样的集群，这种恢复方式保留了所有原集群的配置，唯一变化的是承载集群运行的基础设施，如ECS、SLB等等。相当于在一个新的克隆体中恢复之前备份的大脑，意识下载。\n```shell\n(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120\n```\n\n### 场景四：Region故障\n整个Region发生故障的概率应该是非常非常的低的了，但wdrip设计上仍然能够处理这种灾难。wdrip在运行时会定期备份集群的状态到OSS（可以跨Region写多份实现高可用），当Region出现故障的时候，可以尝试在另外一个Region创建一个新的集群，然后通过备份的snapshot快速恢复业务。参考可用区故障。但目前很多存储资源是可用区级别的，因此还需要进一步考虑存储的跨Region恢复。\n\n\n### 场景五：删库跑路\n一直想提的一个场景就是删库跑路。wdrip定期备份了整个集群的意识，无论你怎么折腾集群，只要备份存在，意识就存在，wdrip就可以再完完全全的还原出一个集群的克隆体出来，恢复现场到以前的状态。\n\n**删库跑路？不存在的。**\n\n你可以尝试删除集群试试，删除方法\n1. 到ROS控制台删除该集群对应的ROS模板，按名称搜索即可找到对应的模板\n2. wdrip delete -n kubernetes-wdrip-120\n\n恢复的方法：\n1. 创建新的集群。wdrip create --config cluster.yml\n2. 恢复集群的意识备份到这个新建的集群本体上。 wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120\n\n\n### 灾难恢复（手动）\n```bash\n# -n 指定恢复的目标集群（本体）， -f 指定恢复的配置来源于哪个集群（意识）。\n(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120\n\n```\n\n### ChaosMonkey\n为了测试基础设施复原力我们专门编写了ChaosMonkey部署在集群中，他能够定期破坏集群(删除集群ECS资源、停止关键组件)，等待系统自愈。\n\n如何使用\n```shell\n(base) ➜  export CLUSTER_NAME=xxxx\n(base) ➜  bash hack/wdrip.sh config\n(base) ➜  bash hack/wdrip.sh chaos\n(base) ➜  kubectl --kubeconfig ~/.kube/config.wdrip -n kube-system get po|grep monkey\n```\n\n## 小结\n\nk8s的复杂性虽然让让人生畏，但wdrip提供的复原力让我们在面对危机的时候能够更加的从容。\n","source":"_posts/遇见复原力.md","raw":"title: 遇见复原力\ndate: 2022-06-08 12:32:49\ntags:\n---\n\n## 复原力\n\n复原力一词来源于微软创新研究院，全称是社会复原力，它研究的是通过大规模部署可以适应未来各种变数以及不确定性的创新技术来应对社会的各种潜在危机，从而增强社会复原力。如COVID-19、全球气候变暖、环境污染，探索在危机来临前中后的创新技术储备，快速应对方法，快速复原的能力。是一种从无序和混乱中回归秩序的能力。这也是运维所要解决的问题，让系统回归秩序。\n\n本文探索的是云原生时代下容器基础设施在面临各种危机时的复原力问题，即基础设施复原力。\n\n<!-- more -->\n\n## 基础设施复原力\n随着容器技术的不断发展，k8s已经成为了云原生时代下的基础设施，它为应用向下屏蔽了云上基础设施的复杂性，向上提供了应用的全生命周期管理能力，将分布式应用管理的复杂性收敛转化到了分布式系统本身的管理问题上。然而，它虽然解决了应用在分布式环境下的编排问题，但自身的分布式系统的运维复杂性仍然存在。底层基础设施失效，可用区故障，节点故障，组件失效，网络分区，抖动，甚至地震、火灾等风险始终存在，如何保证系统的稳定性以及面对这些危机时候的系统复原力是一项非常具有挑战的工作，这也构成了云原生时代下的基础设施复原力问题。\n\n事实上，k8s面向终态的设计已经让应用具备了从外部扰动中回到稳态的能力，面对应用进程异常退出，容器失败，网络故障，配置紊乱等问题，k8s能够实现检测并自行原地恢复，或者替换掉失效应用副本来使应用处于预期状态，这是k8s的应用复原力。通过以上理念，我们希望云原生时代的基础设施也能具备从外部扰动中自动回归稳态的能力，同时以一种尽可能小的代价去解决k8s平台自身运维复杂性的问题，因此我们设计了`wdrip`，以一种逐渐收敛的方案来解决k8s分布式系统的自动化运维问题，实现基础设施的复原力。\n\n\n## 故障复原场景示例\nk8s 面向终态的设计为应用的复原力提供了支撑。从终态设计上你能看到k8s 让应用从外部扰动中回归稳态的思想, wdrip也同样采用面向终态的思想，在基础设施层解决k8s的自运维问题。下面会演示一些常见的k8s故障场景，并且观察系统是如何在无人为干预的情况下自动恢复的。\n\n### 前置条件\nwdrip的基本功能及其使用，请移步{% post_link '管理集群' %}\n\n- 请使用wdrip创建一个k8s集群。\n\n- 请部署示例应用，或者部署您自己觉得合适的[容器化应用]{% post_link '云原生应用示例' %}。\n\n\n### 场景一：节点故障\n节点故障是k8s集群故障里最常见的一种场景了，也是危害相对较小的故障，wdrip可以侦测节点的异常状态，然后尝试修复。修复的流程如下\n如果节点状态变为NotReady，那么等待2个heartbeat周期\n- 如果周期内持续NotReady，wdrip尝试重启kubelet，并等待节点Ready\n- 如果节点仍然NotReady，wdrip尝试驱逐节点Pod，然后继续尝试重启ECS，并等待节点Ready\n- 如果节点仍然NotReady，wdrip继续尝试重置ECS（替换节点的系统盘，并重新初始化节点），并等待节点Ready\n- 如果节点仍然NotReady，wdrip删除这个节点，等待系统重启创建一个全新的节点副本。\n- 如果节点仍然NotReady，修复失败。等待throttle周期后重复以上步骤。\n\n通常可以通过以下手段模拟节点故障：\n1. 方法一：通过`kubectl delete no ${your-node}` 删除对应的Node Object。等待节点自动恢复并注册回来，恢复全程无需人工干预，时长约2分钟。\n2. 方法二：登录ECS节点，systemctl stop kubelet/docker等。等待节点NotReady后，系统会自行恢复节点状态。恢复全程无需人工干预。\n3. 方法三：到ECS控制台上【关闭】节点对应的ECS。系统会自行恢复节点状态。恢复全程无需人工干预。\n4. 方法四：到ECS控制台上【删除】节点对应的ECS。系统会自行恢复节点状态。恢复全程无需人工干预。\n\n[去试一试]{% post_link '管理集群' %}\n\n### 场景二：管控故障\n管控故障通常情况下是比较严重的，k8s集群依赖etcd组成高可用集群，一旦etcd quorum丢失，集群管控将会直接不可用。因此正常情况下管控的恢复要更加的复杂。\nwdrip精心抽象了管控的恢复流程，操作前会尽量确保etcd quorum始终保持在可用状态。但仍然具备在quorum丢失的情况下从混乱中使用snapshot恢复集群管控。\n\n通常可以通过以下方法模拟管控故障：\n1. 方法一：通过kubectl命令删除master node。\n2. 方法二：登录Master节点ECS，stop ETCD服务，删除etcd数据，stop kubelet服务，stop docker服务等。\n3. 方法三：登录ECS控制台【关闭】一台Master节点ECS。\n4. 方法四：登录ECS控制台【删除】一台Master节点ECS。\n\n对于3个master或更多个master副本的集群，你可以操作让一台ECS节点故障，等待查看系统如何回复到稳定状态，这个过程中整个系统全程仍然可以提供服务。重要的是，你可以操作多台Master ECS节点，让他们同时处于故障状态，这时候系统大概率会因为quorum丢失从而造成集群不可用，但你仍然能够观察到wdrip会通过各种手段将集群恢复到正常的稳定状态，并且无需人工干预。\n对于worker节点，只要不涉及到管控面访问相关的业务，恢复的过程中用户的业务将不受任何影响，实现业务的连续性。\n\n### 场景三：可用区故障\n虽然可用区级别的故障的概率非常低，但我们仍然在设计的时候考虑到这种场景，wdrip支持创建跨可用区的集群，添加多个不同可用区的节点池可以让节点分布到不同可用区，提高集群面对可用区故障时候的韧性。\n\n当管控所在的可用区出现故障时候，集群不可用，集群上的应用大概率也会不可用，wdrip支持snapshot整个k8s集群，然后在其他可用区使用该snapshot一键重新拉起一个一模一样的集群，这种恢复方式保留了所有原集群的配置，唯一变化的是承载集群运行的基础设施，如ECS、SLB等等。相当于在一个新的克隆体中恢复之前备份的大脑，意识下载。\n```shell\n(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120\n```\n\n### 场景四：Region故障\n整个Region发生故障的概率应该是非常非常的低的了，但wdrip设计上仍然能够处理这种灾难。wdrip在运行时会定期备份集群的状态到OSS（可以跨Region写多份实现高可用），当Region出现故障的时候，可以尝试在另外一个Region创建一个新的集群，然后通过备份的snapshot快速恢复业务。参考可用区故障。但目前很多存储资源是可用区级别的，因此还需要进一步考虑存储的跨Region恢复。\n\n\n### 场景五：删库跑路\n一直想提的一个场景就是删库跑路。wdrip定期备份了整个集群的意识，无论你怎么折腾集群，只要备份存在，意识就存在，wdrip就可以再完完全全的还原出一个集群的克隆体出来，恢复现场到以前的状态。\n\n**删库跑路？不存在的。**\n\n你可以尝试删除集群试试，删除方法\n1. 到ROS控制台删除该集群对应的ROS模板，按名称搜索即可找到对应的模板\n2. wdrip delete -n kubernetes-wdrip-120\n\n恢复的方法：\n1. 创建新的集群。wdrip create --config cluster.yml\n2. 恢复集群的意识备份到这个新建的集群本体上。 wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120\n\n\n### 灾难恢复（手动）\n```bash\n# -n 指定恢复的目标集群（本体）， -f 指定恢复的配置来源于哪个集群（意识）。\n(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120\n\n```\n\n### ChaosMonkey\n为了测试基础设施复原力我们专门编写了ChaosMonkey部署在集群中，他能够定期破坏集群(删除集群ECS资源、停止关键组件)，等待系统自愈。\n\n如何使用\n```shell\n(base) ➜  export CLUSTER_NAME=xxxx\n(base) ➜  bash hack/wdrip.sh config\n(base) ➜  bash hack/wdrip.sh chaos\n(base) ➜  kubectl --kubeconfig ~/.kube/config.wdrip -n kube-system get po|grep monkey\n```\n\n## 小结\n\nk8s的复杂性虽然让让人生畏，但wdrip提供的复原力让我们在面对危机的时候能够更加的从容。\n","slug":"遇见复原力","published":1,"updated":"2022-06-08T12:10:56.053Z","_id":"cl453lgm90000wgrh72650o48","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"复原力\"><a href=\"#复原力\" class=\"headerlink\" title=\"复原力\"></a>复原力</h2><p>复原力一词来源于微软创新研究院，全称是社会复原力，它研究的是通过大规模部署可以适应未来各种变数以及不确定性的创新技术来应对社会的各种潜在危机，从而增强社会复原力。如COVID-19、全球气候变暖、环境污染，探索在危机来临前中后的创新技术储备，快速应对方法，快速复原的能力。是一种从无序和混乱中回归秩序的能力。这也是运维所要解决的问题，让系统回归秩序。</p>\n<p>本文探索的是云原生时代下容器基础设施在面临各种危机时的复原力问题，即基础设施复原力。</p>\n<span id=\"more\"></span>\n\n<h2 id=\"基础设施复原力\"><a href=\"#基础设施复原力\" class=\"headerlink\" title=\"基础设施复原力\"></a>基础设施复原力</h2><p>随着容器技术的不断发展，k8s已经成为了云原生时代下的基础设施，它为应用向下屏蔽了云上基础设施的复杂性，向上提供了应用的全生命周期管理能力，将分布式应用管理的复杂性收敛转化到了分布式系统本身的管理问题上。然而，它虽然解决了应用在分布式环境下的编排问题，但自身的分布式系统的运维复杂性仍然存在。底层基础设施失效，可用区故障，节点故障，组件失效，网络分区，抖动，甚至地震、火灾等风险始终存在，如何保证系统的稳定性以及面对这些危机时候的系统复原力是一项非常具有挑战的工作，这也构成了云原生时代下的基础设施复原力问题。</p>\n<p>事实上，k8s面向终态的设计已经让应用具备了从外部扰动中回到稳态的能力，面对应用进程异常退出，容器失败，网络故障，配置紊乱等问题，k8s能够实现检测并自行原地恢复，或者替换掉失效应用副本来使应用处于预期状态，这是k8s的应用复原力。通过以上理念，我们希望云原生时代的基础设施也能具备从外部扰动中自动回归稳态的能力，同时以一种尽可能小的代价去解决k8s平台自身运维复杂性的问题，因此我们设计了<code>wdrip</code>，以一种逐渐收敛的方案来解决k8s分布式系统的自动化运维问题，实现基础设施的复原力。</p>\n<h2 id=\"故障复原场景示例\"><a href=\"#故障复原场景示例\" class=\"headerlink\" title=\"故障复原场景示例\"></a>故障复原场景示例</h2><p>k8s 面向终态的设计为应用的复原力提供了支撑。从终态设计上你能看到k8s 让应用从外部扰动中回归稳态的思想, wdrip也同样采用面向终态的思想，在基础设施层解决k8s的自运维问题。下面会演示一些常见的k8s故障场景，并且观察系统是如何在无人为干预的情况下自动恢复的。</p>\n<h3 id=\"前置条件\"><a href=\"#前置条件\" class=\"headerlink\" title=\"前置条件\"></a>前置条件</h3><p>wdrip的基本功能及其使用，请移步<a href=\"/2022/06/08/%E7%AE%A1%E7%90%86%E9%9B%86%E7%BE%A4/\" title=\"管理集群\">管理集群</a></p>\n<ul>\n<li><p>请使用wdrip创建一个k8s集群。</p>\n</li>\n<li><p>请部署示例应用，或者部署您自己觉得合适的[容器化应用]<a href=\"/2022/06/08/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/\" title=\"云原生应用示例\">云原生应用示例</a>。</p>\n</li>\n</ul>\n<h3 id=\"场景一：节点故障\"><a href=\"#场景一：节点故障\" class=\"headerlink\" title=\"场景一：节点故障\"></a>场景一：节点故障</h3><p>节点故障是k8s集群故障里最常见的一种场景了，也是危害相对较小的故障，wdrip可以侦测节点的异常状态，然后尝试修复。修复的流程如下<br>如果节点状态变为NotReady，那么等待2个heartbeat周期</p>\n<ul>\n<li>如果周期内持续NotReady，wdrip尝试重启kubelet，并等待节点Ready</li>\n<li>如果节点仍然NotReady，wdrip尝试驱逐节点Pod，然后继续尝试重启ECS，并等待节点Ready</li>\n<li>如果节点仍然NotReady，wdrip继续尝试重置ECS（替换节点的系统盘，并重新初始化节点），并等待节点Ready</li>\n<li>如果节点仍然NotReady，wdrip删除这个节点，等待系统重启创建一个全新的节点副本。</li>\n<li>如果节点仍然NotReady，修复失败。等待throttle周期后重复以上步骤。</li>\n</ul>\n<p>通常可以通过以下手段模拟节点故障：</p>\n<ol>\n<li>方法一：通过<code>kubectl delete no $&#123;your-node&#125;</code> 删除对应的Node Object。等待节点自动恢复并注册回来，恢复全程无需人工干预，时长约2分钟。</li>\n<li>方法二：登录ECS节点，systemctl stop kubelet&#x2F;docker等。等待节点NotReady后，系统会自行恢复节点状态。恢复全程无需人工干预。</li>\n<li>方法三：到ECS控制台上【关闭】节点对应的ECS。系统会自行恢复节点状态。恢复全程无需人工干预。</li>\n<li>方法四：到ECS控制台上【删除】节点对应的ECS。系统会自行恢复节点状态。恢复全程无需人工干预。</li>\n</ol>\n<p>[去试一试]<a href=\"/2022/06/08/%E7%AE%A1%E7%90%86%E9%9B%86%E7%BE%A4/\" title=\"管理集群\">管理集群</a></p>\n<h3 id=\"场景二：管控故障\"><a href=\"#场景二：管控故障\" class=\"headerlink\" title=\"场景二：管控故障\"></a>场景二：管控故障</h3><p>管控故障通常情况下是比较严重的，k8s集群依赖etcd组成高可用集群，一旦etcd quorum丢失，集群管控将会直接不可用。因此正常情况下管控的恢复要更加的复杂。<br>wdrip精心抽象了管控的恢复流程，操作前会尽量确保etcd quorum始终保持在可用状态。但仍然具备在quorum丢失的情况下从混乱中使用snapshot恢复集群管控。</p>\n<p>通常可以通过以下方法模拟管控故障：</p>\n<ol>\n<li>方法一：通过kubectl命令删除master node。</li>\n<li>方法二：登录Master节点ECS，stop ETCD服务，删除etcd数据，stop kubelet服务，stop docker服务等。</li>\n<li>方法三：登录ECS控制台【关闭】一台Master节点ECS。</li>\n<li>方法四：登录ECS控制台【删除】一台Master节点ECS。</li>\n</ol>\n<p>对于3个master或更多个master副本的集群，你可以操作让一台ECS节点故障，等待查看系统如何回复到稳定状态，这个过程中整个系统全程仍然可以提供服务。重要的是，你可以操作多台Master ECS节点，让他们同时处于故障状态，这时候系统大概率会因为quorum丢失从而造成集群不可用，但你仍然能够观察到wdrip会通过各种手段将集群恢复到正常的稳定状态，并且无需人工干预。<br>对于worker节点，只要不涉及到管控面访问相关的业务，恢复的过程中用户的业务将不受任何影响，实现业务的连续性。</p>\n<h3 id=\"场景三：可用区故障\"><a href=\"#场景三：可用区故障\" class=\"headerlink\" title=\"场景三：可用区故障\"></a>场景三：可用区故障</h3><p>虽然可用区级别的故障的概率非常低，但我们仍然在设计的时候考虑到这种场景，wdrip支持创建跨可用区的集群，添加多个不同可用区的节点池可以让节点分布到不同可用区，提高集群面对可用区故障时候的韧性。</p>\n<p>当管控所在的可用区出现故障时候，集群不可用，集群上的应用大概率也会不可用，wdrip支持snapshot整个k8s集群，然后在其他可用区使用该snapshot一键重新拉起一个一模一样的集群，这种恢复方式保留了所有原集群的配置，唯一变化的是承载集群运行的基础设施，如ECS、SLB等等。相当于在一个新的克隆体中恢复之前备份的大脑，意识下载。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"场景四：Region故障\"><a href=\"#场景四：Region故障\" class=\"headerlink\" title=\"场景四：Region故障\"></a>场景四：Region故障</h3><p>整个Region发生故障的概率应该是非常非常的低的了，但wdrip设计上仍然能够处理这种灾难。wdrip在运行时会定期备份集群的状态到OSS（可以跨Region写多份实现高可用），当Region出现故障的时候，可以尝试在另外一个Region创建一个新的集群，然后通过备份的snapshot快速恢复业务。参考可用区故障。但目前很多存储资源是可用区级别的，因此还需要进一步考虑存储的跨Region恢复。</p>\n<h3 id=\"场景五：删库跑路\"><a href=\"#场景五：删库跑路\" class=\"headerlink\" title=\"场景五：删库跑路\"></a>场景五：删库跑路</h3><p>一直想提的一个场景就是删库跑路。wdrip定期备份了整个集群的意识，无论你怎么折腾集群，只要备份存在，意识就存在，wdrip就可以再完完全全的还原出一个集群的克隆体出来，恢复现场到以前的状态。</p>\n<p><strong>删库跑路？不存在的。</strong></p>\n<p>你可以尝试删除集群试试，删除方法</p>\n<ol>\n<li>到ROS控制台删除该集群对应的ROS模板，按名称搜索即可找到对应的模板</li>\n<li>wdrip delete -n kubernetes-wdrip-120</li>\n</ol>\n<p>恢复的方法：</p>\n<ol>\n<li>创建新的集群。wdrip create –config cluster.yml</li>\n<li>恢复集群的意识备份到这个新建的集群本体上。 wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120</li>\n</ol>\n<h3 id=\"灾难恢复（手动）\"><a href=\"#灾难恢复（手动）\" class=\"headerlink\" title=\"灾难恢复（手动）\"></a>灾难恢复（手动）</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -n 指定恢复的目标集群（本体）， -f 指定恢复的配置来源于哪个集群（意识）。</span></span><br><span class=\"line\">(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"ChaosMonkey\"><a href=\"#ChaosMonkey\" class=\"headerlink\" title=\"ChaosMonkey\"></a>ChaosMonkey</h3><p>为了测试基础设施复原力我们专门编写了ChaosMonkey部署在集群中，他能够定期破坏集群(删除集群ECS资源、停止关键组件)，等待系统自愈。</p>\n<p>如何使用</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜  export CLUSTER_NAME=xxxx</span><br><span class=\"line\">(base) ➜  bash hack/wdrip.sh config</span><br><span class=\"line\">(base) ➜  bash hack/wdrip.sh chaos</span><br><span class=\"line\">(base) ➜  kubectl --kubeconfig ~/.kube/config.wdrip -n kube-system get po|grep monkey</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h2><p>k8s的复杂性虽然让让人生畏，但wdrip提供的复原力让我们在面对危机的时候能够更加的从容。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"复原力\"><a href=\"#复原力\" class=\"headerlink\" title=\"复原力\"></a>复原力</h2><p>复原力一词来源于微软创新研究院，全称是社会复原力，它研究的是通过大规模部署可以适应未来各种变数以及不确定性的创新技术来应对社会的各种潜在危机，从而增强社会复原力。如COVID-19、全球气候变暖、环境污染，探索在危机来临前中后的创新技术储备，快速应对方法，快速复原的能力。是一种从无序和混乱中回归秩序的能力。这也是运维所要解决的问题，让系统回归秩序。</p>\n<p>本文探索的是云原生时代下容器基础设施在面临各种危机时的复原力问题，即基础设施复原力。</p>","more":"<h2 id=\"基础设施复原力\"><a href=\"#基础设施复原力\" class=\"headerlink\" title=\"基础设施复原力\"></a>基础设施复原力</h2><p>随着容器技术的不断发展，k8s已经成为了云原生时代下的基础设施，它为应用向下屏蔽了云上基础设施的复杂性，向上提供了应用的全生命周期管理能力，将分布式应用管理的复杂性收敛转化到了分布式系统本身的管理问题上。然而，它虽然解决了应用在分布式环境下的编排问题，但自身的分布式系统的运维复杂性仍然存在。底层基础设施失效，可用区故障，节点故障，组件失效，网络分区，抖动，甚至地震、火灾等风险始终存在，如何保证系统的稳定性以及面对这些危机时候的系统复原力是一项非常具有挑战的工作，这也构成了云原生时代下的基础设施复原力问题。</p>\n<p>事实上，k8s面向终态的设计已经让应用具备了从外部扰动中回到稳态的能力，面对应用进程异常退出，容器失败，网络故障，配置紊乱等问题，k8s能够实现检测并自行原地恢复，或者替换掉失效应用副本来使应用处于预期状态，这是k8s的应用复原力。通过以上理念，我们希望云原生时代的基础设施也能具备从外部扰动中自动回归稳态的能力，同时以一种尽可能小的代价去解决k8s平台自身运维复杂性的问题，因此我们设计了<code>wdrip</code>，以一种逐渐收敛的方案来解决k8s分布式系统的自动化运维问题，实现基础设施的复原力。</p>\n<h2 id=\"故障复原场景示例\"><a href=\"#故障复原场景示例\" class=\"headerlink\" title=\"故障复原场景示例\"></a>故障复原场景示例</h2><p>k8s 面向终态的设计为应用的复原力提供了支撑。从终态设计上你能看到k8s 让应用从外部扰动中回归稳态的思想, wdrip也同样采用面向终态的思想，在基础设施层解决k8s的自运维问题。下面会演示一些常见的k8s故障场景，并且观察系统是如何在无人为干预的情况下自动恢复的。</p>\n<h3 id=\"前置条件\"><a href=\"#前置条件\" class=\"headerlink\" title=\"前置条件\"></a>前置条件</h3><p>wdrip的基本功能及其使用，请移步<a href=\"/2022/06/08/%E7%AE%A1%E7%90%86%E9%9B%86%E7%BE%A4/\" title=\"管理集群\">管理集群</a></p>\n<ul>\n<li><p>请使用wdrip创建一个k8s集群。</p>\n</li>\n<li><p>请部署示例应用，或者部署您自己觉得合适的[容器化应用]<a href=\"/2022/06/08/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/\" title=\"云原生应用示例\">云原生应用示例</a>。</p>\n</li>\n</ul>\n<h3 id=\"场景一：节点故障\"><a href=\"#场景一：节点故障\" class=\"headerlink\" title=\"场景一：节点故障\"></a>场景一：节点故障</h3><p>节点故障是k8s集群故障里最常见的一种场景了，也是危害相对较小的故障，wdrip可以侦测节点的异常状态，然后尝试修复。修复的流程如下<br>如果节点状态变为NotReady，那么等待2个heartbeat周期</p>\n<ul>\n<li>如果周期内持续NotReady，wdrip尝试重启kubelet，并等待节点Ready</li>\n<li>如果节点仍然NotReady，wdrip尝试驱逐节点Pod，然后继续尝试重启ECS，并等待节点Ready</li>\n<li>如果节点仍然NotReady，wdrip继续尝试重置ECS（替换节点的系统盘，并重新初始化节点），并等待节点Ready</li>\n<li>如果节点仍然NotReady，wdrip删除这个节点，等待系统重启创建一个全新的节点副本。</li>\n<li>如果节点仍然NotReady，修复失败。等待throttle周期后重复以上步骤。</li>\n</ul>\n<p>通常可以通过以下手段模拟节点故障：</p>\n<ol>\n<li>方法一：通过<code>kubectl delete no $&#123;your-node&#125;</code> 删除对应的Node Object。等待节点自动恢复并注册回来，恢复全程无需人工干预，时长约2分钟。</li>\n<li>方法二：登录ECS节点，systemctl stop kubelet&#x2F;docker等。等待节点NotReady后，系统会自行恢复节点状态。恢复全程无需人工干预。</li>\n<li>方法三：到ECS控制台上【关闭】节点对应的ECS。系统会自行恢复节点状态。恢复全程无需人工干预。</li>\n<li>方法四：到ECS控制台上【删除】节点对应的ECS。系统会自行恢复节点状态。恢复全程无需人工干预。</li>\n</ol>\n<p>[去试一试]<a href=\"/2022/06/08/%E7%AE%A1%E7%90%86%E9%9B%86%E7%BE%A4/\" title=\"管理集群\">管理集群</a></p>\n<h3 id=\"场景二：管控故障\"><a href=\"#场景二：管控故障\" class=\"headerlink\" title=\"场景二：管控故障\"></a>场景二：管控故障</h3><p>管控故障通常情况下是比较严重的，k8s集群依赖etcd组成高可用集群，一旦etcd quorum丢失，集群管控将会直接不可用。因此正常情况下管控的恢复要更加的复杂。<br>wdrip精心抽象了管控的恢复流程，操作前会尽量确保etcd quorum始终保持在可用状态。但仍然具备在quorum丢失的情况下从混乱中使用snapshot恢复集群管控。</p>\n<p>通常可以通过以下方法模拟管控故障：</p>\n<ol>\n<li>方法一：通过kubectl命令删除master node。</li>\n<li>方法二：登录Master节点ECS，stop ETCD服务，删除etcd数据，stop kubelet服务，stop docker服务等。</li>\n<li>方法三：登录ECS控制台【关闭】一台Master节点ECS。</li>\n<li>方法四：登录ECS控制台【删除】一台Master节点ECS。</li>\n</ol>\n<p>对于3个master或更多个master副本的集群，你可以操作让一台ECS节点故障，等待查看系统如何回复到稳定状态，这个过程中整个系统全程仍然可以提供服务。重要的是，你可以操作多台Master ECS节点，让他们同时处于故障状态，这时候系统大概率会因为quorum丢失从而造成集群不可用，但你仍然能够观察到wdrip会通过各种手段将集群恢复到正常的稳定状态，并且无需人工干预。<br>对于worker节点，只要不涉及到管控面访问相关的业务，恢复的过程中用户的业务将不受任何影响，实现业务的连续性。</p>\n<h3 id=\"场景三：可用区故障\"><a href=\"#场景三：可用区故障\" class=\"headerlink\" title=\"场景三：可用区故障\"></a>场景三：可用区故障</h3><p>虽然可用区级别的故障的概率非常低，但我们仍然在设计的时候考虑到这种场景，wdrip支持创建跨可用区的集群，添加多个不同可用区的节点池可以让节点分布到不同可用区，提高集群面对可用区故障时候的韧性。</p>\n<p>当管控所在的可用区出现故障时候，集群不可用，集群上的应用大概率也会不可用，wdrip支持snapshot整个k8s集群，然后在其他可用区使用该snapshot一键重新拉起一个一模一样的集群，这种恢复方式保留了所有原集群的配置，唯一变化的是承载集群运行的基础设施，如ECS、SLB等等。相当于在一个新的克隆体中恢复之前备份的大脑，意识下载。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"场景四：Region故障\"><a href=\"#场景四：Region故障\" class=\"headerlink\" title=\"场景四：Region故障\"></a>场景四：Region故障</h3><p>整个Region发生故障的概率应该是非常非常的低的了，但wdrip设计上仍然能够处理这种灾难。wdrip在运行时会定期备份集群的状态到OSS（可以跨Region写多份实现高可用），当Region出现故障的时候，可以尝试在另外一个Region创建一个新的集群，然后通过备份的snapshot快速恢复业务。参考可用区故障。但目前很多存储资源是可用区级别的，因此还需要进一步考虑存储的跨Region恢复。</p>\n<h3 id=\"场景五：删库跑路\"><a href=\"#场景五：删库跑路\" class=\"headerlink\" title=\"场景五：删库跑路\"></a>场景五：删库跑路</h3><p>一直想提的一个场景就是删库跑路。wdrip定期备份了整个集群的意识，无论你怎么折腾集群，只要备份存在，意识就存在，wdrip就可以再完完全全的还原出一个集群的克隆体出来，恢复现场到以前的状态。</p>\n<p><strong>删库跑路？不存在的。</strong></p>\n<p>你可以尝试删除集群试试，删除方法</p>\n<ol>\n<li>到ROS控制台删除该集群对应的ROS模板，按名称搜索即可找到对应的模板</li>\n<li>wdrip delete -n kubernetes-wdrip-120</li>\n</ol>\n<p>恢复的方法：</p>\n<ol>\n<li>创建新的集群。wdrip create –config cluster.yml</li>\n<li>恢复集群的意识备份到这个新建的集群本体上。 wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120</li>\n</ol>\n<h3 id=\"灾难恢复（手动）\"><a href=\"#灾难恢复（手动）\" class=\"headerlink\" title=\"灾难恢复（手动）\"></a>灾难恢复（手动）</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -n 指定恢复的目标集群（本体）， -f 指定恢复的配置来源于哪个集群（意识）。</span></span><br><span class=\"line\">(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"ChaosMonkey\"><a href=\"#ChaosMonkey\" class=\"headerlink\" title=\"ChaosMonkey\"></a>ChaosMonkey</h3><p>为了测试基础设施复原力我们专门编写了ChaosMonkey部署在集群中，他能够定期破坏集群(删除集群ECS资源、停止关键组件)，等待系统自愈。</p>\n<p>如何使用</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜  export CLUSTER_NAME=xxxx</span><br><span class=\"line\">(base) ➜  bash hack/wdrip.sh config</span><br><span class=\"line\">(base) ➜  bash hack/wdrip.sh chaos</span><br><span class=\"line\">(base) ➜  kubectl --kubeconfig ~/.kube/config.wdrip -n kube-system get po|grep monkey</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h2><p>k8s的复杂性虽然让让人生畏，但wdrip提供的复原力让我们在面对危机的时候能够更加的从容。</p>"},{"title":"管理集群","date":"2022-06-08T10:56:51.000Z","_content":"\n\n## 准备工作\n**安装wdrip**\n下载最新版本wdrip.当前版本0.1.1\n```bash\n(base) ➜ curl -sSL --retry 3 https://host-wdrip-cn-hangzhou.oss-cn-hangzhou.aliyuncs.com/wdrip/install.sh |bash\n(base) ➜ ls -lht /usr/local/bin/wdrip\n\n# use wdrip -h to see wdrip help command\n(base) ➜ wdrip -h\n\nwdrip creates and manages infrastructure agnostic Kubernetes clusters\n            _         _\n           | |       (_)\n _ _ _   __| |  ____  _  ____\n| | | | / _  | / ___)| ||  _ \\\n| | | |( (_| || |    | || |_| |\n \\___/  \\____||_|    |_||  __/\n                        |_|\n\n\nwdrip creates and manages infrastructure agnostic Kubernetes clusters and empower strong auto heal ability and easy recovery\n\nUsage:\n  wdrip [command]\n\nAvailable Commands:\n  bootstrap   Bootstrap a Kubernetes cluster\n  build       Kubernetes cluster build package\n  ......\n\nUse \"wdrip [command] --help\" for more information about a command.\n```\n\n<!-- more -->\n\n**配置wdrip**\nwdrip 目前仅支持阿里云上管理k8s集群，更多的CloudProvider未来会逐步加入。\nwdrip 需要您的阿里云账号信息来帮助您管理您的云上k8s资源。将`replace-with-your-own-accessKeyId`及`replace-with-your-own-accessKeySecret`替换成您自己的主账号AK信息。\nwdrip 会额外为您创建OSS bucket，用来备份集群，用来紧急修复。bucket名称见下面的`wdrip-index`\n```bash\n(base) ➜ vi ~/.wdrip/config\n\napiVersion: alibabacloud.com/v1\ncontexts:\n- context:\n    provider-key: alibaba.dev\n  name: devEnv\ncurrent-context: devEnv\nkind: Config\nproviders:\n- name: alibaba.dev\n  provider:\n    name: alibaba\n    value:\n      accessKeyId: {replace-with-your-own-accessKeyId}\n      accessKeySecret: {replace-with-your-own-accessKeySecret}\n      bucketName: wdrip-index\n      region: cn-hangzhou\n```\n\n\n## 创建集群\nwdrip遵循结构化原则，最小核心原则，模块化设计，因此具有非常高的灵活性。\nwdrip会首先在云上初始化一个单Master节点的k8s集群（最小可用原则），这个阶段的速度最快，并且具有最小的故障面，具有最高的系统稳定性。\n以下配置为您创建一个最小k8s集群，仅有一个master节点。预计3分钟内完成。\n\n\n```bash\n(base) ➜ export CLUSTER_NAME=kubernetes-id-001 \\\n              REGION=cn-hangzhou ZONE_ID=cn-hangzhou-k \\\n              IMAGE_ID=centos_7_9_x64_20G_alibase_20210623.vhd \\\n              DISK_TYPE=cloud_essd \\\n              INSTANCE_TYPE=ecs.c6.xlarge \\\n              TOKEN=$(/usr/local/bin/wdrip token new)\n\n(base) ➜ cat > config.yaml << EOF\nclusterid: \"${CLUSTER_NAME}\"\niaas:\n  workerCount: 1\n  image: \"${IMAGE_ID}\"\n  disk:\n    size: 40G\n    type: \"${DISK_TYPE}\"\n  region: \"${REGION}\"\n  zoneid: ${ZONE_ID}\n  instance: \"${INSTANCE_TYPE}\"\nregistry: registry-vpc.${REGION}.aliyuncs.com\nnamespace: default\ncloudType: public\nkubernetes:\n  name: kubernetes\n  version: 1.20.4-aliyun.1\n  kubeadmToken: ${TOKEN}\netcd:\n  name: etcd\n  version: v3.4.3\nruntime:\n  name: docker\n  version: 19.03.5\nsans:\n  - 192.168.0.1\nnetwork:\n  mode: ipvs\n  podcidr: 172.16.0.1/16\n  svccidr: 172.19.0.1/20\n  domain: cluster.domain\n  netMask: 25\nEOF\n\n(base) ➜ wdrip create --config config.yaml\n\nwdrip: kubernetes cluster lifecycle management.\n            _         _\n           | |       (_)\n _ _ _   __| |  ____  _  ____\n| | | | / _  | / ___)| ||  _ \\\n| | | |( (_| || |    | || |_| |\n \\___/  \\____||_|    |_||  __/\n                        |_|\n\nI1002 15:31:37.593094   96000 provider.go:268] use HOME dir: [/Users/aoxn]\nI1002 15:31:37.593265   96000 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config\nI1002 15:31:37.595669   96000 provider.go:52] use command line config as bootconfig: [config.yaml] with provider[alibaba]\nI1002 15:31:37.726288   96000 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]\n....\n\nI1002 15:31:41.989201   96000 iaas.go:96] watch cluster create progress with command:  [ wdrip watch --name kubernetes-id-001 ]\n```\n\n### 观测集群的创建过程\n创建的集群是一个异步的过程，因此我们提供了一个watch命令，用来观测创建的进度。执行watch命令前请将terminal窗口最大化，保证最佳输出效果。\n```bash\n(base) ➜  wdrip watch --name kubernetes-id-001\nI1002 15:42:05.600911   96142 provider.go:268] use HOME dir: [/Users/aoxn]\nI1002 15:42:05.601063   96142 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config\n✓ 【ALIYUN::ROS::Stack                  】(kubernetes-id-001         ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:42 2021-10-02T15:35:52\n✓ 【ALIYUN::ROS::WaitCondition          】(k8s_master_waiter         ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:52 2021-10-02T15:35:51\n\n.....\n\n✓ 【ALIYUN::RAM::Role                   】(KubernetesWorkerRole      ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:42 2021-10-02T15:31:52\n✓ 【WDRIP::MESSAGE::OUTPUT              】(extra_mesage_id           ) [CREATE_COMPLETE,23, 23]  TimeElapse: 251s\nI1002 15:42:14.446231   96142 ros.go:477] ===========================================================\nI1002 15:42:14.446254   96142 ros.go:478] StackName: kubernetes-id-001\nI1002 15:42:14.446259   96142 ros.go:479]   StackId: 2d302c6c-24b3-4535-8875-8c7dd9a48bd7\n\n```\n\n### 查看集群列表\nwdrip提供了命令用来查看本账号的provider所创建的集群列表\n```bash\n(base) ➜ wdrip get\nI1002 16:16:59.615092   97592 provider.go:268] use HOME dir: [/Users/aoxn]\nI1002 16:16:59.615225   97592 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config\nI1002 16:16:59.670775   97592 index.go:76] get cluster: [wdrip/clusters/kubernetes-id-001.json]\nI1002 16:16:59.670802   97592 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]\nI1002 16:16:59.735396   97592 index.go:76] get cluster: [wdrip/clusters/kubernetes-wdrip-77.json]\nI1002 16:16:59.735420   97592 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-wdrip-77.json]\nI1002 16:16:59.807551   97592 iaas.go:190]\nNAME                  ENDPOINT\nkubernetes-id-001     47.96.27.46/192.168.0.75\nkubernetes-wdrip-77   116.62.24.127/192.168.0.53\n```\n通过`wdrip get -n kubernetes-id-001 -o yaml` 可以查看该集群的详细信息\n\n### 连接集群\n\n当集群创建完成后,可以通过wdrip get命令下载kubeconfig文件来访问我们的集群。 当前wdrip创建的集群通过EIP在公网暴露了apiserver，因此可以通过公网本地访问。\n\n```bash\n(base) ➜ wdrip get -r kubeconfig -n kubernetes-id-001 -w ~/.kube/config.txt\n\nI1002 16:14:38.136200   97541 provider.go:268] use HOME dir: [/Users/aoxn]\nI1002 16:14:38.136349   97541 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config\nI1002 16:14:38.136730   97541 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]\nI1002 16:14:40.241796   97541 sign.go:223] sign kubernetes: []\nI1002 16:14:40.612558   97541 iaas.go:301] write kubeconfig to file [/Users/aoxn/.kube/config.txt]\n\n(base) ➜  kubectl --kubeconfig ~/.kube/config.txt get no\n\nNAME                                  STATUS   ROLES                  AGE    VERSION\n192.168.0.77.i-bp12a2wcmbrd4383cai3   Ready    control-plane,master   2m2s   v1.20.4-aliyun.1\n```\n\n## 添加工作节点\n### 创建节点池\nwdrip 提供了NodePool的概念，将一组具有相同的配置的节点组作为一个节点池统一管理。通过以下yaml可以创建具有N个节点的节点池。\n\n```bash\n(base) ➜ kubectl --kubeconfig ~/.kube/config.txt apply -f - <<EOF\napiVersion: alibabacloud.com/v1\nkind: NodePool\nmetadata:\n  name: nodepool-01\n  namespace: kube-system\nspec:\n  id: \"very-long-id-xxxxx\"\n  infra:\n    desiredCapacity: 1\n    cpu: 4\n    memory: 8\n    imageId: centos_7_9_x64_20G_alibase_20210623.vhd\nEOF\n\n# output\nnodepool.alibabacloud.com/nodepool-01 created\n```\n\n通过`kubectl --kubeconfig ~/.kube/config.txt get no -w` 观测节点的创建。 节点的创建及加入过程大约需要等待90s。请等待\n\n同样的方式可以创建多个具有不同配置的节点池。\n\n### 扩容节点池\n节点池的管理规划是完全面向终态的，但目前的实现还在早期阶段，您可以通过edit对应的NodePool的CRD的desiredCapacity的值来调整节点池的节点数量。\n观测节点池的加入的过程参考上面方法。可以调大，也可以调小节点池的大小。\n自动扩容节点池的功能Coming Soon。\n\n```bash\n(base) ➜ kubectl --kubeconfig ~/.kube/config.txt edit nodepool default-nodepool\n```\n\n## 集群高可用\n一切都是面向终态的\n\n### 构建高可用的k8s集群\n单个Master的k8s集群不具备高可用，但已足够用作测试集群，经济适用。wdrip同样提供了灵活简单的方式将已有集群扩展成高可用集群。wdrip适用`MasterSet`的CRD资源代表Master节点组。\n以下命令扩展当前集群的Master副本数量到3个。\n\n```bash\n(base) ➜ kubectl --kubeconfig ~/.kube/config.txt edit masterset\napiVersion: alibabacloud.com/v1\nkind: MasterSet\nmetadata:\n  # 请不要改masterset这个名称，没做支持，也没意义。\n  name: masterset\n  namespace: kube-system\nspec:\n  replicas: 3\n```\nwdrip需要2分钟左右的时间来初始化额外的2个Master节点，请等待。可以通过`kubectl --kubeconfig ~/.kube/config.txt get no -w` 观测Master节点的加入过程。\n\n### 大规模集群场景\n当你的集群规模进一步扩大后，3个Master已经不能满足你的需求了，那么你也可以通过MasterSet的`replicas`方便快速的将Master副本数量调整成您喜欢的任意的数量，不过一般不建议超过7个。\n\n```bash\n(base) ➜ kubectl --kubeconfig ~/.kube/config.txt -n kube-system edit masterset\n# set replicas to 5\n```\n然后通过上面的命令观测Master节点数量的变化，同样需要等待大约2分钟时间。\n\n\n### 缩减集群规模\n一切都是面向终态的，当您的集群规模降低后，不在需要这么多的Master后，同样可以通过调整MasterSet的`replicas`来调整Master的数量。可以调整到3个副本，也可以调整到1个Master副本。Master缩减的过程中需要保持多数quorum，因此缩减是逐步发生的。\n\n```bash\n(base) ➜ kubectl --kubeconfig ~/.kube/config.txt -n kube-system edit masterset\n# set replicas to 3 or 1\n```\n然后通过上面的命令观测Master节点数量的变化。\n\n## 集群灾难恢复\n### 备份机制\nwdrip定期备份您的集群的k8s的etcd数据，用于发生集群级别故障的时候快速恢复。默认每10分钟备份一次，保留最近4个备份副本。\n您可用通过`wdrip get -r backup`来查看当前的备份信息。\n```bash\n(base) ➜ wdrip get -r backup -n kubernetes-id-001\nI1002 18:28:13.072037   98760 provider.go:268] use HOME dir: [/Users/aoxn]\nI1002 18:28:13.072184   98760 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config\nI1002 18:28:13.072557   98760 oss.go:32] oss get object from [oss://wdrip-index/wdrip/backup/kubernetes-id-001/index.json]\nI1002 18:28:13.140780   98760 iaas.go:243]\nNAME                PREFIX              DATE                PATH\nkubernetes-id-001   wdrip/backup          20211002-1024       wdrip/backup/kubernetes-id-001/20211002-1024/snapshot.db\nkubernetes-id-001   wdrip/backup          20211002-1014       wdrip/backup/kubernetes-id-001/20211002-1014/snapshot.db\nkubernetes-id-001   wdrip/backup          20211002-1004       wdrip/backup/kubernetes-id-001/20211002-1004/snapshot.db\nkubernetes-id-001   wdrip/backup          20211002-0954       wdrip/backup/kubernetes-id-001/20211002-0954/snapshot.db\n```\n\n### 恢复场景一：在原基础设施上恢复\n如果k8s因未知因素管控完全故障，wdrip没有能够自行恢复，那么您可以手动触发命令执行恢复。仅需要一行命令即可\n```bash\n# -n 指定恢复的目标集群（本体）。\n(base) ➜ wdrip recover -n kubernetes-wdrip-121\n\n```\n等待大概2到3分钟后，通过kubectl get no 查看节点恢复情况。\n\n### 恢复场景二：恢复到新的基础设施上\n\n有些情况下，集群所对应的基础设施被损坏，人为的或者灾难性的，以至于无法在原有基础设施上恢复，那么我们可以通过新建一个具有相同规格的基础设施的集群，然后在本基础设施上恢复出原有集群的备份意识。\n\n```bash\n# 创建具有相同规格的新的集群kubernetes-wdrip-121\n(base) ➜ wdrip create --config config.yaml\n\n# 将备份的旧的kubernetes-wdrip-120集群配置（意识）恢复到新建的集群121上。\n# -n 指定恢复的目标集群（本体）， -f 指定恢复的配置来源于哪个集群（意识）。\n(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120\n\n```\n\n## 节点修复机制\n节点是运行负载的工具而已，对于失效的节点，替换是成本最小的方案，替换之前我们会尝试重启来恢复。\n\n## 自定义集群参数能力\n规划中（节点、集群）\n\n## 运维简化机制\n节点故障后快速恢复，定位问题很难。 在wdrip这里，你可以直接删掉故障节点，让系统直接拉起一个新的节点副本即可。\n\n","source":"_posts/管理集群.md","raw":"title: 管理集群\ndate: 2022-06-08 18:56:51\ntags:\n---\n\n\n## 准备工作\n**安装wdrip**\n下载最新版本wdrip.当前版本0.1.1\n```bash\n(base) ➜ curl -sSL --retry 3 https://host-wdrip-cn-hangzhou.oss-cn-hangzhou.aliyuncs.com/wdrip/install.sh |bash\n(base) ➜ ls -lht /usr/local/bin/wdrip\n\n# use wdrip -h to see wdrip help command\n(base) ➜ wdrip -h\n\nwdrip creates and manages infrastructure agnostic Kubernetes clusters\n            _         _\n           | |       (_)\n _ _ _   __| |  ____  _  ____\n| | | | / _  | / ___)| ||  _ \\\n| | | |( (_| || |    | || |_| |\n \\___/  \\____||_|    |_||  __/\n                        |_|\n\n\nwdrip creates and manages infrastructure agnostic Kubernetes clusters and empower strong auto heal ability and easy recovery\n\nUsage:\n  wdrip [command]\n\nAvailable Commands:\n  bootstrap   Bootstrap a Kubernetes cluster\n  build       Kubernetes cluster build package\n  ......\n\nUse \"wdrip [command] --help\" for more information about a command.\n```\n\n<!-- more -->\n\n**配置wdrip**\nwdrip 目前仅支持阿里云上管理k8s集群，更多的CloudProvider未来会逐步加入。\nwdrip 需要您的阿里云账号信息来帮助您管理您的云上k8s资源。将`replace-with-your-own-accessKeyId`及`replace-with-your-own-accessKeySecret`替换成您自己的主账号AK信息。\nwdrip 会额外为您创建OSS bucket，用来备份集群，用来紧急修复。bucket名称见下面的`wdrip-index`\n```bash\n(base) ➜ vi ~/.wdrip/config\n\napiVersion: alibabacloud.com/v1\ncontexts:\n- context:\n    provider-key: alibaba.dev\n  name: devEnv\ncurrent-context: devEnv\nkind: Config\nproviders:\n- name: alibaba.dev\n  provider:\n    name: alibaba\n    value:\n      accessKeyId: {replace-with-your-own-accessKeyId}\n      accessKeySecret: {replace-with-your-own-accessKeySecret}\n      bucketName: wdrip-index\n      region: cn-hangzhou\n```\n\n\n## 创建集群\nwdrip遵循结构化原则，最小核心原则，模块化设计，因此具有非常高的灵活性。\nwdrip会首先在云上初始化一个单Master节点的k8s集群（最小可用原则），这个阶段的速度最快，并且具有最小的故障面，具有最高的系统稳定性。\n以下配置为您创建一个最小k8s集群，仅有一个master节点。预计3分钟内完成。\n\n\n```bash\n(base) ➜ export CLUSTER_NAME=kubernetes-id-001 \\\n              REGION=cn-hangzhou ZONE_ID=cn-hangzhou-k \\\n              IMAGE_ID=centos_7_9_x64_20G_alibase_20210623.vhd \\\n              DISK_TYPE=cloud_essd \\\n              INSTANCE_TYPE=ecs.c6.xlarge \\\n              TOKEN=$(/usr/local/bin/wdrip token new)\n\n(base) ➜ cat > config.yaml << EOF\nclusterid: \"${CLUSTER_NAME}\"\niaas:\n  workerCount: 1\n  image: \"${IMAGE_ID}\"\n  disk:\n    size: 40G\n    type: \"${DISK_TYPE}\"\n  region: \"${REGION}\"\n  zoneid: ${ZONE_ID}\n  instance: \"${INSTANCE_TYPE}\"\nregistry: registry-vpc.${REGION}.aliyuncs.com\nnamespace: default\ncloudType: public\nkubernetes:\n  name: kubernetes\n  version: 1.20.4-aliyun.1\n  kubeadmToken: ${TOKEN}\netcd:\n  name: etcd\n  version: v3.4.3\nruntime:\n  name: docker\n  version: 19.03.5\nsans:\n  - 192.168.0.1\nnetwork:\n  mode: ipvs\n  podcidr: 172.16.0.1/16\n  svccidr: 172.19.0.1/20\n  domain: cluster.domain\n  netMask: 25\nEOF\n\n(base) ➜ wdrip create --config config.yaml\n\nwdrip: kubernetes cluster lifecycle management.\n            _         _\n           | |       (_)\n _ _ _   __| |  ____  _  ____\n| | | | / _  | / ___)| ||  _ \\\n| | | |( (_| || |    | || |_| |\n \\___/  \\____||_|    |_||  __/\n                        |_|\n\nI1002 15:31:37.593094   96000 provider.go:268] use HOME dir: [/Users/aoxn]\nI1002 15:31:37.593265   96000 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config\nI1002 15:31:37.595669   96000 provider.go:52] use command line config as bootconfig: [config.yaml] with provider[alibaba]\nI1002 15:31:37.726288   96000 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]\n....\n\nI1002 15:31:41.989201   96000 iaas.go:96] watch cluster create progress with command:  [ wdrip watch --name kubernetes-id-001 ]\n```\n\n### 观测集群的创建过程\n创建的集群是一个异步的过程，因此我们提供了一个watch命令，用来观测创建的进度。执行watch命令前请将terminal窗口最大化，保证最佳输出效果。\n```bash\n(base) ➜  wdrip watch --name kubernetes-id-001\nI1002 15:42:05.600911   96142 provider.go:268] use HOME dir: [/Users/aoxn]\nI1002 15:42:05.601063   96142 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config\n✓ 【ALIYUN::ROS::Stack                  】(kubernetes-id-001         ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:42 2021-10-02T15:35:52\n✓ 【ALIYUN::ROS::WaitCondition          】(k8s_master_waiter         ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:52 2021-10-02T15:35:51\n\n.....\n\n✓ 【ALIYUN::RAM::Role                   】(KubernetesWorkerRole      ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:42 2021-10-02T15:31:52\n✓ 【WDRIP::MESSAGE::OUTPUT              】(extra_mesage_id           ) [CREATE_COMPLETE,23, 23]  TimeElapse: 251s\nI1002 15:42:14.446231   96142 ros.go:477] ===========================================================\nI1002 15:42:14.446254   96142 ros.go:478] StackName: kubernetes-id-001\nI1002 15:42:14.446259   96142 ros.go:479]   StackId: 2d302c6c-24b3-4535-8875-8c7dd9a48bd7\n\n```\n\n### 查看集群列表\nwdrip提供了命令用来查看本账号的provider所创建的集群列表\n```bash\n(base) ➜ wdrip get\nI1002 16:16:59.615092   97592 provider.go:268] use HOME dir: [/Users/aoxn]\nI1002 16:16:59.615225   97592 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config\nI1002 16:16:59.670775   97592 index.go:76] get cluster: [wdrip/clusters/kubernetes-id-001.json]\nI1002 16:16:59.670802   97592 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]\nI1002 16:16:59.735396   97592 index.go:76] get cluster: [wdrip/clusters/kubernetes-wdrip-77.json]\nI1002 16:16:59.735420   97592 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-wdrip-77.json]\nI1002 16:16:59.807551   97592 iaas.go:190]\nNAME                  ENDPOINT\nkubernetes-id-001     47.96.27.46/192.168.0.75\nkubernetes-wdrip-77   116.62.24.127/192.168.0.53\n```\n通过`wdrip get -n kubernetes-id-001 -o yaml` 可以查看该集群的详细信息\n\n### 连接集群\n\n当集群创建完成后,可以通过wdrip get命令下载kubeconfig文件来访问我们的集群。 当前wdrip创建的集群通过EIP在公网暴露了apiserver，因此可以通过公网本地访问。\n\n```bash\n(base) ➜ wdrip get -r kubeconfig -n kubernetes-id-001 -w ~/.kube/config.txt\n\nI1002 16:14:38.136200   97541 provider.go:268] use HOME dir: [/Users/aoxn]\nI1002 16:14:38.136349   97541 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config\nI1002 16:14:38.136730   97541 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]\nI1002 16:14:40.241796   97541 sign.go:223] sign kubernetes: []\nI1002 16:14:40.612558   97541 iaas.go:301] write kubeconfig to file [/Users/aoxn/.kube/config.txt]\n\n(base) ➜  kubectl --kubeconfig ~/.kube/config.txt get no\n\nNAME                                  STATUS   ROLES                  AGE    VERSION\n192.168.0.77.i-bp12a2wcmbrd4383cai3   Ready    control-plane,master   2m2s   v1.20.4-aliyun.1\n```\n\n## 添加工作节点\n### 创建节点池\nwdrip 提供了NodePool的概念，将一组具有相同的配置的节点组作为一个节点池统一管理。通过以下yaml可以创建具有N个节点的节点池。\n\n```bash\n(base) ➜ kubectl --kubeconfig ~/.kube/config.txt apply -f - <<EOF\napiVersion: alibabacloud.com/v1\nkind: NodePool\nmetadata:\n  name: nodepool-01\n  namespace: kube-system\nspec:\n  id: \"very-long-id-xxxxx\"\n  infra:\n    desiredCapacity: 1\n    cpu: 4\n    memory: 8\n    imageId: centos_7_9_x64_20G_alibase_20210623.vhd\nEOF\n\n# output\nnodepool.alibabacloud.com/nodepool-01 created\n```\n\n通过`kubectl --kubeconfig ~/.kube/config.txt get no -w` 观测节点的创建。 节点的创建及加入过程大约需要等待90s。请等待\n\n同样的方式可以创建多个具有不同配置的节点池。\n\n### 扩容节点池\n节点池的管理规划是完全面向终态的，但目前的实现还在早期阶段，您可以通过edit对应的NodePool的CRD的desiredCapacity的值来调整节点池的节点数量。\n观测节点池的加入的过程参考上面方法。可以调大，也可以调小节点池的大小。\n自动扩容节点池的功能Coming Soon。\n\n```bash\n(base) ➜ kubectl --kubeconfig ~/.kube/config.txt edit nodepool default-nodepool\n```\n\n## 集群高可用\n一切都是面向终态的\n\n### 构建高可用的k8s集群\n单个Master的k8s集群不具备高可用，但已足够用作测试集群，经济适用。wdrip同样提供了灵活简单的方式将已有集群扩展成高可用集群。wdrip适用`MasterSet`的CRD资源代表Master节点组。\n以下命令扩展当前集群的Master副本数量到3个。\n\n```bash\n(base) ➜ kubectl --kubeconfig ~/.kube/config.txt edit masterset\napiVersion: alibabacloud.com/v1\nkind: MasterSet\nmetadata:\n  # 请不要改masterset这个名称，没做支持，也没意义。\n  name: masterset\n  namespace: kube-system\nspec:\n  replicas: 3\n```\nwdrip需要2分钟左右的时间来初始化额外的2个Master节点，请等待。可以通过`kubectl --kubeconfig ~/.kube/config.txt get no -w` 观测Master节点的加入过程。\n\n### 大规模集群场景\n当你的集群规模进一步扩大后，3个Master已经不能满足你的需求了，那么你也可以通过MasterSet的`replicas`方便快速的将Master副本数量调整成您喜欢的任意的数量，不过一般不建议超过7个。\n\n```bash\n(base) ➜ kubectl --kubeconfig ~/.kube/config.txt -n kube-system edit masterset\n# set replicas to 5\n```\n然后通过上面的命令观测Master节点数量的变化，同样需要等待大约2分钟时间。\n\n\n### 缩减集群规模\n一切都是面向终态的，当您的集群规模降低后，不在需要这么多的Master后，同样可以通过调整MasterSet的`replicas`来调整Master的数量。可以调整到3个副本，也可以调整到1个Master副本。Master缩减的过程中需要保持多数quorum，因此缩减是逐步发生的。\n\n```bash\n(base) ➜ kubectl --kubeconfig ~/.kube/config.txt -n kube-system edit masterset\n# set replicas to 3 or 1\n```\n然后通过上面的命令观测Master节点数量的变化。\n\n## 集群灾难恢复\n### 备份机制\nwdrip定期备份您的集群的k8s的etcd数据，用于发生集群级别故障的时候快速恢复。默认每10分钟备份一次，保留最近4个备份副本。\n您可用通过`wdrip get -r backup`来查看当前的备份信息。\n```bash\n(base) ➜ wdrip get -r backup -n kubernetes-id-001\nI1002 18:28:13.072037   98760 provider.go:268] use HOME dir: [/Users/aoxn]\nI1002 18:28:13.072184   98760 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config\nI1002 18:28:13.072557   98760 oss.go:32] oss get object from [oss://wdrip-index/wdrip/backup/kubernetes-id-001/index.json]\nI1002 18:28:13.140780   98760 iaas.go:243]\nNAME                PREFIX              DATE                PATH\nkubernetes-id-001   wdrip/backup          20211002-1024       wdrip/backup/kubernetes-id-001/20211002-1024/snapshot.db\nkubernetes-id-001   wdrip/backup          20211002-1014       wdrip/backup/kubernetes-id-001/20211002-1014/snapshot.db\nkubernetes-id-001   wdrip/backup          20211002-1004       wdrip/backup/kubernetes-id-001/20211002-1004/snapshot.db\nkubernetes-id-001   wdrip/backup          20211002-0954       wdrip/backup/kubernetes-id-001/20211002-0954/snapshot.db\n```\n\n### 恢复场景一：在原基础设施上恢复\n如果k8s因未知因素管控完全故障，wdrip没有能够自行恢复，那么您可以手动触发命令执行恢复。仅需要一行命令即可\n```bash\n# -n 指定恢复的目标集群（本体）。\n(base) ➜ wdrip recover -n kubernetes-wdrip-121\n\n```\n等待大概2到3分钟后，通过kubectl get no 查看节点恢复情况。\n\n### 恢复场景二：恢复到新的基础设施上\n\n有些情况下，集群所对应的基础设施被损坏，人为的或者灾难性的，以至于无法在原有基础设施上恢复，那么我们可以通过新建一个具有相同规格的基础设施的集群，然后在本基础设施上恢复出原有集群的备份意识。\n\n```bash\n# 创建具有相同规格的新的集群kubernetes-wdrip-121\n(base) ➜ wdrip create --config config.yaml\n\n# 将备份的旧的kubernetes-wdrip-120集群配置（意识）恢复到新建的集群121上。\n# -n 指定恢复的目标集群（本体）， -f 指定恢复的配置来源于哪个集群（意识）。\n(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120\n\n```\n\n## 节点修复机制\n节点是运行负载的工具而已，对于失效的节点，替换是成本最小的方案，替换之前我们会尝试重启来恢复。\n\n## 自定义集群参数能力\n规划中（节点、集群）\n\n## 运维简化机制\n节点故障后快速恢复，定位问题很难。 在wdrip这里，你可以直接删掉故障节点，让系统直接拉起一个新的节点副本即可。\n\n","slug":"管理集群","published":1,"updated":"2022-06-08T12:05:49.764Z","_id":"cl45h8mrv0000dorh17opbsb7","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><p><strong>安装wdrip</strong><br>下载最新版本wdrip.当前版本0.1.1</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ curl -sSL --retry 3 https://host-wdrip-cn-hangzhou.oss-cn-hangzhou.aliyuncs.com/wdrip/install.sh |bash</span><br><span class=\"line\">(base) ➜ <span class=\"built_in\">ls</span> -lht /usr/local/bin/wdrip</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># use wdrip -h to see wdrip help command</span></span><br><span class=\"line\">(base) ➜ wdrip -h</span><br><span class=\"line\"></span><br><span class=\"line\">wdrip creates and manages infrastructure agnostic Kubernetes clusters</span><br><span class=\"line\">            _         _</span><br><span class=\"line\">           | |       (_)</span><br><span class=\"line\"> _ _ _   __| |  ____  _  ____</span><br><span class=\"line\">| | | | / _  | / ___)| ||  _ \\</span><br><span class=\"line\">| | | |( (_| || |    | || |_| |</span><br><span class=\"line\"> \\___/  \\____||_|    |_||  __/</span><br><span class=\"line\">                        |_|</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">wdrip creates and manages infrastructure agnostic Kubernetes clusters and empower strong auto heal ability and easy recovery</span><br><span class=\"line\"></span><br><span class=\"line\">Usage:</span><br><span class=\"line\">  wdrip [<span class=\"built_in\">command</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">Available Commands:</span><br><span class=\"line\">  bootstrap   Bootstrap a Kubernetes cluster</span><br><span class=\"line\">  build       Kubernetes cluster build package</span><br><span class=\"line\">  ......</span><br><span class=\"line\"></span><br><span class=\"line\">Use <span class=\"string\">&quot;wdrip [command] --help&quot;</span> <span class=\"keyword\">for</span> more information about a <span class=\"built_in\">command</span>.</span><br></pre></td></tr></table></figure>\n\n<span id=\"more\"></span>\n\n<p><strong>配置wdrip</strong><br>wdrip 目前仅支持阿里云上管理k8s集群，更多的CloudProvider未来会逐步加入。<br>wdrip 需要您的阿里云账号信息来帮助您管理您的云上k8s资源。将<code>replace-with-your-own-accessKeyId</code>及<code>replace-with-your-own-accessKeySecret</code>替换成您自己的主账号AK信息。<br>wdrip 会额外为您创建OSS bucket，用来备份集群，用来紧急修复。bucket名称见下面的<code>wdrip-index</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ vi ~/.wdrip/config</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: alibabacloud.com/v1</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    provider-key: alibaba.dev</span><br><span class=\"line\">  name: devEnv</span><br><span class=\"line\">current-context: devEnv</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">providers:</span><br><span class=\"line\">- name: alibaba.dev</span><br><span class=\"line\">  provider:</span><br><span class=\"line\">    name: alibaba</span><br><span class=\"line\">    value:</span><br><span class=\"line\">      accessKeyId: &#123;replace-with-your-own-accessKeyId&#125;</span><br><span class=\"line\">      accessKeySecret: &#123;replace-with-your-own-accessKeySecret&#125;</span><br><span class=\"line\">      bucketName: wdrip-index</span><br><span class=\"line\">      region: cn-hangzhou</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"创建集群\"><a href=\"#创建集群\" class=\"headerlink\" title=\"创建集群\"></a>创建集群</h2><p>wdrip遵循结构化原则，最小核心原则，模块化设计，因此具有非常高的灵活性。<br>wdrip会首先在云上初始化一个单Master节点的k8s集群（最小可用原则），这个阶段的速度最快，并且具有最小的故障面，具有最高的系统稳定性。<br>以下配置为您创建一个最小k8s集群，仅有一个master节点。预计3分钟内完成。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ <span class=\"built_in\">export</span> CLUSTER_NAME=kubernetes-id-001 \\</span><br><span class=\"line\">              REGION=cn-hangzhou ZONE_ID=cn-hangzhou-k \\</span><br><span class=\"line\">              IMAGE_ID=centos_7_9_x64_20G_alibase_20210623.vhd \\</span><br><span class=\"line\">              DISK_TYPE=cloud_essd \\</span><br><span class=\"line\">              INSTANCE_TYPE=ecs.c6.xlarge \\</span><br><span class=\"line\">              TOKEN=$(/usr/local/bin/wdrip token new)</span><br><span class=\"line\"></span><br><span class=\"line\">(base) ➜ <span class=\"built_in\">cat</span> &gt; config.yaml &lt;&lt; <span class=\"string\">EOF</span></span><br><span class=\"line\"><span class=\"string\">clusterid: &quot;$&#123;CLUSTER_NAME&#125;&quot;</span></span><br><span class=\"line\"><span class=\"string\">iaas:</span></span><br><span class=\"line\"><span class=\"string\">  workerCount: 1</span></span><br><span class=\"line\"><span class=\"string\">  image: &quot;$&#123;IMAGE_ID&#125;&quot;</span></span><br><span class=\"line\"><span class=\"string\">  disk:</span></span><br><span class=\"line\"><span class=\"string\">    size: 40G</span></span><br><span class=\"line\"><span class=\"string\">    type: &quot;$&#123;DISK_TYPE&#125;&quot;</span></span><br><span class=\"line\"><span class=\"string\">  region: &quot;$&#123;REGION&#125;&quot;</span></span><br><span class=\"line\"><span class=\"string\">  zoneid: $&#123;ZONE_ID&#125;</span></span><br><span class=\"line\"><span class=\"string\">  instance: &quot;$&#123;INSTANCE_TYPE&#125;&quot;</span></span><br><span class=\"line\"><span class=\"string\">registry: registry-vpc.$&#123;REGION&#125;.aliyuncs.com</span></span><br><span class=\"line\"><span class=\"string\">namespace: default</span></span><br><span class=\"line\"><span class=\"string\">cloudType: public</span></span><br><span class=\"line\"><span class=\"string\">kubernetes:</span></span><br><span class=\"line\"><span class=\"string\">  name: kubernetes</span></span><br><span class=\"line\"><span class=\"string\">  version: 1.20.4-aliyun.1</span></span><br><span class=\"line\"><span class=\"string\">  kubeadmToken: $&#123;TOKEN&#125;</span></span><br><span class=\"line\"><span class=\"string\">etcd:</span></span><br><span class=\"line\"><span class=\"string\">  name: etcd</span></span><br><span class=\"line\"><span class=\"string\">  version: v3.4.3</span></span><br><span class=\"line\"><span class=\"string\">runtime:</span></span><br><span class=\"line\"><span class=\"string\">  name: docker</span></span><br><span class=\"line\"><span class=\"string\">  version: 19.03.5</span></span><br><span class=\"line\"><span class=\"string\">sans:</span></span><br><span class=\"line\"><span class=\"string\">  - 192.168.0.1</span></span><br><span class=\"line\"><span class=\"string\">network:</span></span><br><span class=\"line\"><span class=\"string\">  mode: ipvs</span></span><br><span class=\"line\"><span class=\"string\">  podcidr: 172.16.0.1/16</span></span><br><span class=\"line\"><span class=\"string\">  svccidr: 172.19.0.1/20</span></span><br><span class=\"line\"><span class=\"string\">  domain: cluster.domain</span></span><br><span class=\"line\"><span class=\"string\">  netMask: 25</span></span><br><span class=\"line\"><span class=\"string\">EOF</span></span><br><span class=\"line\"></span><br><span class=\"line\">(base) ➜ wdrip create --config config.yaml</span><br><span class=\"line\"></span><br><span class=\"line\">wdrip: kubernetes cluster lifecycle management.</span><br><span class=\"line\">            _         _</span><br><span class=\"line\">           | |       (_)</span><br><span class=\"line\"> _ _ _   __| |  ____  _  ____</span><br><span class=\"line\">| | | | / _  | / ___)| ||  _ \\</span><br><span class=\"line\">| | | |( (_| || |    | || |_| |</span><br><span class=\"line\"> \\___/  \\____||_|    |_||  __/</span><br><span class=\"line\">                        |_|</span><br><span class=\"line\"></span><br><span class=\"line\">I1002 15:31:37.593094   96000 provider.go:268] use HOME <span class=\"built_in\">dir</span>: [/Users/aoxn]</span><br><span class=\"line\">I1002 15:31:37.593265   96000 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config</span><br><span class=\"line\">I1002 15:31:37.595669   96000 provider.go:52] use <span class=\"built_in\">command</span> line config as bootconfig: [config.yaml] with provider[alibaba]</span><br><span class=\"line\">I1002 15:31:37.726288   96000 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]</span><br><span class=\"line\">....</span><br><span class=\"line\"></span><br><span class=\"line\">I1002 15:31:41.989201   96000 iaas.go:96] watch cluster create progress with <span class=\"built_in\">command</span>:  [ wdrip watch --name kubernetes-id-001 ]</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"观测集群的创建过程\"><a href=\"#观测集群的创建过程\" class=\"headerlink\" title=\"观测集群的创建过程\"></a>观测集群的创建过程</h3><p>创建的集群是一个异步的过程，因此我们提供了一个watch命令，用来观测创建的进度。执行watch命令前请将terminal窗口最大化，保证最佳输出效果。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜  wdrip watch --name kubernetes-id-001</span><br><span class=\"line\">I1002 15:42:05.600911   96142 provider.go:268] use HOME <span class=\"built_in\">dir</span>: [/Users/aoxn]</span><br><span class=\"line\">I1002 15:42:05.601063   96142 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config</span><br><span class=\"line\">✓ 【ALIYUN::ROS::Stack                  】(kubernetes-id-001         ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:42 2021-10-02T15:35:52</span><br><span class=\"line\">✓ 【ALIYUN::ROS::WaitCondition          】(k8s_master_waiter         ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:52 2021-10-02T15:35:51</span><br><span class=\"line\"></span><br><span class=\"line\">.....</span><br><span class=\"line\"></span><br><span class=\"line\">✓ 【ALIYUN::RAM::Role                   】(KubernetesWorkerRole      ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:42 2021-10-02T15:31:52</span><br><span class=\"line\">✓ 【WDRIP::MESSAGE::OUTPUT              】(extra_mesage_id           ) [CREATE_COMPLETE,23, 23]  TimeElapse: 251s</span><br><span class=\"line\">I1002 15:42:14.446231   96142 ros.go:477] ===========================================================</span><br><span class=\"line\">I1002 15:42:14.446254   96142 ros.go:478] StackName: kubernetes-id-001</span><br><span class=\"line\">I1002 15:42:14.446259   96142 ros.go:479]   StackId: 2d302c6c-24b3-4535-8875-8c7dd9a48bd7</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"查看集群列表\"><a href=\"#查看集群列表\" class=\"headerlink\" title=\"查看集群列表\"></a>查看集群列表</h3><p>wdrip提供了命令用来查看本账号的provider所创建的集群列表</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ wdrip get</span><br><span class=\"line\">I1002 16:16:59.615092   97592 provider.go:268] use HOME <span class=\"built_in\">dir</span>: [/Users/aoxn]</span><br><span class=\"line\">I1002 16:16:59.615225   97592 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config</span><br><span class=\"line\">I1002 16:16:59.670775   97592 index.go:76] get cluster: [wdrip/clusters/kubernetes-id-001.json]</span><br><span class=\"line\">I1002 16:16:59.670802   97592 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]</span><br><span class=\"line\">I1002 16:16:59.735396   97592 index.go:76] get cluster: [wdrip/clusters/kubernetes-wdrip-77.json]</span><br><span class=\"line\">I1002 16:16:59.735420   97592 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-wdrip-77.json]</span><br><span class=\"line\">I1002 16:16:59.807551   97592 iaas.go:190]</span><br><span class=\"line\">NAME                  ENDPOINT</span><br><span class=\"line\">kubernetes-id-001     47.96.27.46/192.168.0.75</span><br><span class=\"line\">kubernetes-wdrip-77   116.62.24.127/192.168.0.53</span><br></pre></td></tr></table></figure>\n<p>通过<code>wdrip get -n kubernetes-id-001 -o yaml</code> 可以查看该集群的详细信息</p>\n<h3 id=\"连接集群\"><a href=\"#连接集群\" class=\"headerlink\" title=\"连接集群\"></a>连接集群</h3><p>当集群创建完成后,可以通过wdrip get命令下载kubeconfig文件来访问我们的集群。 当前wdrip创建的集群通过EIP在公网暴露了apiserver，因此可以通过公网本地访问。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ wdrip get -r kubeconfig -n kubernetes-id-001 -w ~/.kube/config.txt</span><br><span class=\"line\"></span><br><span class=\"line\">I1002 16:14:38.136200   97541 provider.go:268] use HOME <span class=\"built_in\">dir</span>: [/Users/aoxn]</span><br><span class=\"line\">I1002 16:14:38.136349   97541 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config</span><br><span class=\"line\">I1002 16:14:38.136730   97541 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]</span><br><span class=\"line\">I1002 16:14:40.241796   97541 sign.go:223] sign kubernetes: []</span><br><span class=\"line\">I1002 16:14:40.612558   97541 iaas.go:301] write kubeconfig to file [/Users/aoxn/.kube/config.txt]</span><br><span class=\"line\"></span><br><span class=\"line\">(base) ➜  kubectl --kubeconfig ~/.kube/config.txt get no</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                  STATUS   ROLES                  AGE    VERSION</span><br><span class=\"line\">192.168.0.77.i-bp12a2wcmbrd4383cai3   Ready    control-plane,master   2m2s   v1.20.4-aliyun.1</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"添加工作节点\"><a href=\"#添加工作节点\" class=\"headerlink\" title=\"添加工作节点\"></a>添加工作节点</h2><h3 id=\"创建节点池\"><a href=\"#创建节点池\" class=\"headerlink\" title=\"创建节点池\"></a>创建节点池</h3><p>wdrip 提供了NodePool的概念，将一组具有相同的配置的节点组作为一个节点池统一管理。通过以下yaml可以创建具有N个节点的节点池。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ kubectl --kubeconfig ~/.kube/config.txt apply -f - &lt;&lt;<span class=\"string\">EOF</span></span><br><span class=\"line\"><span class=\"string\">apiVersion: alibabacloud.com/v1</span></span><br><span class=\"line\"><span class=\"string\">kind: NodePool</span></span><br><span class=\"line\"><span class=\"string\">metadata:</span></span><br><span class=\"line\"><span class=\"string\">  name: nodepool-01</span></span><br><span class=\"line\"><span class=\"string\">  namespace: kube-system</span></span><br><span class=\"line\"><span class=\"string\">spec:</span></span><br><span class=\"line\"><span class=\"string\">  id: &quot;very-long-id-xxxxx&quot;</span></span><br><span class=\"line\"><span class=\"string\">  infra:</span></span><br><span class=\"line\"><span class=\"string\">    desiredCapacity: 1</span></span><br><span class=\"line\"><span class=\"string\">    cpu: 4</span></span><br><span class=\"line\"><span class=\"string\">    memory: 8</span></span><br><span class=\"line\"><span class=\"string\">    imageId: centos_7_9_x64_20G_alibase_20210623.vhd</span></span><br><span class=\"line\"><span class=\"string\">EOF</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># output</span></span><br><span class=\"line\">nodepool.alibabacloud.com/nodepool-01 created</span><br></pre></td></tr></table></figure>\n\n<p>通过<code>kubectl --kubeconfig ~/.kube/config.txt get no -w</code> 观测节点的创建。 节点的创建及加入过程大约需要等待90s。请等待</p>\n<p>同样的方式可以创建多个具有不同配置的节点池。</p>\n<h3 id=\"扩容节点池\"><a href=\"#扩容节点池\" class=\"headerlink\" title=\"扩容节点池\"></a>扩容节点池</h3><p>节点池的管理规划是完全面向终态的，但目前的实现还在早期阶段，您可以通过edit对应的NodePool的CRD的desiredCapacity的值来调整节点池的节点数量。<br>观测节点池的加入的过程参考上面方法。可以调大，也可以调小节点池的大小。<br>自动扩容节点池的功能Coming Soon。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ kubectl --kubeconfig ~/.kube/config.txt edit nodepool default-nodepool</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"集群高可用\"><a href=\"#集群高可用\" class=\"headerlink\" title=\"集群高可用\"></a>集群高可用</h2><p>一切都是面向终态的</p>\n<h3 id=\"构建高可用的k8s集群\"><a href=\"#构建高可用的k8s集群\" class=\"headerlink\" title=\"构建高可用的k8s集群\"></a>构建高可用的k8s集群</h3><p>单个Master的k8s集群不具备高可用，但已足够用作测试集群，经济适用。wdrip同样提供了灵活简单的方式将已有集群扩展成高可用集群。wdrip适用<code>MasterSet</code>的CRD资源代表Master节点组。<br>以下命令扩展当前集群的Master副本数量到3个。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ kubectl --kubeconfig ~/.kube/config.txt edit masterset</span><br><span class=\"line\">apiVersion: alibabacloud.com/v1</span><br><span class=\"line\">kind: MasterSet</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  <span class=\"comment\"># 请不要改masterset这个名称，没做支持，也没意义。</span></span><br><span class=\"line\">  name: masterset</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 3</span><br></pre></td></tr></table></figure>\n<p>wdrip需要2分钟左右的时间来初始化额外的2个Master节点，请等待。可以通过<code>kubectl --kubeconfig ~/.kube/config.txt get no -w</code> 观测Master节点的加入过程。</p>\n<h3 id=\"大规模集群场景\"><a href=\"#大规模集群场景\" class=\"headerlink\" title=\"大规模集群场景\"></a>大规模集群场景</h3><p>当你的集群规模进一步扩大后，3个Master已经不能满足你的需求了，那么你也可以通过MasterSet的<code>replicas</code>方便快速的将Master副本数量调整成您喜欢的任意的数量，不过一般不建议超过7个。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ kubectl --kubeconfig ~/.kube/config.txt -n kube-system edit masterset</span><br><span class=\"line\"><span class=\"comment\"># set replicas to 5</span></span><br></pre></td></tr></table></figure>\n<p>然后通过上面的命令观测Master节点数量的变化，同样需要等待大约2分钟时间。</p>\n<h3 id=\"缩减集群规模\"><a href=\"#缩减集群规模\" class=\"headerlink\" title=\"缩减集群规模\"></a>缩减集群规模</h3><p>一切都是面向终态的，当您的集群规模降低后，不在需要这么多的Master后，同样可以通过调整MasterSet的<code>replicas</code>来调整Master的数量。可以调整到3个副本，也可以调整到1个Master副本。Master缩减的过程中需要保持多数quorum，因此缩减是逐步发生的。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ kubectl --kubeconfig ~/.kube/config.txt -n kube-system edit masterset</span><br><span class=\"line\"><span class=\"comment\"># set replicas to 3 or 1</span></span><br></pre></td></tr></table></figure>\n<p>然后通过上面的命令观测Master节点数量的变化。</p>\n<h2 id=\"集群灾难恢复\"><a href=\"#集群灾难恢复\" class=\"headerlink\" title=\"集群灾难恢复\"></a>集群灾难恢复</h2><h3 id=\"备份机制\"><a href=\"#备份机制\" class=\"headerlink\" title=\"备份机制\"></a>备份机制</h3><p>wdrip定期备份您的集群的k8s的etcd数据，用于发生集群级别故障的时候快速恢复。默认每10分钟备份一次，保留最近4个备份副本。<br>您可用通过<code>wdrip get -r backup</code>来查看当前的备份信息。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ wdrip get -r backup -n kubernetes-id-001</span><br><span class=\"line\">I1002 18:28:13.072037   98760 provider.go:268] use HOME <span class=\"built_in\">dir</span>: [/Users/aoxn]</span><br><span class=\"line\">I1002 18:28:13.072184   98760 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config</span><br><span class=\"line\">I1002 18:28:13.072557   98760 oss.go:32] oss get object from [oss://wdrip-index/wdrip/backup/kubernetes-id-001/index.json]</span><br><span class=\"line\">I1002 18:28:13.140780   98760 iaas.go:243]</span><br><span class=\"line\">NAME                PREFIX              DATE                PATH</span><br><span class=\"line\">kubernetes-id-001   wdrip/backup          20211002-1024       wdrip/backup/kubernetes-id-001/20211002-1024/snapshot.db</span><br><span class=\"line\">kubernetes-id-001   wdrip/backup          20211002-1014       wdrip/backup/kubernetes-id-001/20211002-1014/snapshot.db</span><br><span class=\"line\">kubernetes-id-001   wdrip/backup          20211002-1004       wdrip/backup/kubernetes-id-001/20211002-1004/snapshot.db</span><br><span class=\"line\">kubernetes-id-001   wdrip/backup          20211002-0954       wdrip/backup/kubernetes-id-001/20211002-0954/snapshot.db</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"恢复场景一：在原基础设施上恢复\"><a href=\"#恢复场景一：在原基础设施上恢复\" class=\"headerlink\" title=\"恢复场景一：在原基础设施上恢复\"></a>恢复场景一：在原基础设施上恢复</h3><p>如果k8s因未知因素管控完全故障，wdrip没有能够自行恢复，那么您可以手动触发命令执行恢复。仅需要一行命令即可</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -n 指定恢复的目标集群（本体）。</span></span><br><span class=\"line\">(base) ➜ wdrip recover -n kubernetes-wdrip-121</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>等待大概2到3分钟后，通过kubectl get no 查看节点恢复情况。</p>\n<h3 id=\"恢复场景二：恢复到新的基础设施上\"><a href=\"#恢复场景二：恢复到新的基础设施上\" class=\"headerlink\" title=\"恢复场景二：恢复到新的基础设施上\"></a>恢复场景二：恢复到新的基础设施上</h3><p>有些情况下，集群所对应的基础设施被损坏，人为的或者灾难性的，以至于无法在原有基础设施上恢复，那么我们可以通过新建一个具有相同规格的基础设施的集群，然后在本基础设施上恢复出原有集群的备份意识。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 创建具有相同规格的新的集群kubernetes-wdrip-121</span></span><br><span class=\"line\">(base) ➜ wdrip create --config config.yaml</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将备份的旧的kubernetes-wdrip-120集群配置（意识）恢复到新建的集群121上。</span></span><br><span class=\"line\"><span class=\"comment\"># -n 指定恢复的目标集群（本体）， -f 指定恢复的配置来源于哪个集群（意识）。</span></span><br><span class=\"line\">(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"节点修复机制\"><a href=\"#节点修复机制\" class=\"headerlink\" title=\"节点修复机制\"></a>节点修复机制</h2><p>节点是运行负载的工具而已，对于失效的节点，替换是成本最小的方案，替换之前我们会尝试重启来恢复。</p>\n<h2 id=\"自定义集群参数能力\"><a href=\"#自定义集群参数能力\" class=\"headerlink\" title=\"自定义集群参数能力\"></a>自定义集群参数能力</h2><p>规划中（节点、集群）</p>\n<h2 id=\"运维简化机制\"><a href=\"#运维简化机制\" class=\"headerlink\" title=\"运维简化机制\"></a>运维简化机制</h2><p>节点故障后快速恢复，定位问题很难。 在wdrip这里，你可以直接删掉故障节点，让系统直接拉起一个新的节点副本即可。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><p><strong>安装wdrip</strong><br>下载最新版本wdrip.当前版本0.1.1</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ curl -sSL --retry 3 https://host-wdrip-cn-hangzhou.oss-cn-hangzhou.aliyuncs.com/wdrip/install.sh |bash</span><br><span class=\"line\">(base) ➜ <span class=\"built_in\">ls</span> -lht /usr/local/bin/wdrip</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># use wdrip -h to see wdrip help command</span></span><br><span class=\"line\">(base) ➜ wdrip -h</span><br><span class=\"line\"></span><br><span class=\"line\">wdrip creates and manages infrastructure agnostic Kubernetes clusters</span><br><span class=\"line\">            _         _</span><br><span class=\"line\">           | |       (_)</span><br><span class=\"line\"> _ _ _   __| |  ____  _  ____</span><br><span class=\"line\">| | | | / _  | / ___)| ||  _ \\</span><br><span class=\"line\">| | | |( (_| || |    | || |_| |</span><br><span class=\"line\"> \\___/  \\____||_|    |_||  __/</span><br><span class=\"line\">                        |_|</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">wdrip creates and manages infrastructure agnostic Kubernetes clusters and empower strong auto heal ability and easy recovery</span><br><span class=\"line\"></span><br><span class=\"line\">Usage:</span><br><span class=\"line\">  wdrip [<span class=\"built_in\">command</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">Available Commands:</span><br><span class=\"line\">  bootstrap   Bootstrap a Kubernetes cluster</span><br><span class=\"line\">  build       Kubernetes cluster build package</span><br><span class=\"line\">  ......</span><br><span class=\"line\"></span><br><span class=\"line\">Use <span class=\"string\">&quot;wdrip [command] --help&quot;</span> <span class=\"keyword\">for</span> more information about a <span class=\"built_in\">command</span>.</span><br></pre></td></tr></table></figure>","more":"<p><strong>配置wdrip</strong><br>wdrip 目前仅支持阿里云上管理k8s集群，更多的CloudProvider未来会逐步加入。<br>wdrip 需要您的阿里云账号信息来帮助您管理您的云上k8s资源。将<code>replace-with-your-own-accessKeyId</code>及<code>replace-with-your-own-accessKeySecret</code>替换成您自己的主账号AK信息。<br>wdrip 会额外为您创建OSS bucket，用来备份集群，用来紧急修复。bucket名称见下面的<code>wdrip-index</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ vi ~/.wdrip/config</span><br><span class=\"line\"></span><br><span class=\"line\">apiVersion: alibabacloud.com/v1</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    provider-key: alibaba.dev</span><br><span class=\"line\">  name: devEnv</span><br><span class=\"line\">current-context: devEnv</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">providers:</span><br><span class=\"line\">- name: alibaba.dev</span><br><span class=\"line\">  provider:</span><br><span class=\"line\">    name: alibaba</span><br><span class=\"line\">    value:</span><br><span class=\"line\">      accessKeyId: &#123;replace-with-your-own-accessKeyId&#125;</span><br><span class=\"line\">      accessKeySecret: &#123;replace-with-your-own-accessKeySecret&#125;</span><br><span class=\"line\">      bucketName: wdrip-index</span><br><span class=\"line\">      region: cn-hangzhou</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"创建集群\"><a href=\"#创建集群\" class=\"headerlink\" title=\"创建集群\"></a>创建集群</h2><p>wdrip遵循结构化原则，最小核心原则，模块化设计，因此具有非常高的灵活性。<br>wdrip会首先在云上初始化一个单Master节点的k8s集群（最小可用原则），这个阶段的速度最快，并且具有最小的故障面，具有最高的系统稳定性。<br>以下配置为您创建一个最小k8s集群，仅有一个master节点。预计3分钟内完成。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ <span class=\"built_in\">export</span> CLUSTER_NAME=kubernetes-id-001 \\</span><br><span class=\"line\">              REGION=cn-hangzhou ZONE_ID=cn-hangzhou-k \\</span><br><span class=\"line\">              IMAGE_ID=centos_7_9_x64_20G_alibase_20210623.vhd \\</span><br><span class=\"line\">              DISK_TYPE=cloud_essd \\</span><br><span class=\"line\">              INSTANCE_TYPE=ecs.c6.xlarge \\</span><br><span class=\"line\">              TOKEN=$(/usr/local/bin/wdrip token new)</span><br><span class=\"line\"></span><br><span class=\"line\">(base) ➜ <span class=\"built_in\">cat</span> &gt; config.yaml &lt;&lt; <span class=\"string\">EOF</span></span><br><span class=\"line\"><span class=\"string\">clusterid: &quot;$&#123;CLUSTER_NAME&#125;&quot;</span></span><br><span class=\"line\"><span class=\"string\">iaas:</span></span><br><span class=\"line\"><span class=\"string\">  workerCount: 1</span></span><br><span class=\"line\"><span class=\"string\">  image: &quot;$&#123;IMAGE_ID&#125;&quot;</span></span><br><span class=\"line\"><span class=\"string\">  disk:</span></span><br><span class=\"line\"><span class=\"string\">    size: 40G</span></span><br><span class=\"line\"><span class=\"string\">    type: &quot;$&#123;DISK_TYPE&#125;&quot;</span></span><br><span class=\"line\"><span class=\"string\">  region: &quot;$&#123;REGION&#125;&quot;</span></span><br><span class=\"line\"><span class=\"string\">  zoneid: $&#123;ZONE_ID&#125;</span></span><br><span class=\"line\"><span class=\"string\">  instance: &quot;$&#123;INSTANCE_TYPE&#125;&quot;</span></span><br><span class=\"line\"><span class=\"string\">registry: registry-vpc.$&#123;REGION&#125;.aliyuncs.com</span></span><br><span class=\"line\"><span class=\"string\">namespace: default</span></span><br><span class=\"line\"><span class=\"string\">cloudType: public</span></span><br><span class=\"line\"><span class=\"string\">kubernetes:</span></span><br><span class=\"line\"><span class=\"string\">  name: kubernetes</span></span><br><span class=\"line\"><span class=\"string\">  version: 1.20.4-aliyun.1</span></span><br><span class=\"line\"><span class=\"string\">  kubeadmToken: $&#123;TOKEN&#125;</span></span><br><span class=\"line\"><span class=\"string\">etcd:</span></span><br><span class=\"line\"><span class=\"string\">  name: etcd</span></span><br><span class=\"line\"><span class=\"string\">  version: v3.4.3</span></span><br><span class=\"line\"><span class=\"string\">runtime:</span></span><br><span class=\"line\"><span class=\"string\">  name: docker</span></span><br><span class=\"line\"><span class=\"string\">  version: 19.03.5</span></span><br><span class=\"line\"><span class=\"string\">sans:</span></span><br><span class=\"line\"><span class=\"string\">  - 192.168.0.1</span></span><br><span class=\"line\"><span class=\"string\">network:</span></span><br><span class=\"line\"><span class=\"string\">  mode: ipvs</span></span><br><span class=\"line\"><span class=\"string\">  podcidr: 172.16.0.1/16</span></span><br><span class=\"line\"><span class=\"string\">  svccidr: 172.19.0.1/20</span></span><br><span class=\"line\"><span class=\"string\">  domain: cluster.domain</span></span><br><span class=\"line\"><span class=\"string\">  netMask: 25</span></span><br><span class=\"line\"><span class=\"string\">EOF</span></span><br><span class=\"line\"></span><br><span class=\"line\">(base) ➜ wdrip create --config config.yaml</span><br><span class=\"line\"></span><br><span class=\"line\">wdrip: kubernetes cluster lifecycle management.</span><br><span class=\"line\">            _         _</span><br><span class=\"line\">           | |       (_)</span><br><span class=\"line\"> _ _ _   __| |  ____  _  ____</span><br><span class=\"line\">| | | | / _  | / ___)| ||  _ \\</span><br><span class=\"line\">| | | |( (_| || |    | || |_| |</span><br><span class=\"line\"> \\___/  \\____||_|    |_||  __/</span><br><span class=\"line\">                        |_|</span><br><span class=\"line\"></span><br><span class=\"line\">I1002 15:31:37.593094   96000 provider.go:268] use HOME <span class=\"built_in\">dir</span>: [/Users/aoxn]</span><br><span class=\"line\">I1002 15:31:37.593265   96000 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config</span><br><span class=\"line\">I1002 15:31:37.595669   96000 provider.go:52] use <span class=\"built_in\">command</span> line config as bootconfig: [config.yaml] with provider[alibaba]</span><br><span class=\"line\">I1002 15:31:37.726288   96000 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]</span><br><span class=\"line\">....</span><br><span class=\"line\"></span><br><span class=\"line\">I1002 15:31:41.989201   96000 iaas.go:96] watch cluster create progress with <span class=\"built_in\">command</span>:  [ wdrip watch --name kubernetes-id-001 ]</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"观测集群的创建过程\"><a href=\"#观测集群的创建过程\" class=\"headerlink\" title=\"观测集群的创建过程\"></a>观测集群的创建过程</h3><p>创建的集群是一个异步的过程，因此我们提供了一个watch命令，用来观测创建的进度。执行watch命令前请将terminal窗口最大化，保证最佳输出效果。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜  wdrip watch --name kubernetes-id-001</span><br><span class=\"line\">I1002 15:42:05.600911   96142 provider.go:268] use HOME <span class=\"built_in\">dir</span>: [/Users/aoxn]</span><br><span class=\"line\">I1002 15:42:05.601063   96142 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config</span><br><span class=\"line\">✓ 【ALIYUN::ROS::Stack                  】(kubernetes-id-001         ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:42 2021-10-02T15:35:52</span><br><span class=\"line\">✓ 【ALIYUN::ROS::WaitCondition          】(k8s_master_waiter         ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:52 2021-10-02T15:35:51</span><br><span class=\"line\"></span><br><span class=\"line\">.....</span><br><span class=\"line\"></span><br><span class=\"line\">✓ 【ALIYUN::RAM::Role                   】(KubernetesWorkerRole      ) [CREATE_COMPLETE,23, 23] 2021-10-02T15:31:42 2021-10-02T15:31:52</span><br><span class=\"line\">✓ 【WDRIP::MESSAGE::OUTPUT              】(extra_mesage_id           ) [CREATE_COMPLETE,23, 23]  TimeElapse: 251s</span><br><span class=\"line\">I1002 15:42:14.446231   96142 ros.go:477] ===========================================================</span><br><span class=\"line\">I1002 15:42:14.446254   96142 ros.go:478] StackName: kubernetes-id-001</span><br><span class=\"line\">I1002 15:42:14.446259   96142 ros.go:479]   StackId: 2d302c6c-24b3-4535-8875-8c7dd9a48bd7</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"查看集群列表\"><a href=\"#查看集群列表\" class=\"headerlink\" title=\"查看集群列表\"></a>查看集群列表</h3><p>wdrip提供了命令用来查看本账号的provider所创建的集群列表</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ wdrip get</span><br><span class=\"line\">I1002 16:16:59.615092   97592 provider.go:268] use HOME <span class=\"built_in\">dir</span>: [/Users/aoxn]</span><br><span class=\"line\">I1002 16:16:59.615225   97592 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config</span><br><span class=\"line\">I1002 16:16:59.670775   97592 index.go:76] get cluster: [wdrip/clusters/kubernetes-id-001.json]</span><br><span class=\"line\">I1002 16:16:59.670802   97592 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]</span><br><span class=\"line\">I1002 16:16:59.735396   97592 index.go:76] get cluster: [wdrip/clusters/kubernetes-wdrip-77.json]</span><br><span class=\"line\">I1002 16:16:59.735420   97592 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-wdrip-77.json]</span><br><span class=\"line\">I1002 16:16:59.807551   97592 iaas.go:190]</span><br><span class=\"line\">NAME                  ENDPOINT</span><br><span class=\"line\">kubernetes-id-001     47.96.27.46/192.168.0.75</span><br><span class=\"line\">kubernetes-wdrip-77   116.62.24.127/192.168.0.53</span><br></pre></td></tr></table></figure>\n<p>通过<code>wdrip get -n kubernetes-id-001 -o yaml</code> 可以查看该集群的详细信息</p>\n<h3 id=\"连接集群\"><a href=\"#连接集群\" class=\"headerlink\" title=\"连接集群\"></a>连接集群</h3><p>当集群创建完成后,可以通过wdrip get命令下载kubeconfig文件来访问我们的集群。 当前wdrip创建的集群通过EIP在公网暴露了apiserver，因此可以通过公网本地访问。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ wdrip get -r kubeconfig -n kubernetes-id-001 -w ~/.kube/config.txt</span><br><span class=\"line\"></span><br><span class=\"line\">I1002 16:14:38.136200   97541 provider.go:268] use HOME <span class=\"built_in\">dir</span>: [/Users/aoxn]</span><br><span class=\"line\">I1002 16:14:38.136349   97541 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config</span><br><span class=\"line\">I1002 16:14:38.136730   97541 oss.go:32] oss get object from [oss://wdrip-index/wdrip/clusters/kubernetes-id-001.json]</span><br><span class=\"line\">I1002 16:14:40.241796   97541 sign.go:223] sign kubernetes: []</span><br><span class=\"line\">I1002 16:14:40.612558   97541 iaas.go:301] write kubeconfig to file [/Users/aoxn/.kube/config.txt]</span><br><span class=\"line\"></span><br><span class=\"line\">(base) ➜  kubectl --kubeconfig ~/.kube/config.txt get no</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                  STATUS   ROLES                  AGE    VERSION</span><br><span class=\"line\">192.168.0.77.i-bp12a2wcmbrd4383cai3   Ready    control-plane,master   2m2s   v1.20.4-aliyun.1</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"添加工作节点\"><a href=\"#添加工作节点\" class=\"headerlink\" title=\"添加工作节点\"></a>添加工作节点</h2><h3 id=\"创建节点池\"><a href=\"#创建节点池\" class=\"headerlink\" title=\"创建节点池\"></a>创建节点池</h3><p>wdrip 提供了NodePool的概念，将一组具有相同的配置的节点组作为一个节点池统一管理。通过以下yaml可以创建具有N个节点的节点池。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ kubectl --kubeconfig ~/.kube/config.txt apply -f - &lt;&lt;<span class=\"string\">EOF</span></span><br><span class=\"line\"><span class=\"string\">apiVersion: alibabacloud.com/v1</span></span><br><span class=\"line\"><span class=\"string\">kind: NodePool</span></span><br><span class=\"line\"><span class=\"string\">metadata:</span></span><br><span class=\"line\"><span class=\"string\">  name: nodepool-01</span></span><br><span class=\"line\"><span class=\"string\">  namespace: kube-system</span></span><br><span class=\"line\"><span class=\"string\">spec:</span></span><br><span class=\"line\"><span class=\"string\">  id: &quot;very-long-id-xxxxx&quot;</span></span><br><span class=\"line\"><span class=\"string\">  infra:</span></span><br><span class=\"line\"><span class=\"string\">    desiredCapacity: 1</span></span><br><span class=\"line\"><span class=\"string\">    cpu: 4</span></span><br><span class=\"line\"><span class=\"string\">    memory: 8</span></span><br><span class=\"line\"><span class=\"string\">    imageId: centos_7_9_x64_20G_alibase_20210623.vhd</span></span><br><span class=\"line\"><span class=\"string\">EOF</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># output</span></span><br><span class=\"line\">nodepool.alibabacloud.com/nodepool-01 created</span><br></pre></td></tr></table></figure>\n\n<p>通过<code>kubectl --kubeconfig ~/.kube/config.txt get no -w</code> 观测节点的创建。 节点的创建及加入过程大约需要等待90s。请等待</p>\n<p>同样的方式可以创建多个具有不同配置的节点池。</p>\n<h3 id=\"扩容节点池\"><a href=\"#扩容节点池\" class=\"headerlink\" title=\"扩容节点池\"></a>扩容节点池</h3><p>节点池的管理规划是完全面向终态的，但目前的实现还在早期阶段，您可以通过edit对应的NodePool的CRD的desiredCapacity的值来调整节点池的节点数量。<br>观测节点池的加入的过程参考上面方法。可以调大，也可以调小节点池的大小。<br>自动扩容节点池的功能Coming Soon。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ kubectl --kubeconfig ~/.kube/config.txt edit nodepool default-nodepool</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"集群高可用\"><a href=\"#集群高可用\" class=\"headerlink\" title=\"集群高可用\"></a>集群高可用</h2><p>一切都是面向终态的</p>\n<h3 id=\"构建高可用的k8s集群\"><a href=\"#构建高可用的k8s集群\" class=\"headerlink\" title=\"构建高可用的k8s集群\"></a>构建高可用的k8s集群</h3><p>单个Master的k8s集群不具备高可用，但已足够用作测试集群，经济适用。wdrip同样提供了灵活简单的方式将已有集群扩展成高可用集群。wdrip适用<code>MasterSet</code>的CRD资源代表Master节点组。<br>以下命令扩展当前集群的Master副本数量到3个。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ kubectl --kubeconfig ~/.kube/config.txt edit masterset</span><br><span class=\"line\">apiVersion: alibabacloud.com/v1</span><br><span class=\"line\">kind: MasterSet</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  <span class=\"comment\"># 请不要改masterset这个名称，没做支持，也没意义。</span></span><br><span class=\"line\">  name: masterset</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 3</span><br></pre></td></tr></table></figure>\n<p>wdrip需要2分钟左右的时间来初始化额外的2个Master节点，请等待。可以通过<code>kubectl --kubeconfig ~/.kube/config.txt get no -w</code> 观测Master节点的加入过程。</p>\n<h3 id=\"大规模集群场景\"><a href=\"#大规模集群场景\" class=\"headerlink\" title=\"大规模集群场景\"></a>大规模集群场景</h3><p>当你的集群规模进一步扩大后，3个Master已经不能满足你的需求了，那么你也可以通过MasterSet的<code>replicas</code>方便快速的将Master副本数量调整成您喜欢的任意的数量，不过一般不建议超过7个。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ kubectl --kubeconfig ~/.kube/config.txt -n kube-system edit masterset</span><br><span class=\"line\"><span class=\"comment\"># set replicas to 5</span></span><br></pre></td></tr></table></figure>\n<p>然后通过上面的命令观测Master节点数量的变化，同样需要等待大约2分钟时间。</p>\n<h3 id=\"缩减集群规模\"><a href=\"#缩减集群规模\" class=\"headerlink\" title=\"缩减集群规模\"></a>缩减集群规模</h3><p>一切都是面向终态的，当您的集群规模降低后，不在需要这么多的Master后，同样可以通过调整MasterSet的<code>replicas</code>来调整Master的数量。可以调整到3个副本，也可以调整到1个Master副本。Master缩减的过程中需要保持多数quorum，因此缩减是逐步发生的。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ kubectl --kubeconfig ~/.kube/config.txt -n kube-system edit masterset</span><br><span class=\"line\"><span class=\"comment\"># set replicas to 3 or 1</span></span><br></pre></td></tr></table></figure>\n<p>然后通过上面的命令观测Master节点数量的变化。</p>\n<h2 id=\"集群灾难恢复\"><a href=\"#集群灾难恢复\" class=\"headerlink\" title=\"集群灾难恢复\"></a>集群灾难恢复</h2><h3 id=\"备份机制\"><a href=\"#备份机制\" class=\"headerlink\" title=\"备份机制\"></a>备份机制</h3><p>wdrip定期备份您的集群的k8s的etcd数据，用于发生集群级别故障的时候快速恢复。默认每10分钟备份一次，保留最近4个备份副本。<br>您可用通过<code>wdrip get -r backup</code>来查看当前的备份信息。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ wdrip get -r backup -n kubernetes-id-001</span><br><span class=\"line\">I1002 18:28:13.072037   98760 provider.go:268] use HOME <span class=\"built_in\">dir</span>: [/Users/aoxn]</span><br><span class=\"line\">I1002 18:28:13.072184   98760 provider.go:283] trying to load context config from: /Users/aoxn/.wdrip/config</span><br><span class=\"line\">I1002 18:28:13.072557   98760 oss.go:32] oss get object from [oss://wdrip-index/wdrip/backup/kubernetes-id-001/index.json]</span><br><span class=\"line\">I1002 18:28:13.140780   98760 iaas.go:243]</span><br><span class=\"line\">NAME                PREFIX              DATE                PATH</span><br><span class=\"line\">kubernetes-id-001   wdrip/backup          20211002-1024       wdrip/backup/kubernetes-id-001/20211002-1024/snapshot.db</span><br><span class=\"line\">kubernetes-id-001   wdrip/backup          20211002-1014       wdrip/backup/kubernetes-id-001/20211002-1014/snapshot.db</span><br><span class=\"line\">kubernetes-id-001   wdrip/backup          20211002-1004       wdrip/backup/kubernetes-id-001/20211002-1004/snapshot.db</span><br><span class=\"line\">kubernetes-id-001   wdrip/backup          20211002-0954       wdrip/backup/kubernetes-id-001/20211002-0954/snapshot.db</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"恢复场景一：在原基础设施上恢复\"><a href=\"#恢复场景一：在原基础设施上恢复\" class=\"headerlink\" title=\"恢复场景一：在原基础设施上恢复\"></a>恢复场景一：在原基础设施上恢复</h3><p>如果k8s因未知因素管控完全故障，wdrip没有能够自行恢复，那么您可以手动触发命令执行恢复。仅需要一行命令即可</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -n 指定恢复的目标集群（本体）。</span></span><br><span class=\"line\">(base) ➜ wdrip recover -n kubernetes-wdrip-121</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>等待大概2到3分钟后，通过kubectl get no 查看节点恢复情况。</p>\n<h3 id=\"恢复场景二：恢复到新的基础设施上\"><a href=\"#恢复场景二：恢复到新的基础设施上\" class=\"headerlink\" title=\"恢复场景二：恢复到新的基础设施上\"></a>恢复场景二：恢复到新的基础设施上</h3><p>有些情况下，集群所对应的基础设施被损坏，人为的或者灾难性的，以至于无法在原有基础设施上恢复，那么我们可以通过新建一个具有相同规格的基础设施的集群，然后在本基础设施上恢复出原有集群的备份意识。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 创建具有相同规格的新的集群kubernetes-wdrip-121</span></span><br><span class=\"line\">(base) ➜ wdrip create --config config.yaml</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将备份的旧的kubernetes-wdrip-120集群配置（意识）恢复到新建的集群121上。</span></span><br><span class=\"line\"><span class=\"comment\"># -n 指定恢复的目标集群（本体）， -f 指定恢复的配置来源于哪个集群（意识）。</span></span><br><span class=\"line\">(base) ➜ wdrip recover -n kubernetes-wdrip-121 -f kubernetes-wdrip-120</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"节点修复机制\"><a href=\"#节点修复机制\" class=\"headerlink\" title=\"节点修复机制\"></a>节点修复机制</h2><p>节点是运行负载的工具而已，对于失效的节点，替换是成本最小的方案，替换之前我们会尝试重启来恢复。</p>\n<h2 id=\"自定义集群参数能力\"><a href=\"#自定义集群参数能力\" class=\"headerlink\" title=\"自定义集群参数能力\"></a>自定义集群参数能力</h2><p>规划中（节点、集群）</p>\n<h2 id=\"运维简化机制\"><a href=\"#运维简化机制\" class=\"headerlink\" title=\"运维简化机制\"></a>运维简化机制</h2><p>节点故障后快速恢复，定位问题很难。 在wdrip这里，你可以直接删掉故障节点，让系统直接拉起一个新的节点副本即可。</p>"},{"title":"云原生应用示例","date":"2022-06-08T11:39:25.000Z","_content":"\n## DEMO-APPLICATION\n请先将示例应用部署配置文件`git clone`到本地.\n``` shell\n(base) ➜ git clone https://github.com/aoxn/wdrip.git\n```\n\n### 示例应用一： 文件共享服务器\n`filebrowser` 应用提供文件共享服务，存储使用阿里云OSS存储系统。因此需要配置阿里云ACCESS_KEY_ID和ACCESS_KEY_SEC与REGION。\n请先git clone 代码到本地，安装filebrowser的脚本位于`hack/example/filebrowser.sh`\n```shell\n(base) ➜ export REGION=cn-hangzhou\n(base) ➜ export ACCESS_KEY_ID=xxxx\n(base) ➜ export ACCESS_KEY_SECRET=yyyyy\n(base) ➜ export KUBECONFIG=~/.kube/config.wdrip\n(base) ➜ bash hack/example/filebrowser.sh\n(base) ➜\n(base) ➜ kubectl get svc\nNAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nfilebrowser   LoadBalancer   172.19.4.156   47.110.243.41   80:31295/TCP   22d\nkubernetes    ClusterIP      172.19.0.1     <none>          443/TCP        22d\n\n```\n<!-- more -->\n\n访问`http://47.110.243.41`开始玩转filebrowser. 初始密码admin/admin ? 记得改密码\n![img.png](/upload/img.png)\n\n等待10分钟后，wdrip会自动snapshot整个etcd配置。此时可以尝试带着应用场景测试wdrip的基础设施复原力。\n\n\n### 示例应用二：wordpress\n\n```shell\n(base) ➜ export KUBECONFIG=~/.kube/config.wdrip\n(base) ➜ bash hack/example/wordpress.sh\n(base) ➜\n(base) ➜ kubectl get svc\nNAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nfilebrowser   LoadBalancer   172.19.4.157   47.110.243.30   80:31295/TCP   22d\nkubernetes    ClusterIP      172.19.0.1     <none>          443/TCP        22d\n```\n\n访问`http://47.110.243.30`\n\n### 示例应用三：蓝绿发布应用\n\n```shell\n(base) ➜ export KUBECONFIG=~/.kube/config.wdrip\n(base) ➜ kubectl apply -f hack/example/blue.yml\n(base) ➜\n(base) ➜ kubectl get svc\nNAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nfilebrowser   LoadBalancer   172.19.4.158   47.110.243.50   80:31295/TCP   22d\nkubernetes    ClusterIP      172.19.0.1     <none>          443/TCP        22d\n```\n访问`http://47.110.243.50`\n\n","source":"_posts/云原生应用示例.md","raw":"title: 云原生应用示例\ndate: 2022-06-08 19:39:25\ntags:\n---\n\n## DEMO-APPLICATION\n请先将示例应用部署配置文件`git clone`到本地.\n``` shell\n(base) ➜ git clone https://github.com/aoxn/wdrip.git\n```\n\n### 示例应用一： 文件共享服务器\n`filebrowser` 应用提供文件共享服务，存储使用阿里云OSS存储系统。因此需要配置阿里云ACCESS_KEY_ID和ACCESS_KEY_SEC与REGION。\n请先git clone 代码到本地，安装filebrowser的脚本位于`hack/example/filebrowser.sh`\n```shell\n(base) ➜ export REGION=cn-hangzhou\n(base) ➜ export ACCESS_KEY_ID=xxxx\n(base) ➜ export ACCESS_KEY_SECRET=yyyyy\n(base) ➜ export KUBECONFIG=~/.kube/config.wdrip\n(base) ➜ bash hack/example/filebrowser.sh\n(base) ➜\n(base) ➜ kubectl get svc\nNAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nfilebrowser   LoadBalancer   172.19.4.156   47.110.243.41   80:31295/TCP   22d\nkubernetes    ClusterIP      172.19.0.1     <none>          443/TCP        22d\n\n```\n<!-- more -->\n\n访问`http://47.110.243.41`开始玩转filebrowser. 初始密码admin/admin ? 记得改密码\n![img.png](/upload/img.png)\n\n等待10分钟后，wdrip会自动snapshot整个etcd配置。此时可以尝试带着应用场景测试wdrip的基础设施复原力。\n\n\n### 示例应用二：wordpress\n\n```shell\n(base) ➜ export KUBECONFIG=~/.kube/config.wdrip\n(base) ➜ bash hack/example/wordpress.sh\n(base) ➜\n(base) ➜ kubectl get svc\nNAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nfilebrowser   LoadBalancer   172.19.4.157   47.110.243.30   80:31295/TCP   22d\nkubernetes    ClusterIP      172.19.0.1     <none>          443/TCP        22d\n```\n\n访问`http://47.110.243.30`\n\n### 示例应用三：蓝绿发布应用\n\n```shell\n(base) ➜ export KUBECONFIG=~/.kube/config.wdrip\n(base) ➜ kubectl apply -f hack/example/blue.yml\n(base) ➜\n(base) ➜ kubectl get svc\nNAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nfilebrowser   LoadBalancer   172.19.4.158   47.110.243.50   80:31295/TCP   22d\nkubernetes    ClusterIP      172.19.0.1     <none>          443/TCP        22d\n```\n访问`http://47.110.243.50`\n\n","slug":"云原生应用示例","published":1,"updated":"2022-06-08T12:18:41.061Z","_id":"cl45iy25j0000ndrh75l471yg","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"DEMO-APPLICATION\"><a href=\"#DEMO-APPLICATION\" class=\"headerlink\" title=\"DEMO-APPLICATION\"></a>DEMO-APPLICATION</h2><p>请先将示例应用部署配置文件<code>git clone</code>到本地.</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ git clone https://github.com/aoxn/wdrip.git</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"示例应用一：-文件共享服务器\"><a href=\"#示例应用一：-文件共享服务器\" class=\"headerlink\" title=\"示例应用一： 文件共享服务器\"></a>示例应用一： 文件共享服务器</h3><p><code>filebrowser</code> 应用提供文件共享服务，存储使用阿里云OSS存储系统。因此需要配置阿里云ACCESS_KEY_ID和ACCESS_KEY_SEC与REGION。<br>请先git clone 代码到本地，安装filebrowser的脚本位于<code>hack/example/filebrowser.sh</code></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ export REGION=cn-hangzhou</span><br><span class=\"line\">(base) ➜ export ACCESS_KEY_ID=xxxx</span><br><span class=\"line\">(base) ➜ export ACCESS_KEY_SECRET=yyyyy</span><br><span class=\"line\">(base) ➜ export KUBECONFIG=~/.kube/config.wdrip</span><br><span class=\"line\">(base) ➜ bash hack/example/filebrowser.sh</span><br><span class=\"line\">(base) ➜</span><br><span class=\"line\">(base) ➜ kubectl get svc</span><br><span class=\"line\">NAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE</span><br><span class=\"line\">filebrowser   LoadBalancer   172.19.4.156   47.110.243.41   80:31295/TCP   22d</span><br><span class=\"line\">kubernetes    ClusterIP      172.19.0.1     &lt;none&gt;          443/TCP        22d</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<span id=\"more\"></span>\n\n<p>访问<code>http://47.110.243.41</code>开始玩转filebrowser. 初始密码admin&#x2F;admin ? 记得改密码<br><img src=\"/upload/img.png\" alt=\"img.png\"></p>\n<p>等待10分钟后，wdrip会自动snapshot整个etcd配置。此时可以尝试带着应用场景测试wdrip的基础设施复原力。</p>\n<h3 id=\"示例应用二：wordpress\"><a href=\"#示例应用二：wordpress\" class=\"headerlink\" title=\"示例应用二：wordpress\"></a>示例应用二：wordpress</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ export KUBECONFIG=~/.kube/config.wdrip</span><br><span class=\"line\">(base) ➜ bash hack/example/wordpress.sh</span><br><span class=\"line\">(base) ➜</span><br><span class=\"line\">(base) ➜ kubectl get svc</span><br><span class=\"line\">NAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE</span><br><span class=\"line\">filebrowser   LoadBalancer   172.19.4.157   47.110.243.30   80:31295/TCP   22d</span><br><span class=\"line\">kubernetes    ClusterIP      172.19.0.1     &lt;none&gt;          443/TCP        22d</span><br></pre></td></tr></table></figure>\n\n<p>访问<code>http://47.110.243.30</code></p>\n<h3 id=\"示例应用三：蓝绿发布应用\"><a href=\"#示例应用三：蓝绿发布应用\" class=\"headerlink\" title=\"示例应用三：蓝绿发布应用\"></a>示例应用三：蓝绿发布应用</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ export KUBECONFIG=~/.kube/config.wdrip</span><br><span class=\"line\">(base) ➜ kubectl apply -f hack/example/blue.yml</span><br><span class=\"line\">(base) ➜</span><br><span class=\"line\">(base) ➜ kubectl get svc</span><br><span class=\"line\">NAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE</span><br><span class=\"line\">filebrowser   LoadBalancer   172.19.4.158   47.110.243.50   80:31295/TCP   22d</span><br><span class=\"line\">kubernetes    ClusterIP      172.19.0.1     &lt;none&gt;          443/TCP        22d</span><br></pre></td></tr></table></figure>\n<p>访问<code>http://47.110.243.50</code></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"DEMO-APPLICATION\"><a href=\"#DEMO-APPLICATION\" class=\"headerlink\" title=\"DEMO-APPLICATION\"></a>DEMO-APPLICATION</h2><p>请先将示例应用部署配置文件<code>git clone</code>到本地.</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ git clone https://github.com/aoxn/wdrip.git</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"示例应用一：-文件共享服务器\"><a href=\"#示例应用一：-文件共享服务器\" class=\"headerlink\" title=\"示例应用一： 文件共享服务器\"></a>示例应用一： 文件共享服务器</h3><p><code>filebrowser</code> 应用提供文件共享服务，存储使用阿里云OSS存储系统。因此需要配置阿里云ACCESS_KEY_ID和ACCESS_KEY_SEC与REGION。<br>请先git clone 代码到本地，安装filebrowser的脚本位于<code>hack/example/filebrowser.sh</code></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ export REGION=cn-hangzhou</span><br><span class=\"line\">(base) ➜ export ACCESS_KEY_ID=xxxx</span><br><span class=\"line\">(base) ➜ export ACCESS_KEY_SECRET=yyyyy</span><br><span class=\"line\">(base) ➜ export KUBECONFIG=~/.kube/config.wdrip</span><br><span class=\"line\">(base) ➜ bash hack/example/filebrowser.sh</span><br><span class=\"line\">(base) ➜</span><br><span class=\"line\">(base) ➜ kubectl get svc</span><br><span class=\"line\">NAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE</span><br><span class=\"line\">filebrowser   LoadBalancer   172.19.4.156   47.110.243.41   80:31295/TCP   22d</span><br><span class=\"line\">kubernetes    ClusterIP      172.19.0.1     &lt;none&gt;          443/TCP        22d</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>","more":"<p>访问<code>http://47.110.243.41</code>开始玩转filebrowser. 初始密码admin&#x2F;admin ? 记得改密码<br><img src=\"/upload/img.png\" alt=\"img.png\"></p>\n<p>等待10分钟后，wdrip会自动snapshot整个etcd配置。此时可以尝试带着应用场景测试wdrip的基础设施复原力。</p>\n<h3 id=\"示例应用二：wordpress\"><a href=\"#示例应用二：wordpress\" class=\"headerlink\" title=\"示例应用二：wordpress\"></a>示例应用二：wordpress</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ export KUBECONFIG=~/.kube/config.wdrip</span><br><span class=\"line\">(base) ➜ bash hack/example/wordpress.sh</span><br><span class=\"line\">(base) ➜</span><br><span class=\"line\">(base) ➜ kubectl get svc</span><br><span class=\"line\">NAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE</span><br><span class=\"line\">filebrowser   LoadBalancer   172.19.4.157   47.110.243.30   80:31295/TCP   22d</span><br><span class=\"line\">kubernetes    ClusterIP      172.19.0.1     &lt;none&gt;          443/TCP        22d</span><br></pre></td></tr></table></figure>\n\n<p>访问<code>http://47.110.243.30</code></p>\n<h3 id=\"示例应用三：蓝绿发布应用\"><a href=\"#示例应用三：蓝绿发布应用\" class=\"headerlink\" title=\"示例应用三：蓝绿发布应用\"></a>示例应用三：蓝绿发布应用</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ➜ export KUBECONFIG=~/.kube/config.wdrip</span><br><span class=\"line\">(base) ➜ kubectl apply -f hack/example/blue.yml</span><br><span class=\"line\">(base) ➜</span><br><span class=\"line\">(base) ➜ kubectl get svc</span><br><span class=\"line\">NAME          TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE</span><br><span class=\"line\">filebrowser   LoadBalancer   172.19.4.158   47.110.243.50   80:31295/TCP   22d</span><br><span class=\"line\">kubernetes    ClusterIP      172.19.0.1     &lt;none&gt;          443/TCP        22d</span><br></pre></td></tr></table></figure>\n<p>访问<code>http://47.110.243.50</code></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cl4507g6k0003jjrh0irda8vc","category_id":"cl4507g6s000ajjrhe2nj4uby","_id":"cl4507g6y000tjjrh5rcz6onm"},{"post_id":"cl4507g6k0003jjrh0irda8vc","category_id":"cl4507g6x000pjjrhb7wmeltk","_id":"cl4507g6z000wjjrh7bgfeep6"},{"post_id":"cl4507g6p0007jjrhdr5m8yz5","category_id":"cl4507g6t000djjrh6oq49xe5","_id":"cl4507g6z000yjjrh50ii00yo"},{"post_id":"cl4507g6p0007jjrhdr5m8yz5","category_id":"cl4507g6y000rjjrhdug55iit","_id":"cl4507g6z000zjjrhct2e7mft"},{"post_id":"cl4507g6q0008jjrh5y1c7ymc","category_id":"cl4507g6t000djjrh6oq49xe5","_id":"cl4507g6z0010jjrh9c3teo6v"},{"post_id":"cl4507g6q0008jjrh5y1c7ymc","category_id":"cl4507g6y000rjjrhdug55iit","_id":"cl4507g700012jjrhgi1d0ipx"},{"post_id":"cl4507g6g0001jjrh2lra3s19","category_id":"cl4507g6m0004jjrh5sed1t6d","_id":"cl4507g700013jjrh1dvz3x55"},{"post_id":"cl4507g6g0001jjrh2lra3s19","category_id":"cl4507g6v000jjjrh1u4lfnoy","_id":"cl4507g700014jjrh8bba46v4"},{"post_id":"cl4507g6g0001jjrh2lra3s19","category_id":"cl4507g6z000xjjrh2qbbdjs7","_id":"cl4507g700015jjrhfluzeolp"},{"post_id":"cl4507g6r0009jjrh90bk0sw6","category_id":"cl4507g6t000djjrh6oq49xe5","_id":"cl4507g700016jjrh5a3n2tva"},{"post_id":"cl4507g6r0009jjrh90bk0sw6","category_id":"cl4507g6y000rjjrhdug55iit","_id":"cl4507g700017jjrh4q0m2wn2"}],"PostTag":[{"post_id":"cl4507g6g0001jjrh2lra3s19","tag_id":"cl4507g6o0005jjrhdjqh2goq","_id":"cl4507g6t000cjjrh50m64iin"},{"post_id":"cl4507g6k0003jjrh0irda8vc","tag_id":"cl4507g6s000bjjrh2l6158m5","_id":"cl4507g6v000hjjrh8fbd38w2"},{"post_id":"cl4507g6k0003jjrh0irda8vc","tag_id":"cl4507g6t000ejjrheyve6bha","_id":"cl4507g6v000ijjrh09tvbksf"},{"post_id":"cl4507g6p0007jjrhdr5m8yz5","tag_id":"cl4507g6u000gjjrhgz24fh5v","_id":"cl4507g6w000ljjrhao3t3f61"},{"post_id":"cl4507g6q0008jjrh5y1c7ymc","tag_id":"cl4507g6w000kjjrh3cn2cv7i","_id":"cl4507g6x000ojjrh9ggk7pxo"},{"post_id":"cl4507g6r0009jjrh90bk0sw6","tag_id":"cl4507g6w000njjrh8iz67nhr","_id":"cl4507g6y000sjjrh2j0ba2yq"},{"post_id":"cl4507g6r0009jjrh90bk0sw6","tag_id":"cl4507g6x000qjjrh97gj7r5z","_id":"cl4507g6y000vjjrhdv6hdzeq"}],"Tag":[{"name":"Kubernetes","_id":"cl4507g6o0005jjrhdjqh2goq"},{"name":"Docker","_id":"cl4507g6s000bjjrh2l6158m5"},{"name":"Registry","_id":"cl4507g6t000ejjrheyve6bha"},{"name":"页表","_id":"cl4507g6u000gjjrhgz24fh5v"},{"name":"进程切换","_id":"cl4507g6w000kjjrh3cn2cv7i"},{"name":"实模式","_id":"cl4507g6w000njjrh8iz67nhr"},{"name":"保护模式","_id":"cl4507g6x000qjjrh97gj7r5z"}]}}